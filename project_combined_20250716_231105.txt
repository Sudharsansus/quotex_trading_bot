================================================================================
QUOTEX TRADING BOT - COMPLETE PROJECT FILES
================================================================================
Generated: 2025-07-16 23:11:05.701218
Project: quotex_trading_bot
================================================================================


================================================================================
TABLE OF CONTENTS
================================================================================

 1. main.py (ROOT)
 2. requirements.txt (ROOT)
 3. config\__init__.py (config)
 4. config\credentials.py (config)
 5. config\settings.py (config)
 6. docs\README.md (docs)
 7. docs\installation.md (docs)
 8. docs\usage.md (docs)
 9. scripts\backtest.py (scripts)
10. scripts\optimize.py (scripts)
11. scripts\setup.py (scripts)
12. src\__init__.py (src)
13. src\api\__init__.py (src\api)
14. src\api\market_data.py (src\api)
15. src\api\quotex_api.py (src\api)
16. src\bot\__init__.py (src\bot)
17. src\bot\risk_manager.py (src\bot)
18. src\bot\strategy_manager.py (src\bot)
19. src\bot\trading_bot.py (src\bot)
20. src\database\__init__.py (src\database)
21. src\database\database_manager.py (src\database)
22. src\database\models.py (src\database)
23. src\indicators\__init__.py (src\indicators)
24. src\indicators\custom_indicators.py (src\indicators)
25. src\indicators\signal_generator.py (src\indicators)
26. src\indicators\technical_indicators.py (src\indicators)
27. src\ml\__init__.py (src\ml)
28. src\ml\feature_engineering.py (src\ml)
29. src\ml\model_trainer.py (src\ml)
30. src\ml\prediction_engine.py (src\ml)
31. src\utils\__init__.py (src\utils)
32. src\utils\data_processor.py (src\utils)
33. src\utils\helpers.py (src\utils)
34. src\utils\logger.py (src\utils)
35. tests\__init__.py (tests)
36. tests\test_bot.py (tests)
37. tests\test_indicators.py (tests)
38. tests\test_ml.py (tests)

================================================================================

fully!")
            return True
            
        except Exception as e:
            self.logger.error(f"❌ System initialization failed: {e}")
            return False
    
    async def start_trading(self):
        """Start the trading system"""
        if not await self.initialize():
            return
        
        self.running = True
        self.logger.info("🎯 Starting trading operations...")
        
        try:
            # Register signal handlers for graceful shutdown
            signal.signal(signal.SIGINT, self._signal_handler)
            signal.signal(signal.SIGTERM, self._signal_handler)
            
            # Start the main trading loop
            await self.bot.start_trading()
            
        except Exception as e:
            self.logger.error(f"❌ Trading error: {e}")
        finally:
            await self.shutdown()
    
    def _signal_handler(self, signum, frame):
        """Handle shutdown signals"""
        self.logger.info(f"🛑 Received signal {signum}, shutting down...")
        self.running = False
        
    async def shutdown(self):
        """Gracefully shutdown the system"""
        self.logger.info("🔄 Shutting down trading system...")
        
        if self.bot:
            await self.bot.stop_trading()
        
        if self.db_manager:
            await self.db_manager.close()
        
        self.logger.info("✅ System shutdown complete")

def main():
    """Main entry point"""
    print("""
    ╔═══════════════════════════════════════════════════════════════╗
    ║                    QUOTEX TRADING BOT                         ║
    ║                   Advanced AI-Powered System                  ║
    ║                                                               ║
    ║  Features:                                                    ║
    ║  • Advanced Technical Analysis                                ║
    ║  • Machine Learning Predictions                               ║
    ║  • Risk Management System                                     ║
    ║  • Real-time Market Data                                      ║
    ║  • Multi-timeframe Analysis                                   ║
    ║                                                               ║
    ║  ⚠️  WARNING: Trading involves significant risk               ║
    ║      Only use with funds you can afford to lose              ║
    ╚═══════════════════════════════════════════════════════════════╝
    """)
    
    # Create and run the trading system
    trading_system = QuotexTradingSystem()
    
    try:
        # Run the async event loop
        asyncio.run(trading_system.start_trading())
    except KeyboardInterrupt:
        print("\n🛑 Trading bot stopped by user")
    except Exception as e:
        print(f"❌ Fatal error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()

================================================================================
FILE: requirements.txt
DIRECTORY: ROOT
SIZE: 689 bytes
================================================================================

# Core dependencies
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.0
tensorflow==2.13.0
keras==2.13.1

# Technical Analysis
ta==0.10.2
TA-Lib==0.4.26
pandas-ta==0.3.14b0

# API and Web
requests==2.31.0
websocket-client==1.6.1
aiohttp==3.8.5
asyncio==3.4.3

# Database
sqlalchemy==2.0.19
sqlite3

# Data Processing
plotly==5.15.0
matplotlib==3.7.2
seaborn==0.12.2

# Machine Learning
xgboost==1.7.6
lightgbm==4.0.0
catboost==1.2.2

# Utilities
python-dotenv==1.0.0
schedule==1.2.0
colorama==0.4.6
tqdm==4.65.0
joblib==1.3.2

# Testing
pytest==7.4.0
pytest-asyncio==0.21.1

# Logging
loguru==0.7.0

# Configuration
pydantic==2.1.1
PyYAML==6.0.1

================================================================================
FILE: config\__init__.py
DIRECTORY: config
SIZE: 0 bytes
================================================================================



================================================================================
FILE: config\credentials.py
DIRECTORY: config
SIZE: 3735 bytes
================================================================================

"""
Quotex API Credentials Management
Secure handling of API keys and authentication
"""

import os
from typing import Optional
from cryptography.fernet import Fernet
import base64
import hashlib

class CredentialsManager:
    def __init__(self):
        self.encryption_key = self._get_or_create_key()
        self.fernet = Fernet(self.encryption_key)
    
    def _get_or_create_key(self) -> bytes:
        """Get or create encryption key for credentials"""
        key_file = "config/.key"
        if os.path.exists(key_file):
            with open(key_file, 'rb') as f:
                return f.read()
        else:
            key = Fernet.generate_key()
            with open(key_file, 'wb') as f:
                f.write(key)
            return key
    
    def encrypt_credential(self, credential: str) -> str:
        """Encrypt a credential string"""
        return self.fernet.encrypt(credential.encode()).decode()
    
    def decrypt_credential(self, encrypted_credential: str) -> str:
        """Decrypt a credential string"""
        return self.fernet.decrypt(encrypted_credential.encode()).decode()
    
    def save_credentials(self, email: str, password: str, api_key: Optional[str] = None):
        """Save encrypted credentials to file"""
        creds = {
            'email': self.encrypt_credential(email),
            'password': self.encrypt_credential(password),
            'api_key': self.encrypt_credential(api_key) if api_key else None
        }
        
        with open('config/.credentials', 'w') as f:
            import json
            json.dump(creds, f)
    
    def load_credentials(self) -> dict:
        """Load and decrypt credentials from file"""
        try:
            with open('config/.credentials', 'r') as f:
                import json
                encrypted_creds = json.load(f)
            
            return {
                'email': self.decrypt_credential(encrypted_creds['email']),
                'password': self.decrypt_credential(encrypted_creds['password']),
                'api_key': self.decrypt_credential(encrypted_creds['api_key']) if encrypted_creds.get('api_key') else None
            }
        except FileNotFoundError:
            return {}

# Quotex API Configuration
QUOTEX_CONFIG = {
    'base_url': 'https://qxbroker.com',
    'api_url': 'https://qxbroker.com/api',
    'websocket_url': 'wss://ws.qxbroker.com/socket.io/',
    'timeout': 30,
    'max_retries': 3,
    'retry_delay': 1
}

# Demo vs Live trading
TRADING_MODE = {
    'demo': True,  # Set to False for live trading
    'demo_balance': 10000,
    'live_balance': 0
}

# Initialize credentials manager
credentials_manager = CredentialsManager()

def get_quotex_credentials() -> dict:
    """Get Quotex credentials from environment or file"""
    # Try environment variables first
    email = os.getenv('QUOTEX_EMAIL')
    password = os.getenv('QUOTEX_PASSWORD')
    api_key = os.getenv('QUOTEX_API_KEY')
    
    if email and password:
        return {
            'email': email,
            'password': password,
            'api_key': api_key
        }
    
    # Try encrypted file
    return credentials_manager.load_credentials()

def setup_credentials():
    """Interactive setup for credentials"""
    print("Setting up Quotex credentials...")
    email = input("Enter your Quotex email: ")
    password = input("Enter your Quotex password: ")
    api_key = input("Enter your Quotex API key (optional): ")
    
    credentials_manager.save_credentials(email, password, api_key)
    print("Credentials saved securely!")

if __name__ == "__main__":
    setup_credentials()

================================================================================
FILE: config\settings.py
DIRECTORY: config
SIZE: 6857 bytes
================================================================================

"""
Configuration settings for Quotex Trading Bot
"""

import os
from pathlib import Path
from typing import Dict, List, Any

# Project paths
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
MODELS_DIR = DATA_DIR / "models"
LOGS_DIR = DATA_DIR / "logs"
HISTORICAL_DIR = DATA_DIR / "historical"

# Create directories if they don't exist
for directory in [DATA_DIR, MODELS_DIR, LOGS_DIR, HISTORICAL_DIR]:
    directory.mkdir(parents=True, exist_ok=True)

# Trading Configuration
TRADING_CONFIG = {
    "assets": ["EURUSD", "GBPUSD", "USDJPY", "AUDUSD", "USDCAD", "EURJPY"],
    "timeframes": ["1m", "5m", "15m", "30m", "1h"],
    "primary_timeframe": "5m",
    "trade_amount": 10,  # Starting trade amount
    "max_trade_amount": 100,
    "max_daily_trades": 50,
    "max_concurrent_trades": 3,
    "trading_hours": {
        "start": "08:00",
        "end": "18:00",
        "timezone": "UTC"
    },
    "min_confidence": 0.7,  # Minimum confidence for trade execution
    "auto_trading": True,
    "demo_mode": True,  # Set to False for live trading
}

# Risk Management Configuration
RISK_CONFIG = {
    "max_risk_per_trade": 0.02,  # 2% of account balance
    "max_daily_loss": 0.10,  # 10% of account balance
    "max_drawdown": 0.20,  # 20% maximum drawdown
    "profit_target": 0.05,  # 5% daily profit target
    "stop_loss_multiplier": 2.0,
    "take_profit_multiplier": 3.0,
    "martingale_enabled": False,
    "martingale_multiplier": 2.0,
    "max_martingale_steps": 3,
    "dynamic_position_sizing": True,
    "kelly_criterion": True,
}

# Technical Analysis Configuration
TA_CONFIG = {
    "indicators": {
        "sma": [20, 50, 200],
        "ema": [12, 26, 50],
        "rsi": {"period": 14, "overbought": 70, "oversold": 30},
        "macd": {"fast": 12, "slow": 26, "signal": 9},
        "bollinger": {"period": 20, "std": 2},
        "stochastic": {"k_period": 14, "d_period": 3},
        "adx": {"period": 14, "threshold": 25},
        "williams_r": {"period": 14},
        "cci": {"period": 20},
        "mfi": {"period": 14}
    },
    "custom_indicators": {
        "trend_strength": True,
        "support_resistance": True,
        "volume_profile": True,
        "market_structure": True
    },
    "signal_weights": {
        "trend": 0.35,
        "momentum": 0.25,
        "volume": 0.20,
        "volatility": 0.20
    }
}

# Machine Learning Configuration
ML_CONFIG = {
    "models": {
        "primary": "ensemble",  # ensemble, xgboost, lightgbm, neural_network
        "ensemble_models": ["xgboost", "lightgbm", "neural_network"],
        "retrain_interval": 24,  # hours
        "min_training_samples": 1000,
        "validation_split": 0.2,
        "test_split": 0.1
    },
    "features": {
        "technical_indicators": True,
        "price_action": True,
        "volume_indicators": True,
        "market_microstructure": True,
        "sentiment_analysis": False,  # Requires news API
        "time_features": True,
        "lag_features": [1, 2, 3, 5, 10],
        "rolling_stats": [5, 10, 20, 50]
    },
    "preprocessing": {
        "normalize": True,
        "handle_missing": "forward_fill",
        "outlier_detection": True,
        "feature_selection": True,
        "pca": False
    },
    "hyperparameters": {
        "xgboost": {
            "n_estimators": 100,
            "max_depth": 6,
            "learning_rate": 0.1,
            "subsample": 0.8,
            "colsample_bytree": 0.8
        },
        "lightgbm": {
            "n_estimators": 100,
            "max_depth": 6,
            "learning_rate": 0.1,
            "subsample": 0.8,
            "colsample_bytree": 0.8
        },
        "neural_network": {
            "hidden_layers": [128, 64, 32],
            "dropout": 0.2,
            "learning_rate": 0.001,
            "batch_size": 32,
            "epochs": 100
        }
    }
}

# API Configuration
API_CONFIG = {
    "quotex": {
        "base_url": "https://api.quotex.io",
        "websocket_url": "wss://ws.quotex.io",
        "timeout": 30,
        "max_retries": 3,
        "retry_delay": 1
    },
    "rate_limits": {
        "requests_per_minute": 100,
        "websocket_connections": 5
    }
}

# Database Configuration
DATABASE_CONFIG = {
    "type": "sqlite",
    "name": "quotex_trading.db",
    "path": DATA_DIR / "quotex_trading.db",
    "backup_interval": 3600,  # seconds
    "max_backup_files": 10,
    "tables": {
        "trades": True,
        "market_data": True,
        "predictions": True,
        "performance": True,
        "signals": True
    }
}

# Logging Configuration
LOGGING_CONFIG = {
    "level": "INFO",
    "format": "%(asctime)s | %(name)s | %(levelname)s | %(message)s",
    "file_rotation": "1 day",
    "file_retention": "30 days",
    "max_file_size": "10 MB",
    "console_output": True,
    "file_output": True,
    "log_file": LOGS_DIR / "quotex_bot.log"
}

# Notification Configuration
NOTIFICATION_CONFIG = {
    "enabled": True,
    "methods": ["console", "file"],  # Add "email", "telegram", "discord" if needed
    "triggers": {
        "trade_opened": True,
        "trade_closed": True,
        "profit_target_reached": True,
        "stop_loss_hit": True,
        "daily_summary": True,
        "system_errors": True
    }
}

# Backtesting Configuration
BACKTEST_CONFIG = {
    "initial_balance": 1000,
    "commission": 0.0,  # Quotex doesn't charge commission
    "slippage": 0.0001,  # Minimal slippage for binary options
    "start_date": "2023-01-01",
    "end_date": "2024-01-01",
    "benchmark": "EURUSD",
    "metrics": ["total_return", "sharpe_ratio", "max_drawdown", "win_rate", "profit_factor"]
}

# Environment Variables (override with .env file)
def get_env_var(key: str, default: Any = None) -> Any:
    """Get environment variable with fallback to default"""
    return os.getenv(key, default)

# Quotex API credentials (set via environment variables)
QUOTEX_EMAIL = get_env_var("QUOTEX_EMAIL", "")
QUOTEX_PASSWORD = get_env_var("QUOTEX_PASSWORD", "")
QUOTEX_API_KEY = get_env_var("QUOTEX_API_KEY", "")

# Security settings
SECURITY_CONFIG = {
    "encrypt_credentials": True,
    "api_key_rotation": True,
    "secure_logging": True,
    "max_login_attempts": 3,
    "session_timeout": 3600  # seconds
}

# Development/Debug settings
DEBUG_CONFIG = {
    "debug_mode": get_env_var("DEBUG", "False").lower() == "true",
    "verbose_logging": get_env_var("VERBOSE", "False").lower() == "true",
    "save_debug_data": True,
    "plot_charts": False,  # Set to True for visual debugging
    "dry_run": get_env_var("DRY_RUN", "False").lower() == "true"
}

================================================================================
FILE: docs\README.md
DIRECTORY: docs
SIZE: 0 bytes
================================================================================



================================================================================
FILE: docs\installation.md
DIRECTORY: docs
SIZE: 0 bytes
================================================================================



================================================================================
FILE: docs\usage.md
DIRECTORY: docs
SIZE: 0 bytes
================================================================================



================================================================================
FILE: scripts\backtest.py
DIRECTORY: scripts
SIZE: 0 bytes
================================================================================



================================================================================
FILE: scripts\optimize.py
DIRECTORY: scripts
SIZE: 0 bytes
================================================================================



================================================================================
FILE: scripts\setup.py
DIRECTORY: scripts
SIZE: 0 bytes
================================================================================



================================================================================
FILE: src\__init__.py
DIRECTORY: src
SIZE: 4137 bytes
================================================================================

"""
Quotex Trading Bot - Main Package

A comprehensive trading bot for Quotex platform with advanced features:
- Machine learning-based predictions
- Technical indicator analysis
- Risk management
- Real-time market data processing
- Automated trading strategies
"""

__version__ = "1.0.0"
__author__ = "Trading Bot Team"
__description__ = "Advanced Trading Bot for Quotex Platform"

# Package metadata
PACKAGE_INFO = {
    "name": "quotex_trading_bot",
    "version": __version__,
    "author": __author__,
    "description": __description__,
    "modules": [
        "api",        # Market data and Quotex API integration
        "bot",        # Trading bot core functionality
        "database",   # Database management
        "indicators", # Technical indicators
        "ml",         # Machine learning components
        "utils"       # Utilities and helpers
    ]
}

# Try to import utils package
try:
    from . import utils
    utils_available = True
except ImportError as e:
    print(f"Warning: Utils package not available: {e}")
    utils_available = False

# Try to import other main modules
try:
    from . import api
    api_available = True
except ImportError:
    api_available = False

try:
    from . import bot
    bot_available = True
except ImportError:
    bot_available = False

try:
    from . import database
    database_available = True
except ImportError:
    database_available = False

try:
    from . import indicators
    indicators_available = True
except ImportError:
    indicators_available = False

try:
    from . import ml
    ml_available = True
except ImportError:
    ml_available = False

# Package status
PACKAGE_STATUS = {
    "utils": utils_available,
    "api": api_available, 
    "bot": bot_available,
    "database": database_available,
    "indicators": indicators_available,
    "ml": ml_available
}

def get_package_status():
    """Get the status of all package modules."""
    return PACKAGE_STATUS

def print_package_status():
    """Print the status of all package modules."""
    print(f"\n{PACKAGE_INFO['name']} v{PACKAGE_INFO['version']}")
    print(f"Description: {PACKAGE_INFO['description']}")
    print(f"Author: {PACKAGE_INFO['author']}")
    print("\nModule Status:")
    for module, available in PACKAGE_STATUS.items():
        status = "✓ Available" if available else "✗ Not Available"
        print(f"  {module}: {status}")

def setup_project():
    """
    Initialize the trading bot project.
    This function can be called to set up logging, create directories, etc.
    """
    print("Setting up Quotex Trading Bot...")
    
    # Create necessary directories
    import os
    directories = [
        "data/logs",
        "data/historical", 
        "data/models",
        "config"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"✓ Created directory: {directory}")
    
    # Initialize logging if utils is available
    if utils_available:
        try:
            from .utils import setup_logging
            setup_logging("INFO", "data/logs")
            print("✓ Logging initialized")
        except Exception as e:
            print(f"✗ Failed to setup logging: {e}")
    
    print("Project setup completed!")

# Convenience imports - only if modules are available
if utils_available:
    try:
        from .utils import get_logger, setup_logging
    except ImportError:
        pass

# Main exports
__all__ = [
    "PACKAGE_INFO",
    "PACKAGE_STATUS", 
    "get_package_status",
    "print_package_status",
    "setup_project"
]

# Add available module imports to exports
if utils_available:
    __all__.extend(["utils"])
if api_available:
    __all__.extend(["api"])
if bot_available:
    __all__.extend(["bot"])
if database_available:
    __all__.extend(["database"])
if indicators_available:
    __all__.extend(["indicators"])
if ml_available:
    __all__.extend(["ml"])

if __name__ == "__main__":
    print_package_status()

================================================================================
FILE: src\api\__init__.py
DIRECTORY: src\api
SIZE: 0 bytes
================================================================================



================================================================================
FILE: src\api\market_data.py
DIRECTORY: src\api
SIZE: 11035 bytes
================================================================================

"""
Market Data Handler
Real-time and historical market data management
"""

import asyncio
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import logging
from collections import deque
import json

from .quotex_api import quotex_api

logger = logging.getLogger(__name__)

class MarketDataManager:
    def __init__(self, max_history_length: int = 1000):
        self.max_history_length = max_history_length
        self.candle_data: Dict[str, deque] = {}
        self.tick_data: Dict[str, deque] = {}
        self.subscribed_assets: set = set()
        self.data_callbacks: Dict[str, List] = {}
        
    async def initialize(self):
        """Initialize market data manager"""
        try:
            # Set up event handlers
            quotex_api.on_candle_update = self._on_candle_update
            logger.info("Market data manager initialized")
        except Exception as e:
            logger.error(f"Failed to initialize market data manager: {e}")
    
    async def subscribe_to_asset(self, asset: str, timeframe: int = 60):
        """Subscribe to real-time data for an asset"""
        try:
            if asset not in self.subscribed_assets:
                await quotex_api.subscribe_to_asset(asset, timeframe)
                self.subscribed_assets.add(asset)
                
                # Initialize data storage
                if asset not in self.candle_data:
                    self.candle_data[asset] = deque(maxlen=self.max_history_length)
                    self.tick_data[asset] = deque(maxlen=self.max_history_length)
                
                # Load historical data
                await self._load_historical_data(asset, timeframe)
                
                logger.info(f"Subscribed to {asset} with timeframe {timeframe}")
                
        except Exception as e:
            logger.error(f"Failed to subscribe to {asset}: {e}")
    
    async def unsubscribe_from_asset(self, asset: str):
        """Unsubscribe from real-time data for an asset"""
        try:
            if asset in self.subscribed_assets:
                await quotex_api.unsubscribe_from_asset(asset)
                self.subscribed_assets.remove(asset)
                logger.info(f"Unsubscribed from {asset}")
        except Exception as e:
            logger.error(f"Failed to unsubscribe from {asset}: {e}")
    
    async def _load_historical_data(self, asset: str, timeframe: int, count: int = 100):
        """Load historical candle data"""
        try:
            candles = await quotex_api.get_candles(asset, timeframe, count)
            
            for candle in candles:
                processed_candle = self._process_candle(candle)
                self.candle_data[asset].append(processed_candle)
            
            logger.info(f"Loaded {len(candles)} historical candles for {asset}")
            
        except Exception as e:
            logger.error(f"Failed to load historical data for {asset}: {e}")
    
    async def _on_candle_update(self, data: dict):
        """Handle real-time candle updates"""
        try:
            asset = data.get('asset')
            if asset and asset in self.subscribed_assets:
                processed_candle = self._process_candle(data)
                self.candle_data[asset].append(processed_candle)
                
                # Trigger callbacks
                if asset in self.data_callbacks:
                    for callback in self.data_callbacks[asset]:
                        await callback(asset, processed_candle)
                        
        except Exception as e:
            logger.error(f"Error processing candle update: {e}")
    
    def _process_candle(self, candle_data: dict) -> dict:
        """Process raw candle data"""
        return {
            'timestamp': candle_data.get('timestamp', datetime.now().timestamp()),
            'open': float(candle_data.get('open', 0)),
            'high': float(candle_data.get('high', 0)),
            'low': float(candle_data.get('low', 0)),
            'close': float(candle_data.get('close', 0)),
            'volume': float(candle_data.get('volume', 0)),
            'asset': candle_data.get('asset', ''),
            'timeframe': candle_data.get('timeframe', 60)
        }
    
    def get_candle_data(self, asset: str, count: int = 100) -> pd.DataFrame:
        """Get candle data as DataFrame"""
        try:
            if asset not in self.candle_data:
                return pd.DataFrame()
            
            candles = list(self.candle_data[asset])[-count:]
            if not candles:
                return pd.DataFrame()
            
            df = pd.DataFrame(candles)
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
            df.set_index('timestamp', inplace=True)
            
            return df
            
        except Exception as e:
            logger.error(f"Failed to get candle data for {asset}: {e}")
            return pd.DataFrame()
    
    def get_latest_price(self, asset: str) -> float:
        """Get latest price for an asset"""
        try:
            if asset in self.candle_data and self.candle_data[asset]:
                return self.candle_data[asset][-1]['close']
            return 0.0
        except Exception as e:
            logger.error(f"Failed to get latest price for {asset}: {e}")
            return 0.0
    
    def get_price_change(self, asset: str, periods: int = 1) -> float:
        """Get price change over specified periods"""
        try:
            if asset not in self.candle_data or len(self.candle_data[asset]) < periods + 1:
                return 0.0
            
            current_price = self.candle_data[asset][-1]['close']
            previous_price = self.candle_data[asset][-periods-1]['close']
            
            return ((current_price - previous_price) / previous_price) * 100
            
        except Exception as e:
            logger.error(f"Failed to get price change for {asset}: {e}")
            return 0.0
    
    def get_volatility(self, asset: str, periods: int = 20) -> float:
        """Calculate volatility for an asset"""
        try:
            df = self.get_candle_data(asset, periods)
            if df.empty:
                return 0.0
            
            returns = df['close'].pct_change().dropna()
            volatility = returns.std() * np.sqrt(periods)
            
            return volatility
            
        except Exception as e:
            logger.error(f"Failed to calculate volatility for {asset}: {e}")
            return 0.0
    
    def get_support_resistance(self, asset: str, periods: int = 50) -> Tuple[float, float]:
        """Calculate support and resistance levels"""
        try:
            df = self.get_candle_data(asset, periods)
            if df.empty:
                return 0.0, 0.0
            
            # Simple support/resistance calculation
            support = df['low'].rolling(window=10).min().iloc[-1]
            resistance = df['high'].rolling(window=10).max().iloc[-1]
            
            return support, resistance
            
        except Exception as e:
            logger.error(f"Failed to calculate support/resistance for {asset}: {e}")
            return 0.0, 0.0
    
    def add_data_callback(self, asset: str, callback):
        """Add callback for real-time data updates"""
        if asset not in self.data_callbacks:
            self.data_callbacks[asset] = []
        self.data_callbacks[asset].append(callback)
    
    def remove_data_callback(self, asset: str, callback):
        """Remove callback for real-time data updates"""
        if asset in self.data_callbacks:
            try:
                self.data_callbacks[asset].remove(callback)
            except ValueError:
                pass
    
    def get_market_summary(self) -> dict:
        """Get market summary for all subscribed assets"""
        summary = {}
        
        for asset in self.subscribed_assets:
            try:
                latest_price = self.get_latest_price(asset)
                price_change = self.get_price_change(asset)
                volatility = self.get_volatility(asset)
                support, resistance = self.get_support_resistance(asset)
                
                summary[asset] = {
                    'price': latest_price,
                    'change_pct': price_change,
                    'volatility': volatility,
                    'support': support,
                    'resistance': resistance,
                    'data_points': len(self.candle_data.get(asset, []))
                }
                
            except Exception as e:
                logger.error(f"Failed to get summary for {asset}: {e}")
                summary[asset] = {
                    'price': 0.0,
                    'change_pct': 0.0,
                    'volatility': 0.0,
                    'support': 0.0,
                    'resistance': 0.0,
                    'data_points': 0
                }
        
        return summary
    
    def export_data(self, asset: str, filepath: str):
        """Export asset data to CSV"""
        try:
            df = self.get_candle_data(asset)
            if not df.empty:
                df.to_csv(filepath)
                logger.info(f"Exported {asset} data to {filepath}")
            else:
                logger.warning(f"No data available for {asset}")
        except Exception as e:
            logger.error(f"Failed to export data for {asset}: {e}")
    
    def clear_data(self, asset: str = None):
        """Clear data for specific asset or all assets"""
        try:
            if asset:
                if asset in self.candle_data:
                    self.candle_data[asset].clear()
                if asset in self.tick_data:
                    self.tick_data[asset].clear()
            else:
                self.candle_data.clear()
                self.tick_data.clear()
            
            logger.info(f"Cleared data for {asset if asset else 'all assets'}")
            
        except Exception as e:
            logger.error(f"Failed to clear data: {e}")
    
    async def cleanup(self):
        """Cleanup resources"""
        try:
            # Unsubscribe from all assets
            for asset in list(self.subscribed_assets):
                await self.unsubscribe_from_asset(asset)
            
            # Clear all data
            self.clear_data()
            
            logger.info("Market data manager cleaned up")
            
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")

# Singleton instance
market_data_manager = MarketDataManager()

================================================================================
FILE: src\api\quotex_api.py
DIRECTORY: src\api
SIZE: 11763 bytes
================================================================================

"""
Quotex API Interface
Complete API wrapper for Quotex trading platform
"""

import asyncio
import json
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import aiohttp
import websockets
from urllib.parse import urlencode
import hashlib
import hmac
import base64

from config.credentials import get_quotex_credentials, QUOTEX_CONFIG, TRADING_MODE

logger = logging.getLogger(__name__)

class QuotexAPI:
    def __init__(self):
        self.session = None
        self.ws_connection = None
        self.is_connected = False
        self.is_authenticated = False
        self.credentials = get_quotex_credentials()
        self.user_id = None
        self.session_token = None
        self.balance = 0
        self.demo_mode = TRADING_MODE['demo']
        
        # Event handlers
        self.on_candle_update = None
        self.on_order_update = None
        self.on_balance_update = None
        
    async def connect(self) -> bool:
        """Connect to Quotex API"""
        try:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=QUOTEX_CONFIG['timeout'])
            )
            
            # Login to get session token
            login_success = await self._login()
            if not login_success:
                return False
            
            # Connect to WebSocket
            await self._connect_websocket()
            
            self.is_connected = True
            logger.info("Successfully connected to Quotex API")
            return True
            
        except Exception as e:
            logger.error(f"Failed to connect to Quotex API: {e}")
            return False
    
    async def _login(self) -> bool:
        """Login to Quotex platform"""
        try:
            login_data = {
                'email': self.credentials['email'],
                'password': self.credentials['password'],
                'platform': 'web'
            }
            
            async with self.session.post(
                f"{QUOTEX_CONFIG['api_url']}/login",
                json=login_data
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    self.session_token = data.get('token')
                    self.user_id = data.get('user_id')
                    self.is_authenticated = True
                    
                    # Set session headers
                    self.session.headers.update({
                        'Authorization': f'Bearer {self.session_token}',
                        'Content-Type': 'application/json'
                    })
                    
                    logger.info("Successfully authenticated with Quotex")
                    return True
                else:
                    logger.error(f"Login failed with status: {response.status}")
                    return False
                    
        except Exception as e:
            logger.error(f"Login error: {e}")
            return False
    
    async def _connect_websocket(self):
        """Connect to WebSocket for real-time data"""
        try:
            headers = {
                'Authorization': f'Bearer {self.session_token}'
            }
            
            self.ws_connection = await websockets.connect(
                QUOTEX_CONFIG['websocket_url'],
                extra_headers=headers
            )
            
            # Start listening for messages
            asyncio.create_task(self._ws_message_handler())
            
        except Exception as e:
            logger.error(f"WebSocket connection failed: {e}")
            raise
    
    async def _ws_message_handler(self):
        """Handle WebSocket messages"""
        try:
            async for message in self.ws_connection:
                data = json.loads(message)
                await self._process_ws_message(data)
                
        except websockets.exceptions.ConnectionClosed:
            logger.warning("WebSocket connection closed")
            self.is_connected = False
        except Exception as e:
            logger.error(f"WebSocket message handler error: {e}")
    
    async def _process_ws_message(self, data: dict):
        """Process WebSocket messages"""
        message_type = data.get('type')
        
        if message_type == 'candle':
            if self.on_candle_update:
                await self.on_candle_update(data)
        
        elif message_type == 'order':
            if self.on_order_update:
                await self.on_order_update(data)
        
        elif message_type == 'balance':
            self.balance = data.get('balance', 0)
            if self.on_balance_update:
                await self.on_balance_update(data)
    
    async def get_balance(self) -> float:
        """Get current account balance"""
        try:
            async with self.session.get(
                f"{QUOTEX_CONFIG['api_url']}/balance"
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    balance_type = 'demo' if self.demo_mode else 'live'
                    self.balance = data.get(balance_type, 0)
                    return self.balance
                return 0
        except Exception as e:
            logger.error(f"Failed to get balance: {e}")
            return 0
    
    async def get_candles(self, asset: str, timeframe: int, count: int = 100) -> List[dict]:
        """Get historical candle data"""
        try:
            params = {
                'asset': asset,
                'timeframe': timeframe,
                'count': count,
                'timestamp': int(time.time())
            }
            
            async with self.session.get(
                f"{QUOTEX_CONFIG['api_url']}/candles",
                params=params
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get('candles', [])
                return []
        except Exception as e:
            logger.error(f"Failed to get candles: {e}")
            return []
    
    async def place_order(self, asset: str, direction: str, amount: float, 
                         duration: int = 60) -> dict:
        """Place a binary options order"""
        try:
            order_data = {
                'asset': asset,
                'direction': direction,  # 'call' or 'put'
                'amount': amount,
                'duration': duration,
                'type': 'binary',
                'demo': self.demo_mode
            }
            
            async with self.session.post(
                f"{QUOTEX_CONFIG['api_url']}/orders",
                json=order_data
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    logger.info(f"Order placed: {data}")
                    return data
                else:
                    logger.error(f"Order placement failed: {response.status}")
                    return {}
        except Exception as e:
            logger.error(f"Failed to place order: {e}")
            return {}
    
    async def get_assets(self) -> List[dict]:
        """Get available trading assets"""
        try:
            async with self.session.get(
                f"{QUOTEX_CONFIG['api_url']}/assets"
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get('assets', [])
                return []
        except Exception as e:
            logger.error(f"Failed to get assets: {e}")
            return []
    
    async def get_orders(self, limit: int = 50) -> List[dict]:
        """Get order history"""
        try:
            params = {'limit': limit}
            
            async with self.session.get(
                f"{QUOTEX_CONFIG['api_url']}/orders",
                params=params
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get('orders', [])
                return []
        except Exception as e:
            logger.error(f"Failed to get orders: {e}")
            return []
    
    async def cancel_order(self, order_id: str) -> bool:
        """Cancel an active order"""
        try:
            async with self.session.delete(
                f"{QUOTEX_CONFIG['api_url']}/orders/{order_id}"
            ) as response:
                return response.status == 200
        except Exception as e:
            logger.error(f"Failed to cancel order: {e}")
            return False
    
    async def subscribe_to_asset(self, asset: str, timeframe: int = 60):
        """Subscribe to real-time asset data"""
        try:
            if self.ws_connection:
                subscription_data = {
                    'type': 'subscribe',
                    'asset': asset,
                    'timeframe': timeframe
                }
                await self.ws_connection.send(json.dumps(subscription_data))
        except Exception as e:
            logger.error(f"Failed to subscribe to asset: {e}")
    
    async def unsubscribe_from_asset(self, asset: str):
        """Unsubscribe from real-time asset data"""
        try:
            if self.ws_connection:
                subscription_data = {
                    'type': 'unsubscribe',
                    'asset': asset
                }
                await self.ws_connection.send(json.dumps(subscription_data))
        except Exception as e:
            logger.error(f"Failed to unsubscribe from asset: {e}")
    
    async def get_market_status(self) -> dict:
        """Get market status information"""
        try:
            async with self.session.get(
                f"{QUOTEX_CONFIG['api_url']}/market-status"
            ) as response:
                if response.status == 200:
                    return await response.json()
                return {}
        except Exception as e:
            logger.error(f"Failed to get market status: {e}")
            return {}
    
    async def disconnect(self):
        """Disconnect from Quotex API"""
        try:
            if self.ws_connection:
                await self.ws_connection.close()
            
            if self.session:
                await self.session.close()
            
            self.is_connected = False
            self.is_authenticated = False
            logger.info("Disconnected from Quotex API")
            
        except Exception as e:
            logger.error(f"Error during disconnect: {e}")
    
    def set_demo_mode(self, demo: bool):
        """Switch between demo and live trading"""
        self.demo_mode = demo
        logger.info(f"Trading mode set to: {'Demo' if demo else 'Live'}")
    
    def is_market_open(self, asset: str) -> bool:
        """Check if market is open for specific asset"""
        # This would typically check market hours
        # For now, return True (implement based on actual market hours)
        return True
    
    async def __aenter__(self):
        await self.connect()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.disconnect()

# Singleton instance
quotex_api = QuotexAPI()

================================================================================
FILE: src\bot\__init__.py
DIRECTORY: src\bot
SIZE: 0 bytes
================================================================================



================================================================================
FILE: src\bot\risk_manager.py
DIRECTORY: src\bot
SIZE: 21348 bytes
================================================================================

"""
Risk Management System for Quotex Trading Bot
Handles position sizing, stop-loss, drawdown protection, and risk metrics
"""

import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import numpy as np
from datetime import datetime, timedelta
import json

logger = logging.getLogger(__name__)

class RiskLevel(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    EXTREME = "extreme"

class TradeType(Enum):
    CALL = "call"
    PUT = "put"

@dataclass
class RiskMetrics:
    """Container for risk metrics"""
    current_drawdown: float = 0.0
    max_drawdown: float = 0.0
    win_rate: float = 0.0
    profit_factor: float = 0.0
    sharpe_ratio: float = 0.0
    current_streak: int = 0
    max_losing_streak: int = 0
    daily_pnl: float = 0.0
    weekly_pnl: float = 0.0
    monthly_pnl: float = 0.0
    var_95: float = 0.0  # Value at Risk 95%
    expected_shortfall: float = 0.0

@dataclass
class TradeRecord:
    """Individual trade record for risk calculation"""
    timestamp: datetime
    symbol: str
    trade_type: TradeType
    amount: float
    pnl: float
    duration: int  # in minutes
    win: bool

@dataclass
class RiskLimits:
    """Risk limits configuration"""
    max_position_size: float = 100.0  # Maximum position size
    max_daily_loss: float = 500.0     # Maximum daily loss
    max_weekly_loss: float = 2000.0   # Maximum weekly loss
    max_monthly_loss: float = 5000.0  # Maximum monthly loss
    max_drawdown_percent: float = 20.0  # Maximum drawdown percentage
    max_consecutive_losses: int = 5   # Maximum consecutive losses
    max_risk_per_trade: float = 2.0   # Maximum risk per trade as % of balance
    min_balance: float = 1000.0       # Minimum balance to continue trading

class RiskManager:
    """
    Comprehensive risk management system for trading bot
    """
    
    def __init__(self, initial_balance: float = 10000.0, config: Dict = None):
        self.initial_balance = initial_balance
        self.current_balance = initial_balance
        self.peak_balance = initial_balance
        
        # Risk limits
        self.risk_limits = RiskLimits()
        if config:
            self._load_config(config)
        
        # Trade history
        self.trade_history: List[TradeRecord] = []
        
        # Risk metrics
        self.risk_metrics = RiskMetrics()
        
        # Internal state
        self.consecutive_losses = 0
        self.consecutive_wins = 0
        self.daily_trades = 0
        self.last_trade_date = None
        self.trading_enabled = True
        
        # Emergency stop conditions
        self.emergency_stop = False
        self.emergency_reason = ""
        
        logger.info(f"Risk Manager initialized with balance: ${initial_balance}")
    
    def _load_config(self, config: Dict):
        """Load risk configuration from dictionary"""
        risk_config = config.get('risk_management', {})
        
        for key, value in risk_config.items():
            if hasattr(self.risk_limits, key):
                setattr(self.risk_limits, key, value)
    
    def calculate_position_size(self, 
                              symbol: str, 
                              entry_price: float, 
                              stop_loss: float, 
                              risk_percent: float = None) -> float:
        """
        Calculate optimal position size based on risk management rules
        
        Args:
            symbol: Trading symbol
            entry_price: Entry price for the trade
            stop_loss: Stop loss price
            risk_percent: Risk percentage (optional, uses default if None)
        
        Returns:
            Calculated position size
        """
        if not self.trading_enabled or self.emergency_stop:
            return 0.0
        
        # Use default risk percentage if not provided
        if risk_percent is None:
            risk_percent = self.risk_limits.max_risk_per_trade
        
        # Calculate risk per trade in dollars
        risk_amount = self.current_balance * (risk_percent / 100)
        
        # For binary options, position size is the investment amount
        # Risk is typically 80-90% of investment (loss) vs 70-80% profit
        # Assuming 85% loss rate for calculation
        position_size = risk_amount / 0.85
        
        # Apply maximum position size limit
        position_size = min(position_size, self.risk_limits.max_position_size)
        
        # Ensure minimum viable position
        position_size = max(position_size, 1.0)
        
        # Additional risk checks
        position_size = self._apply_risk_adjustments(position_size)
        
        logger.debug(f"Calculated position size: ${position_size:.2f} for {symbol}")
        return position_size
    
    def _apply_risk_adjustments(self, base_position_size: float) -> float:
        """Apply additional risk adjustments based on current conditions"""
        
        adjusted_size = base_position_size
        
        # Reduce size based on consecutive losses
        if self.consecutive_losses > 0:
            reduction_factor = 1 - (self.consecutive_losses * 0.1)
            reduction_factor = max(0.3, reduction_factor)  # Minimum 30% of base size
            adjusted_size *= reduction_factor
        
        # Reduce size based on current drawdown
        if self.risk_metrics.current_drawdown > 10:
            drawdown_factor = 1 - (self.risk_metrics.current_drawdown / 100)
            adjusted_size *= max(0.2, drawdown_factor)
        
        # Reduce size based on daily performance
        if self.risk_metrics.daily_pnl < -100:  # If losing more than $100 today
            adjusted_size *= 0.5
        
        # Increase size after winning streaks (with caution)
        if self.consecutive_wins > 3:
            boost_factor = min(1.2, 1 + (self.consecutive_wins * 0.05))
            adjusted_size *= boost_factor
        
        return adjusted_size
    
    def can_place_trade(self, position_size: float, symbol: str = None) -> Tuple[bool, str]:
        """
        Check if a trade can be placed based on risk limits
        
        Args:
            position_size: Proposed position size
            symbol: Trading symbol (optional)
        
        Returns:
            Tuple of (can_trade, reason)
        """
        if self.emergency_stop:
            return False, f"Emergency stop active: {self.emergency_reason}"
        
        if not self.trading_enabled:
            return False, "Trading disabled by risk manager"
        
        # Check balance
        if self.current_balance < self.risk_limits.min_balance:
            return False, f"Balance below minimum: ${self.current_balance:.2f}"
        
        # Check position size
        if position_size > self.risk_limits.max_position_size:
            return False, f"Position size too large: ${position_size:.2f}"
        
        # Check daily loss limit
        if abs(self.risk_metrics.daily_pnl) > self.risk_limits.max_daily_loss:
            return False, f"Daily loss limit exceeded: ${abs(self.risk_metrics.daily_pnl):.2f}"
        
        # Check consecutive losses
        if self.consecutive_losses >= self.risk_limits.max_consecutive_losses:
            return False, f"Maximum consecutive losses reached: {self.consecutive_losses}"
        
        # Check drawdown
        if self.risk_metrics.current_drawdown > self.risk_limits.max_drawdown_percent:
            return False, f"Maximum drawdown exceeded: {self.risk_metrics.current_drawdown:.2f}%"
        
        # Check if sufficient balance for trade
        if position_size > self.current_balance:
            return False, f"Insufficient balance: ${self.current_balance:.2f}"
        
        return True, "Trade approved"
    
    def record_trade(self, 
                    symbol: str, 
                    trade_type: TradeType, 
                    amount: float, 
                    pnl: float, 
                    duration: int = 60) -> None:
        """
        Record a completed trade and update risk metrics
        
        Args:
            symbol: Trading symbol
            trade_type: Type of trade (CALL/PUT)
            amount: Position size
            pnl: Profit/Loss from the trade
            duration: Trade duration in minutes
        """
        # Create trade record
        trade = TradeRecord(
            timestamp=datetime.now(),
            symbol=symbol,
            trade_type=trade_type,
            amount=amount,
            pnl=pnl,
            duration=duration,
            win=pnl > 0
        )
        
        # Add to history
        self.trade_history.append(trade)
        
        # Update balance
        self.current_balance += pnl
        
        # Update peak balance
        if self.current_balance > self.peak_balance:
            self.peak_balance = self.current_balance
        
        # Update streaks
        if pnl > 0:
            self.consecutive_wins += 1
            self.consecutive_losses = 0
        else:
            self.consecutive_losses += 1
            self.consecutive_wins = 0
        
        # Update daily trade count
        current_date = datetime.now().date()
        if self.last_trade_date != current_date:
            self.daily_trades = 0
            self.last_trade_date = current_date
        self.daily_trades += 1
        
        # Recalculate risk metrics
        self._update_risk_metrics()
        
        # Check for emergency conditions
        self._check_emergency_conditions()
        
        logger.info(f"Trade recorded: {symbol} {trade_type.value} ${amount:.2f} PnL: ${pnl:.2f}")
    
    def _update_risk_metrics(self) -> None:
        """Update all risk metrics based on trade history"""
        if not self.trade_history:
            return
        
        # Calculate drawdown
        self.risk_metrics.current_drawdown = ((self.peak_balance - self.current_balance) / self.peak_balance) * 100
        
        # Calculate maximum drawdown
        peak = self.initial_balance
        max_dd = 0
        for trade in self.trade_history:
            balance = peak + sum(t.pnl for t in self.trade_history[:self.trade_history.index(trade)+1])
            if balance > peak:
                peak = balance
            drawdown = ((peak - balance) / peak) * 100
            if drawdown > max_dd:
                max_dd = drawdown
        self.risk_metrics.max_drawdown = max_dd
        
        # Calculate win rate
        wins = sum(1 for trade in self.trade_history if trade.win)
        total_trades = len(self.trade_history)
        self.risk_metrics.win_rate = (wins / total_trades) * 100 if total_trades > 0 else 0
        
        # Calculate profit factor
        gross_profit = sum(trade.pnl for trade in self.trade_history if trade.pnl > 0)
        gross_loss = abs(sum(trade.pnl for trade in self.trade_history if trade.pnl < 0))
        self.risk_metrics.profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')
        
        # Calculate current streak
        self.risk_metrics.current_streak = max(self.consecutive_wins, -self.consecutive_losses)
        
        # Calculate max losing streak
        max_losing_streak = 0
        current_losing_streak = 0
        for trade in self.trade_history:
            if not trade.win:
                current_losing_streak += 1
                max_losing_streak = max(max_losing_streak, current_losing_streak)
            else:
                current_losing_streak = 0
        self.risk_metrics.max_losing_streak = max_losing_streak
        
        # Calculate period PnL
        self._calculate_period_pnl()
        
        # Calculate VaR and Expected Shortfall
        self._calculate_var_metrics()
    
    def _calculate_period_pnl(self) -> None:
        """Calculate PnL for different time periods"""
        now = datetime.now()
        
        # Daily PnL
        daily_trades = [t for t in self.trade_history if t.timestamp.date() == now.date()]
        self.risk_metrics.daily_pnl = sum(t.pnl for t in daily_trades)
        
        # Weekly PnL
        week_start = now - timedelta(days=7)
        weekly_trades = [t for t in self.trade_history if t.timestamp >= week_start]
        self.risk_metrics.weekly_pnl = sum(t.pnl for t in weekly_trades)
        
        # Monthly PnL
        month_start = now - timedelta(days=30)
        monthly_trades = [t for t in self.trade_history if t.timestamp >= month_start]
        self.risk_metrics.monthly_pnl = sum(t.pnl for t in monthly_trades)
    
    def _calculate_var_metrics(self) -> None:
        """Calculate Value at Risk and Expected Shortfall"""
        if len(self.trade_history) < 20:  # Need sufficient data
            return
        
        # Get daily returns
        daily_returns = []
        current_date = None
        daily_pnl = 0
        
        for trade in sorted(self.trade_history, key=lambda t: t.timestamp):
            trade_date = trade.timestamp.date()
            if current_date != trade_date:
                if current_date is not None:
                    daily_returns.append(daily_pnl)
                current_date = trade_date
                daily_pnl = trade.pnl
            else:
                daily_pnl += trade.pnl
        
        if daily_pnl != 0:  # Add last day
            daily_returns.append(daily_pnl)
        
        if len(daily_returns) < 10:
            return
        
        # Calculate VaR (95% confidence)
        sorted_returns = sorted(daily_returns)
        var_index = int(len(sorted_returns) * 0.05)
        self.risk_metrics.var_95 = abs(sorted_returns[var_index])
        
        # Calculate Expected Shortfall (average of worst 5%)
        worst_returns = sorted_returns[:var_index+1]
        self.risk_metrics.expected_shortfall = abs(np.mean(worst_returns)) if worst_returns else 0
    
    def _check_emergency_conditions(self) -> None:
        """Check for emergency stop conditions"""
        
        # Emergency stop if balance is too low
        if self.current_balance < self.risk_limits.min_balance:
            self.emergency_stop = True
            self.emergency_reason = f"Balance below minimum: ${self.current_balance:.2f}"
            self.trading_enabled = False
            return
        
        # Emergency stop if drawdown is too high
        if self.risk_metrics.current_drawdown > self.risk_limits.max_drawdown_percent:
            self.emergency_stop = True
            self.emergency_reason = f"Maximum drawdown exceeded: {self.risk_metrics.current_drawdown:.2f}%"
            self.trading_enabled = False
            return
        
        # Emergency stop if daily loss is too high
        if abs(self.risk_metrics.daily_pnl) > self.risk_limits.max_daily_loss:
            self.emergency_stop = True
            self.emergency_reason = f"Daily loss limit exceeded: ${abs(self.risk_metrics.daily_pnl):.2f}"
            self.trading_enabled = False
            return
        
        # Emergency stop if too many consecutive losses
        if self.consecutive_losses >= self.risk_limits.max_consecutive_losses:
            self.emergency_stop = True
            self.emergency_reason = f"Maximum consecutive losses: {self.consecutive_losses}"
            self.trading_enabled = False
            return
    
    def get_risk_level(self) -> RiskLevel:
        """Determine current risk level based on metrics"""
        
        risk_score = 0
        
        # Drawdown factor
        if self.risk_metrics.current_drawdown > 15:
            risk_score += 3
        elif self.risk_metrics.current_drawdown > 10:
            risk_score += 2
        elif self.risk_metrics.current_drawdown > 5:
            risk_score += 1
        
        # Consecutive losses factor
        if self.consecutive_losses > 4:
            risk_score += 3
        elif self.consecutive_losses > 2:
            risk_score += 2
        elif self.consecutive_losses > 0:
            risk_score += 1
        
        # Win rate factor
        if self.risk_metrics.win_rate < 40:
            risk_score += 2
        elif self.risk_metrics.win_rate < 50:
            risk_score += 1
        
        # Daily PnL factor
        if self.risk_metrics.daily_pnl < -200:
            risk_score += 2
        elif self.risk_metrics.daily_pnl < -100:
            risk_score += 1
        
        # Determine risk level
        if risk_score >= 8:
            return RiskLevel.EXTREME
        elif risk_score >= 5:
            return RiskLevel.HIGH
        elif risk_score >= 3:
            return RiskLevel.MEDIUM
        else:
            return RiskLevel.LOW
    
    def get_risk_report(self) -> Dict:
        """Generate comprehensive risk report"""
        
        return {
            "balance": {
                "current": self.current_balance,
                "initial": self.initial_balance,
                "peak": self.peak_balance,
                "total_return": ((self.current_balance - self.initial_balance) / self.initial_balance) * 100
            },
            "risk_metrics": {
                "current_drawdown": self.risk_metrics.current_drawdown,
                "max_drawdown": self.risk_metrics.max_drawdown,
                "win_rate": self.risk_metrics.win_rate,
                "profit_factor": self.risk_metrics.profit_factor,
                "current_streak": self.risk_metrics.current_streak,
                "max_losing_streak": self.risk_metrics.max_losing_streak,
                "var_95": self.risk_metrics.var_95,
                "expected_shortfall": self.risk_metrics.expected_shortfall
            },
            "period_pnl": {
                "daily": self.risk_metrics.daily_pnl,
                "weekly": self.risk_metrics.weekly_pnl,
                "monthly": self.risk_metrics.monthly_pnl
            },
            "trading_status": {
                "enabled": self.trading_enabled,
                "emergency_stop": self.emergency_stop,
                "emergency_reason": self.emergency_reason,
                "risk_level": self.get_risk_level().value,
                "consecutive_losses": self.consecutive_losses,
                "consecutive_wins": self.consecutive_wins,
                "daily_trades": self.daily_trades
            },
            "limits": {
                "max_position_size": self.risk_limits.max_position_size,
                "max_daily_loss": self.risk_limits.max_daily_loss,
                "max_drawdown_percent": self.risk_limits.max_drawdown_percent,
                "max_consecutive_losses": self.risk_limits.max_consecutive_losses,
                "max_risk_per_trade": self.risk_limits.max_risk_per_trade
            }
        }
    
    def reset_emergency_stop(self) -> bool:
        """Reset emergency stop if conditions are met"""
        
        if not self.emergency_stop:
            return True
        
        # Check if conditions have improved
        if (self.current_balance >= self.risk_limits.min_balance and
            self.risk_metrics.current_drawdown <= self.risk_limits.max_drawdown_percent and
            self.consecutive_losses < self.risk_limits.max_consecutive_losses):
            
            self.emergency_stop = False
            self.emergency_reason = ""
            self.trading_enabled = True
            
            logger.info("Emergency stop reset - trading enabled")
            return True
        
        return False
    
    def adjust_risk_limits(self, new_limits: Dict) -> None:
        """Adjust risk limits dynamically"""
        
        for key, value in new_limits.items():
            if hasattr(self.risk_limits, key):
                setattr(self.risk_limits, key, value)
                logger.info(f"Risk limit updated: {key} = {value}")
    
    def export_trade_history(self, filename: str = None) -> str:
        """Export trade history to JSON file"""
        
        if filename is None:
            filename = f"trade_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        trade_data = []
        for trade in self.trade_history:
            trade_data.append({
                "timestamp": trade.timestamp.isoformat(),
                "symbol": trade.symbol,
                "trade_type": trade.trade_type.value,
                "amount": trade.amount,
                "pnl": trade.pnl,
                "duration": trade.duration,
                "win": trade.win
            })
        
        export_data = {
            "trade_history": trade_data,
            "risk_report": self.get_risk_report()
        }
        
        with open(filename, 'w') as f:
            json.dump(export_data, f, indent=2)
        
        logger.info(f"Trade history exported to {filename}")
        return filename

================================================================================
FILE: src\bot\strategy_manager.py
DIRECTORY: src\bot
SIZE: 21003 bytes
================================================================================

"""
Strategy Manager for Quotex Trading Bot
Handles multiple trading strategies, strategy selection, and performance tracking
"""

import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import pandas as pd
import numpy as np
from abc import ABC, abstractmethod

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SignalType(Enum):
    """Trading signal types"""
    BUY = "buy"
    SELL = "sell"
    HOLD = "hold"
    STRONG_BUY = "strong_buy"
    STRONG_SELL = "strong_sell"


class TimeFrame(Enum):
    """Trading timeframes"""
    M1 = "1m"
    M5 = "5m"
    M15 = "15m"
    M30 = "30m"
    H1 = "1h"
    H4 = "4h"
    D1 = "1d"


@dataclass
class TradingSignal:
    """Trading signal data structure"""
    signal_type: SignalType
    confidence: float  # 0.0 to 1.0
    entry_price: float
    stop_loss: Optional[float] = None
    take_profit: Optional[float] = None
    timeframe: TimeFrame = TimeFrame.M5
    timestamp: datetime = field(default_factory=datetime.now)
    strategy_name: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class StrategyPerformance:
    """Strategy performance metrics"""
    total_trades: int = 0
    winning_trades: int = 0
    losing_trades: int = 0
    total_profit: float = 0.0
    total_loss: float = 0.0
    win_rate: float = 0.0
    profit_factor: float = 0.0
    max_drawdown: float = 0.0
    sharpe_ratio: float = 0.0
    last_updated: datetime = field(default_factory=datetime.now)


class BaseStrategy(ABC):
    """Base class for all trading strategies"""
    
    def __init__(self, name: str, timeframe: TimeFrame = TimeFrame.M5):
        self.name = name
        self.timeframe = timeframe
        self.enabled = True
        self.performance = StrategyPerformance()
        self.parameters = {}
        
    @abstractmethod
    def generate_signal(self, data: pd.DataFrame) -> Optional[TradingSignal]:
        """Generate trading signal based on market data"""
        pass
    
    @abstractmethod
    def update_parameters(self, **kwargs):
        """Update strategy parameters"""
        pass
    
    def calculate_performance(self, trades: List[Dict]) -> StrategyPerformance:
        """Calculate strategy performance metrics"""
        if not trades:
            return self.performance
        
        total_trades = len(trades)
        winning_trades = sum(1 for trade in trades if trade.get('profit', 0) > 0)
        losing_trades = total_trades - winning_trades
        
        total_profit = sum(trade.get('profit', 0) for trade in trades if trade.get('profit', 0) > 0)
        total_loss = abs(sum(trade.get('profit', 0) for trade in trades if trade.get('profit', 0) < 0))
        
        win_rate = winning_trades / total_trades if total_trades > 0 else 0
        profit_factor = total_profit / total_loss if total_loss > 0 else float('inf')
        
        # Calculate drawdown
        cumulative_pnl = np.cumsum([trade.get('profit', 0) for trade in trades])
        running_max = np.maximum.accumulate(cumulative_pnl)
        drawdown = (cumulative_pnl - running_max) / running_max
        max_drawdown = np.min(drawdown) if len(drawdown) > 0 else 0
        
        self.performance = StrategyPerformance(
            total_trades=total_trades,
            winning_trades=winning_trades,
            losing_trades=losing_trades,
            total_profit=total_profit,
            total_loss=total_loss,
            win_rate=win_rate,
            profit_factor=profit_factor,
            max_drawdown=max_drawdown,
            last_updated=datetime.now()
        )
        
        return self.performance


class MovingAverageStrategy(BaseStrategy):
    """Simple Moving Average crossover strategy"""
    
    def __init__(self, fast_period: int = 10, slow_period: int = 20):
        super().__init__("Moving Average Crossover", TimeFrame.M5)
        self.parameters = {
            'fast_period': fast_period,
            'slow_period': slow_period,
            'min_confidence': 0.6
        }
    
    def generate_signal(self, data: pd.DataFrame) -> Optional[TradingSignal]:
        """Generate MA crossover signal"""
        if len(data) < self.parameters['slow_period']:
            return None
        
        # Calculate moving averages
        fast_ma = data['close'].rolling(window=self.parameters['fast_period']).mean()
        slow_ma = data['close'].rolling(window=self.parameters['slow_period']).mean()
        
        # Get current and previous values
        current_fast = fast_ma.iloc[-1]
        current_slow = slow_ma.iloc[-1]
        prev_fast = fast_ma.iloc[-2]
        prev_slow = slow_ma.iloc[-2]
        
        current_price = data['close'].iloc[-1]
        
        # Check for crossover
        if prev_fast <= prev_slow and current_fast > current_slow:
            # Bullish crossover
            confidence = min(abs(current_fast - current_slow) / current_slow, 1.0)
            if confidence >= self.parameters['min_confidence']:
                return TradingSignal(
                    signal_type=SignalType.BUY,
                    confidence=confidence,
                    entry_price=current_price,
                    strategy_name=self.name,
                    timeframe=self.timeframe
                )
        
        elif prev_fast >= prev_slow and current_fast < current_slow:
            # Bearish crossover
            confidence = min(abs(current_fast - current_slow) / current_slow, 1.0)
            if confidence >= self.parameters['min_confidence']:
                return TradingSignal(
                    signal_type=SignalType.SELL,
                    confidence=confidence,
                    entry_price=current_price,
                    strategy_name=self.name,
                    timeframe=self.timeframe
                )
        
        return None
    
    def update_parameters(self, **kwargs):
        """Update strategy parameters"""
        for key, value in kwargs.items():
            if key in self.parameters:
                self.parameters[key] = value


class RSIStrategy(BaseStrategy):
    """RSI-based trading strategy"""
    
    def __init__(self, rsi_period: int = 14):
        super().__init__("RSI Strategy", TimeFrame.M5)
        self.parameters = {
            'rsi_period': rsi_period,
            'overbought_level': 70,
            'oversold_level': 30,
            'min_confidence': 0.5
        }
    
    def generate_signal(self, data: pd.DataFrame) -> Optional[TradingSignal]:
        """Generate RSI-based signal"""
        if len(data) < self.parameters['rsi_period'] + 1:
            return None
        
        # Calculate RSI
        rsi = self._calculate_rsi(data['close'], self.parameters['rsi_period'])
        
        current_rsi = rsi.iloc[-1]
        current_price = data['close'].iloc[-1]
        
        # Generate signals
        if current_rsi <= self.parameters['oversold_level']:
            # Oversold condition - potential buy
            confidence = (self.parameters['oversold_level'] - current_rsi) / self.parameters['oversold_level']
            if confidence >= self.parameters['min_confidence']:
                return TradingSignal(
                    signal_type=SignalType.BUY,
                    confidence=confidence,
                    entry_price=current_price,
                    strategy_name=self.name,
                    timeframe=self.timeframe,
                    metadata={'rsi': current_rsi}
                )
        
        elif current_rsi >= self.parameters['overbought_level']:
            # Overbought condition - potential sell
            confidence = (current_rsi - self.parameters['overbought_level']) / (100 - self.parameters['overbought_level'])
            if confidence >= self.parameters['min_confidence']:
                return TradingSignal(
                    signal_type=SignalType.SELL,
                    confidence=confidence,
                    entry_price=current_price,
                    strategy_name=self.name,
                    timeframe=self.timeframe,
                    metadata={'rsi': current_rsi}
                )
        
        return None
    
    def _calculate_rsi(self, prices: pd.Series, period: int) -> pd.Series:
        """Calculate RSI indicator"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def update_parameters(self, **kwargs):
        """Update strategy parameters"""
        for key, value in kwargs.items():
            if key in self.parameters:
                self.parameters[key] = value


class BollingerBandsStrategy(BaseStrategy):
    """Bollinger Bands strategy"""
    
    def __init__(self, bb_period: int = 20, bb_std: float = 2.0):
        super().__init__("Bollinger Bands", TimeFrame.M5)
        self.parameters = {
            'bb_period': bb_period,
            'bb_std': bb_std,
            'min_confidence': 0.6
        }
    
    def generate_signal(self, data: pd.DataFrame) -> Optional[TradingSignal]:
        """Generate Bollinger Bands signal"""
        if len(data) < self.parameters['bb_period']:
            return None
        
        # Calculate Bollinger Bands
        sma = data['close'].rolling(window=self.parameters['bb_period']).mean()
        std = data['close'].rolling(window=self.parameters['bb_period']).std()
        
        upper_band = sma + (std * self.parameters['bb_std'])
        lower_band = sma - (std * self.parameters['bb_std'])
        
        current_price = data['close'].iloc[-1]
        current_upper = upper_band.iloc[-1]
        current_lower = lower_band.iloc[-1]
        current_sma = sma.iloc[-1]
        
        # Generate signals
        if current_price <= current_lower:
            # Price touched lower band - potential buy
            confidence = (current_lower - current_price) / (current_sma - current_lower)
            confidence = min(confidence, 1.0)
            if confidence >= self.parameters['min_confidence']:
                return TradingSignal(
                    signal_type=SignalType.BUY,
                    confidence=confidence,
                    entry_price=current_price,
                    strategy_name=self.name,
                    timeframe=self.timeframe,
                    metadata={
                        'upper_band': current_upper,
                        'lower_band': current_lower,
                        'sma': current_sma
                    }
                )
        
        elif current_price >= current_upper:
            # Price touched upper band - potential sell
            confidence = (current_price - current_upper) / (current_upper - current_sma)
            confidence = min(confidence, 1.0)
            if confidence >= self.parameters['min_confidence']:
                return TradingSignal(
                    signal_type=SignalType.SELL,
                    confidence=confidence,
                    entry_price=current_price,
                    strategy_name=self.name,
                    timeframe=self.timeframe,
                    metadata={
                        'upper_band': current_upper,
                        'lower_band': current_lower,
                        'sma': current_sma
                    }
                )
        
        return None
    
    def update_parameters(self, **kwargs):
        """Update strategy parameters"""
        for key, value in kwargs.items():
            if key in self.parameters:
                self.parameters[key] = value


class StrategyManager:
    """Main strategy manager class"""
    
    def __init__(self):
        self.strategies: Dict[str, BaseStrategy] = {}
        self.active_strategies: List[str] = []
        self.strategy_weights: Dict[str, float] = {}
        self.performance_history: Dict[str, List[StrategyPerformance]] = {}
        self.trade_history: Dict[str, List[Dict]] = {}
        
        # Initialize default strategies
        self._initialize_default_strategies()
        
        logger.info("Strategy Manager initialized")
    
    def _initialize_default_strategies(self):
        """Initialize default trading strategies"""
        # Add default strategies
        self.add_strategy(MovingAverageStrategy(fast_period=10, slow_period=20))
        self.add_strategy(RSIStrategy(rsi_period=14))
        self.add_strategy(BollingerBandsStrategy(bb_period=20, bb_std=2.0))
        
        # Set default weights
        self.strategy_weights = {
            "Moving Average Crossover": 0.4,
            "RSI Strategy": 0.3,
            "Bollinger Bands": 0.3
        }
        
        # Enable all strategies by default
        self.active_strategies = list(self.strategies.keys())
    
    def add_strategy(self, strategy: BaseStrategy):
        """Add a new trading strategy"""
        self.strategies[strategy.name] = strategy
        self.performance_history[strategy.name] = []
        self.trade_history[strategy.name] = []
        logger.info(f"Added strategy: {strategy.name}")
    
    def remove_strategy(self, strategy_name: str):
        """Remove a trading strategy"""
        if strategy_name in self.strategies:
            del self.strategies[strategy_name]
            if strategy_name in self.active_strategies:
                self.active_strategies.remove(strategy_name)
            if strategy_name in self.strategy_weights:
                del self.strategy_weights[strategy_name]
            logger.info(f"Removed strategy: {strategy_name}")
    
    def enable_strategy(self, strategy_name: str):
        """Enable a trading strategy"""
        if strategy_name in self.strategies:
            self.strategies[strategy_name].enabled = True
            if strategy_name not in self.active_strategies:
                self.active_strategies.append(strategy_name)
            logger.info(f"Enabled strategy: {strategy_name}")
    
    def disable_strategy(self, strategy_name: str):
        """Disable a trading strategy"""
        if strategy_name in self.strategies:
            self.strategies[strategy_name].enabled = False
            if strategy_name in self.active_strategies:
                self.active_strategies.remove(strategy_name)
            logger.info(f"Disabled strategy: {strategy_name}")
    
    def update_strategy_weights(self, weights: Dict[str, float]):
        """Update strategy weights"""
        # Normalize weights
        total_weight = sum(weights.values())
        if total_weight > 0:
            self.strategy_weights = {k: v / total_weight for k, v in weights.items()}
            logger.info(f"Updated strategy weights: {self.strategy_weights}")
    
    def generate_signals(self, data: pd.DataFrame) -> List[TradingSignal]:
        """Generate signals from all active strategies"""
        signals = []
        
        for strategy_name in self.active_strategies:
            strategy = self.strategies[strategy_name]
            if strategy.enabled:
                try:
                    signal = strategy.generate_signal(data)
                    if signal:
                        signals.append(signal)
                        logger.debug(f"Generated signal from {strategy_name}: {signal.signal_type}")
                except Exception as e:
                    logger.error(f"Error generating signal from {strategy_name}: {str(e)}")
        
        return signals
    
    def get_consensus_signal(self, data: pd.DataFrame) -> Optional[TradingSignal]:
        """Get consensus signal from all active strategies"""
        signals = self.generate_signals(data)
        
        if not signals:
            return None
        
        # Calculate weighted consensus
        buy_weight = 0.0
        sell_weight = 0.0
        total_confidence = 0.0
        
        for signal in signals:
            weight = self.strategy_weights.get(signal.strategy_name, 0.0)
            weighted_confidence = signal.confidence * weight
            
            if signal.signal_type in [SignalType.BUY, SignalType.STRONG_BUY]:
                buy_weight += weighted_confidence
            elif signal.signal_type in [SignalType.SELL, SignalType.STRONG_SELL]:
                sell_weight += weighted_confidence
            
            total_confidence += weighted_confidence
        
        # Determine consensus
        if buy_weight > sell_weight and buy_weight > 0.5:
            return TradingSignal(
                signal_type=SignalType.BUY,
                confidence=buy_weight,
                entry_price=data['close'].iloc[-1],
                strategy_name="Consensus",
                timeframe=TimeFrame.M5
            )
        elif sell_weight > buy_weight and sell_weight > 0.5:
            return TradingSignal(
                signal_type=SignalType.SELL,
                confidence=sell_weight,
                entry_price=data['close'].iloc[-1],
                strategy_name="Consensus",
                timeframe=TimeFrame.M5
            )
        
        return None
    
    def update_trade_result(self, strategy_name: str, trade_result: Dict):
        """Update trade result for a strategy"""
        if strategy_name in self.trade_history:
            self.trade_history[strategy_name].append(trade_result)
            
            # Update performance
            if strategy_name in self.strategies:
                performance = self.strategies[strategy_name].calculate_performance(
                    self.trade_history[strategy_name]
                )
                self.performance_history[strategy_name].append(performance)
                
                logger.info(f"Updated performance for {strategy_name}: "
                           f"Win Rate: {performance.win_rate:.2%}, "
                           f"Profit Factor: {performance.profit_factor:.2f}")
    
    def get_strategy_performance(self, strategy_name: str) -> Optional[StrategyPerformance]:
        """Get performance metrics for a strategy"""
        if strategy_name in self.strategies:
            return self.strategies[strategy_name].performance
        return None
    
    def get_all_performances(self) -> Dict[str, StrategyPerformance]:
        """Get performance metrics for all strategies"""
        performances = {}
        for name, strategy in self.strategies.items():
            performances[name] = strategy.performance
        return performances
    
    def optimize_weights(self, lookback_days: int = 30):
        """Optimize strategy weights based on recent performance"""
        current_time = datetime.now()
        cutoff_time = current_time - timedelta(days=lookback_days)
        
        strategy_scores = {}
        
        for strategy_name, strategy in self.strategies.items():
            # Get recent trades
            recent_trades = [
                trade for trade in self.trade_history.get(strategy_name, [])
                if trade.get('timestamp', current_time) >= cutoff_time
            ]
            
            if recent_trades:
                # Calculate performance score
                performance = strategy.calculate_performance(recent_trades)
                score = performance.win_rate * performance.profit_factor
                strategy_scores[strategy_name] = max(score, 0.1)  # Minimum weight
            else:
                strategy_scores[strategy_name] = 0.5  # Default weight
        
        # Normalize scores to weights
        if strategy_scores:
            self.update_strategy_weights(strategy_scores)
            logger.info("Optimized strategy weights based on recent performance")
    
    def get_status(self) -> Dict[str, Any]:
        """Get current status of strategy manager"""
        return {
            'total_strategies': len(self.strategies),
            'active_strategies': len(self.active_strategies),
            'strategy_weights': self.strategy_weights,
            'active_strategy_names': self.active_strategies,
            'performances': {name: {
                'win_rate': perf.win_rate,
                'profit_factor': perf.profit_factor,
                'total_trades': perf.total_trades
            } for name, perf in self.get_all_performances().items()}
        }

================================================================================
FILE: src\bot\trading_bot.py
DIRECTORY: src\bot
SIZE: 19732 bytes
================================================================================

"""
Advanced Quotex Trading Bot
Core trading bot implementation with AI-powered decision making
"""

import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
import pandas as pd
import numpy as np
from dataclasses import dataclass, asdict

from ..api.quotex_api import QuotexAPI
from ..api.market_data import MarketDataManager
from ..indicators.signal_generator import SignalGenerator
from ..ml.prediction_engine import PredictionEngine
from .strategy_manager import StrategyManager
from .risk_manager import RiskManager
from ..utils.logger import setup_logger
from ..utils.data_processor import DataProcessor
from ..database.database_manager import DatabaseManager
from config.settings import TRADING_CONFIG, RISK_CONFIG, ML_CONFIG

@dataclass
class TradeSignal:
    """Trade signal data structure"""
    asset: str
    direction: str  # 'CALL' or 'PUT'
    confidence: float
    entry_price: float
    expiry_time: int
    timestamp: datetime
    indicators: Dict[str, Any]
    ml_prediction: Optional[Dict[str, Any]] = None
    risk_score: float = 0.0
    
class TradingBot:
    """Advanced AI-powered trading bot for Quotex"""
    
    def __init__(self, db_manager: DatabaseManager):
        self.logger = setup_logger("TradingBot")
        self.db_manager = db_manager
        
        # Initialize components
        self.api = QuotexAPI()
        self.market_data = MarketDataManager()
        self.signal_generator = SignalGenerator()
        self.prediction_engine = PredictionEngine()
        self.strategy_manager = StrategyManager()
        self.risk_manager = RiskManager()
        self.data_processor = DataProcessor()
        
        # Bot state
        self.is_running = False
        self.is_trading = False
        self.active_trades: Dict[str, Dict] = {}
        self.daily_stats = {
            'trades_count': 0,
            'wins': 0,
            'losses': 0,
            'profit_loss': 0.0,
            'win_rate': 0.0
        }
        
        # Performance tracking
        self.account_balance = 0.0
        self.daily_start_balance = 0.0
        self.max_drawdown = 0.0
        self.peak_balance = 0.0
        
        # Trade history
        self.trade_history: List[Dict] = []
        
    async def initialize(self):
        """Initialize the trading bot"""
        try:
            self.logger.info("🔧 Initializing trading bot components...")
            
            # Initialize API connection
            await self.api.connect()
            
            # Initialize market data streams
            await self.market_data.initialize(TRADING_CONFIG["assets"])
            
            # Initialize ML prediction engine
            await self.prediction_engine.initialize()
            
            # Get account information
            account_info = await self.api.get_account_info()
            self.account_balance = account_info.get('balance', 0)
            self.daily_start_balance = self.account_balance
            self.peak_balance = self.account_balance
            
            self.logger.info(f"💰 Account Balance: ${self.account_balance:.2f}")
            self.logger.info("✅ Trading bot initialized successfully")
            
        except Exception as e:
            self.logger.error(f"❌ Bot initialization failed: {e}")
            raise
    
    async def start_trading(self):
        """Start the main trading loop"""
        if self.is_running:
            self.logger.warning("⚠️ Trading bot is already running")
            return
        
        self.is_running = True
        self.is_trading = True
        
        self.logger.info("🚀 Starting trading operations...")
        
        try:
            # Start market data streams
            await self.market_data.start_streams()
            
            # Start main trading loop
            await self._main_trading_loop()
            
        except Exception as e:
            self.logger.error(f"❌ Trading loop error: {e}")
        finally:
            self.is_running = False
            self.is_trading = False
    
    async def stop_trading(self):
        """Stop trading operations"""
        self.logger.info("🛑 Stopping trading operations...")
        self.is_trading = False
        
        # Close all active trades
        await self._close_all_trades()
        
        # Stop market data streams
        await self.market_data.stop_streams()
        
        # Disconnect API
        await self.api.disconnect()
        
        # Save final statistics
        await self._save_daily_stats()
        
        self.logger.info("✅ Trading stopped successfully")
    
    async def _main_trading_loop(self):
        """Main trading loop with signal generation and execution"""
        self.logger.info("🔄 Starting main trading loop...")
        
        while self.is_trading:
            try:
                # Check trading conditions
                if not await self._check_trading_conditions():
                    await asyncio.sleep(60)  # Wait 1 minute before next check
                    continue
                
                # Process each asset
                for asset in TRADING_CONFIG["assets"]:
                    if not self.is_trading:
                        break
                    
                    # Get latest market data
                    market_data = await self.market_data.get_latest_data(asset)
                    if market_data is None:
                        continue
                    
                    # Generate trading signal
                    signal = await self._generate_trading_signal(asset, market_data)
                    
                    if signal and signal.confidence >= TRADING_CONFIG["min_confidence"]:
                        # Execute trade
                        await self._execute_trade(signal)
                
                # Update bot statistics
                await self._update_statistics()
                
                # Check for trade exits
                await self._check_trade_exits()
                
                # Sleep before next iteration
                await asyncio.sleep(30)  # Check every 30 seconds
                
            except Exception as e:
                self.logger.error(f"❌ Error in trading loop: {e}")
                await asyncio.sleep(60)
    
    async def _generate_trading_signal(self, asset: str, market_data: pd.DataFrame) -> Optional[TradeSignal]:
        """Generate trading signal using technical analysis and ML predictions"""
        try:
            # Generate technical indicators
            indicators = await self.signal_generator.generate_indicators(market_data)
            
            # Get ML prediction
            ml_prediction = await self.prediction_engine.predict(asset, market_data)
            
            # Combine signals using strategy manager
            combined_signal = await self.strategy_manager.combine_signals(
                asset, indicators, ml_prediction
            )
            
            if combined_signal is None:
                return None
            
            # Calculate risk score
            risk_score = await self.risk_manager.calculate_risk_score(
                asset, combined_signal, market_data
            )
            
            # Create trade signal
            signal = TradeSignal(
                asset=asset,
                direction=combined_signal['direction'],
                confidence=combined_signal['confidence'],
                entry_price=market_data.iloc[-1]['close'],
                expiry_time=combined_signal['expiry_time'],
                timestamp=datetime.now(),
                indicators=indicators,
                ml_prediction=ml_prediction,
                risk_score=risk_score
            )
            
            self.logger.info(f"📊 Generated signal for {asset}: {signal.direction} "
                           f"(Confidence: {signal.confidence:.2f}, Risk: {risk_score:.2f})")
            
            return signal
            
        except Exception as e:
            self.logger.error(f"❌ Error generating signal for {asset}: {e}")
            return None
    
    async def _execute_trade(self, signal: TradeSignal):
        """Execute a trade based on the signal"""
        try:
            # Check if we can open a new trade
            if not await self._can_open_trade(signal):
                return
            
            # Calculate trade amount
            trade_amount = await self.risk_manager.calculate_trade_amount(
                self.account_balance, signal.risk_score
            )
            
            # Place the trade
            trade_result = await self.api.place_trade(
                asset=signal.asset,
                amount=trade_amount,
                direction=signal.direction,
                expiry_time=signal.expiry_time
            )
            
            if trade_result and trade_result.get('success'):
                trade_id = trade_result.get('trade_id')
                
                # Store trade information
                trade_info = {
                    'id': trade_id,
                    'asset': signal.asset,
                    'direction': signal.direction,
                    'amount': trade_amount,
                    'entry_price': signal.entry_price,
                    'entry_time': signal.timestamp,
                    'expiry_time': signal.expiry_time,
                    'confidence': signal.confidence,
                    'risk_score': signal.risk_score,
                    'status': 'open'
                }
                
                self.active_trades[trade_id] = trade_info
                
                # Update statistics
                self.daily_stats['trades_count'] += 1
                
                # Save to database
                await self.db_manager.save_trade(trade_info)
                
                self.logger.info(f"✅ Trade executed: {signal.asset} {signal.direction} "
                               f"${trade_amount:.2f} (ID: {trade_id})")
                
            else:
                self.logger.warning(f"⚠️ Trade execution failed: {trade_result}")
                
        except Exception as e:
            self.logger.error(f"❌ Error executing trade: {e}")
    
    async def _check_trade_exits(self):
        """Check for trade exits and update results"""
        completed_trades = []
        
        for trade_id, trade_info in self.active_trades.items():
            try:
                # Check if trade has expired
                if datetime.now() >= trade_info['expiry_time']:
                    # Get trade result
                    result = await self.api.get_trade_result(trade_id)
                    
                    if result:
                        # Update trade info
                        trade_info['exit_price'] = result.get('exit_price', 0)
                        trade_info['profit_loss'] = result.get('profit_loss', 0)
                        trade_info['status'] = result.get('status', 'closed')
                        trade_info['exit_time'] = datetime.now()
                        
                        # Update statistics
                        if trade_info['profit_loss'] > 0:
                            self.daily_stats['wins'] += 1
                        else:
                            self.daily_stats['losses'] += 1
                        
                        self.daily_stats['profit_loss'] += trade_info['profit_loss']
                        self.account_balance += trade_info['profit_loss']
                        
                        # Update win rate
                        total_trades = self.daily_stats['wins'] + self.daily_stats['losses']
                        if total_trades > 0:
                            self.daily_stats['win_rate'] = self.daily_stats['wins'] / total_trades
                        
                        # Add to trade history
                        self.trade_history.append(trade_info.copy())
                        
                        # Save to database
                        await self.db_manager.update_trade(trade_info)
                        
                        # Log result
                        result_emoji = "🟢" if trade_info['profit_loss'] > 0 else "🔴"
                        self.logger.info(f"{result_emoji} Trade closed: {trade_info['asset']} "
                                       f"P&L: ${trade_info['profit_loss']:.2f}")
                        
                        completed_trades.append(trade_id)
                        
            except Exception as e:
                self.logger.error(f"❌ Error checking trade {trade_id}: {e}")
        
        # Remove completed trades
        for trade_id in completed_trades:
            del self.active_trades[trade_id]
    
    async def _can_open_trade(self, signal: TradeSignal) -> bool:
        """Check if we can open a new trade"""
        # Check maximum concurrent trades
        if len(self.active_trades) >= TRADING_CONFIG["max_concurrent_trades"]:
            return False
        
        # Check maximum daily trades
        if self.daily_stats['trades_count'] >= TRADING_CONFIG["max_daily_trades"]:
            return False
        
        # Check daily loss limit
        if self.daily_stats['profit_loss'] <= -RISK_CONFIG["max_daily_loss"] * self.daily_start_balance:
            return False
        
        # Check maximum drawdown
        current_drawdown = (self.peak_balance - self.account_balance) / self.peak_balance
        if current_drawdown > RISK_CONFIG["max_drawdown"]:
            return False
        
        # Check minimum confidence
        if signal.confidence < TRADING_CONFIG["min_confidence"]:
            return False
        
        # Check risk score
        if signal.risk_score > 0.8:  # High risk threshold
            return False
        
        return True
    
    async def _check_trading_conditions(self) -> bool:
        """Check if trading conditions are met"""
        # Check if market is open
        if not await self._is_market_open():
            return False
        
        # Check if we have sufficient balance
        if self.account_balance < TRADING_CONFIG["trade_amount"]:
            self.logger.warning("⚠️ Insufficient balance for trading")
            return False
        
        # Check if profit target is reached
        daily_profit = self.account_balance - self.daily_start_balance
        profit_target = RISK_CONFIG["profit_target"] * self.daily_start_balance
        
        if daily_profit >= profit_target:
            self.logger.info(f"🎯 Daily profit target reached: ${daily_profit:.2f}")
            return False
        
        return True
    
    async def _is_market_open(self) -> bool:
        """Check if market is open for trading"""
        current_time = datetime.now().time()
        start_time = datetime.strptime(TRADING_CONFIG["trading_hours"]["start"], "%H:%M").time()
        end_time = datetime.strptime(TRADING_CONFIG["trading_hours"]["end"], "%H:%M").time()
        
        return start_time <= current_time <= end_time
    
    async def _update_statistics(self):
        """Update bot statistics and performance metrics"""
        # Update peak balance
        if self.account_balance > self.peak_balance:
            self.peak_balance = self.account_balance
        
        # Calculate current drawdown
        current_drawdown = (self.peak_balance - self.account_balance) / self.peak_balance
        if current_drawdown > self.max_drawdown:
            self.max_drawdown = current_drawdown
        
        # Log periodic statistics
        if self.daily_stats['trades_count'] > 0 and self.daily_stats['trades_count'] % 10 == 0:
            await self._log_statistics()
    
    async def _log_statistics(self):
        """Log current trading statistics"""
        daily_profit = self.account_balance - self.daily_start_balance
        daily_return = (daily_profit / self.daily_start_balance) * 100
        
        self.logger.info(f"""
📊 Trading Statistics:
   Balance: ${self.account_balance:.2f}
   Daily P&L: ${daily_profit:.2f} ({daily_return:.2f}%)
   Trades: {self.daily_stats['trades_count']}
   Win Rate: {self.daily_stats['win_rate']:.1%}
   Max Drawdown: {self.max_drawdown:.1%}
   Active Trades: {len(self.active_trades)}
        """)
    
    async def _save_daily_stats(self):
        """Save daily statistics to database"""
        try:
            daily_stats = {
                'date': datetime.now().date(),
                'starting_balance': self.daily_start_balance,
                'ending_balance': self.account_balance,
                'trades_count': self.daily_stats['trades_count'],
                'wins': self.daily_stats['wins'],
                'losses': self.daily_stats['losses'],
                'profit_loss': self.daily_stats['profit_loss'],
                'win_rate': self.daily_stats['win_rate'],
                'max_drawdown': self.max_drawdown,
                'peak_balance': self.peak_balance
            }
            
            await self.db_manager.save_daily_stats(daily_stats)
            self.logger.info("💾 Daily statistics saved")
            
        except Exception as e:
            self.logger.error(f"❌ Error saving daily stats: {e}")
    
    async def _close_all_trades(self):
        """Close all active trades"""
        if not self.active_trades:
            return
        
        self.logger.info(f"🔄 Closing {len(self.active_trades)} active trades...")
        
        for trade_id in list(self.active_trades.keys()):
            try:
                await self.api.close_trade(trade_id)
                self.logger.info(f"✅ Trade {trade_id} closed")
            except Exception as e:
                self.logger.error(f"❌ Error closing trade {trade_id}: {e}")
    
    async def get_performance_metrics(self) -> Dict[str, Any]:
        """Get current performance metrics"""
        daily_profit = self.account_balance - self.daily_start_balance
        daily_return = (daily_profit / self.daily_start_balance) * 100 if self.daily_start_balance > 0 else 0
        
        return {
            'account_balance': self.account_balance,
            'daily_profit': daily_profit,
            'daily_return': daily_return,
            'trades_count': self.daily_stats['trades_count'],
            'win_rate': self.daily_stats['win_rate'],
            'max_drawdown': self.max_drawdown,
            'active_trades': len(self.active_trades),
            'peak_balance': self.peak_balance
        }
    
    async def get_trade_history(self, limit: int = 50) -> List[Dict]:
        """Get recent trade history"""
        return self.trade_history[-limit:] if self.trade_history else []
    
    async def emergency_stop(self):
        """Emergency stop all trading operations"""
        self.logger.warning("🚨 EMERGENCY STOP ACTIVATED")
        self.is_trading = False
        await self._close_all_trades()
        await self.stop_trading()
        self.logger.warning("🚨 Emergency stop completed")

================================================================================
FILE: src\database\__init__.py
DIRECTORY: src\database
SIZE: 949 bytes
================================================================================

"""
Database package for Quotex Trading Bot

This package contains all database-related functionality including:
- SQLAlchemy models for all data entities
- Database manager for CRUD operations
- Data export/import utilities
- Database maintenance functions
"""

from .models import (
    Base,
    TradingAccount,
    Asset,
    Trade,
    MarketData,
    TradingSignal,
    Strategy,
    BacktestResult,
    MLModel,
    ModelPrediction,
    SystemLog,
    SystemConfig
)

from .database_manager import DatabaseManager

__all__ = [
    'Base',
    'TradingAccount',
    'Asset',
    'Trade',
    'MarketData',
    'TradingSignal',
    'Strategy',
    'BacktestResult',
    'MLModel',
    'ModelPrediction',
    'SystemLog',
    'SystemConfig',
    'DatabaseManager'
]

# Version info
__version__ = '1.0.0'
__author__ = 'Quotex Trading Bot'
__description__ = 'Database layer for trading bot operations'

================================================================================
FILE: src\database\database_manager.py
DIRECTORY: src\database
SIZE: 32541 bytes
================================================================================

import os
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any, Union
from sqlalchemy import create_engine, func, and_, or_, desc, asc
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.exc import SQLAlchemyError
import pandas as pd
import logging

from .models import (
    Base, TradingAccount, Asset, Trade, MarketData, TradingSignal,
    Strategy, BacktestResult, MLModel, ModelPrediction, SystemLog, SystemConfig
)

class DatabaseManager:
    """Database manager for the Quotex trading bot"""
    
    def __init__(self, database_url: str = None):
        """Initialize database manager
        
        Args:
            database_url: Database connection URL. If None, uses SQLite
        """
        if database_url is None:
            db_path = os.path.join(os.path.dirname(__file__), '../../data/trading_bot.db')
            os.makedirs(os.path.dirname(db_path), exist_ok=True)
            database_url = f'sqlite:///{db_path}'
        
        self.engine = create_engine(database_url, echo=False)
        self.SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=self.engine)
        self.logger = logging.getLogger(__name__)
        
        # Create tables
        self.init_database()
    
    def init_database(self):
        """Create database tables"""
        try:
            Base.metadata.create_all(bind=self.engine)
            self.logger.info("Database tables created successfully")
        except SQLAlchemyError as e:
            self.logger.error(f"Error creating database tables: {e}")
            raise
    
    def get_session(self) -> Session:
        """Get database session"""
        return self.SessionLocal()
    
    def close_session(self, session: Session):
        """Close database session"""
        session.close()
    
    # Account Management
    def create_account(self, account_data: Dict[str, Any]) -> TradingAccount:
        """Create a new trading account"""
        session = self.get_session()
        try:
            account = TradingAccount(**account_data)
            session.add(account)
            session.commit()
            session.refresh(account)
            return account
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error creating account: {e}")
            raise
        finally:
            self.close_session(session)
    
    def get_account(self, account_id: str) -> Optional[TradingAccount]:
        """Get account by ID"""
        session = self.get_session()
        try:
            return session.query(TradingAccount).filter(
                TradingAccount.account_id == account_id
            ).first()
        finally:
            self.close_session(session)
    
    def update_account_balance(self, account_id: str, balance: float, equity: float = None):
        """Update account balance"""
        session = self.get_session()
        try:
            account = session.query(TradingAccount).filter(
                TradingAccount.account_id == account_id
            ).first()
            if account:
                account.balance = balance
                if equity is not None:
                    account.equity = equity
                account.updated_at = datetime.utcnow()
                session.commit()
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error updating account balance: {e}")
            raise
        finally:
            self.close_session(session)
    
    # Asset Management
    def create_asset(self, asset_data: Dict[str, Any]) -> Asset:
        """Create a new asset"""
        session = self.get_session()
        try:
            asset = Asset(**asset_data)
            session.add(asset)
            session.commit()
            session.refresh(asset)
            return asset
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error creating asset: {e}")
            raise
        finally:
            self.close_session(session)
    
    def get_asset(self, symbol: str) -> Optional[Asset]:
        """Get asset by symbol"""
        session = self.get_session()
        try:
            return session.query(Asset).filter(Asset.symbol == symbol).first()
        finally:
            self.close_session(session)
    
    def get_active_assets(self) -> List[Asset]:
        """Get all active assets"""
        session = self.get_session()
        try:
            return session.query(Asset).filter(Asset.is_active == True).all()
        finally:
            self.close_session(session)
    
    # Trade Management
    def create_trade(self, trade_data: Dict[str, Any]) -> Trade:
        """Create a new trade"""
        session = self.get_session()
        try:
            trade = Trade(**trade_data)
            session.add(trade)
            session.commit()
            session.refresh(trade)
            return trade
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error creating trade: {e}")
            raise
        finally:
            self.close_session(session)
    
    def update_trade(self, trade_id: str, updates: Dict[str, Any]):
        """Update an existing trade"""
        session = self.get_session()
        try:
            trade = session.query(Trade).filter(Trade.trade_id == trade_id).first()
            if trade:
                for key, value in updates.items():
                    setattr(trade, key, value)
                trade.updated_at = datetime.utcnow()
                session.commit()
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error updating trade: {e}")
            raise
        finally:
            self.close_session(session)
    
    def get_trade(self, trade_id: str) -> Optional[Trade]:
        """Get trade by ID"""
        session = self.get_session()
        try:
            return session.query(Trade).filter(Trade.trade_id == trade_id).first()
        finally:
            self.close_session(session)
    
    def get_trades(self, account_id: str = None, symbol: str = None, 
                   status: str = None, limit: int = 100) -> List[Trade]:
        """Get trades with optional filters"""
        session = self.get_session()
        try:
            query = session.query(Trade)
            
            if account_id:
                query = query.join(TradingAccount).filter(
                    TradingAccount.account_id == account_id
                )
            
            if symbol:
                query = query.join(Asset).filter(Asset.symbol == symbol)
            
            if status:
                query = query.filter(Trade.status == status)
            
            return query.order_by(desc(Trade.open_time)).limit(limit).all()
        finally:
            self.close_session(session)
    
    def get_trade_statistics(self, account_id: str = None, 
                           start_date: datetime = None, 
                           end_date: datetime = None) -> Dict[str, Any]:
        """Get trading statistics"""
        session = self.get_session()
        try:
            query = session.query(Trade)
            
            if account_id:
                query = query.join(TradingAccount).filter(
                    TradingAccount.account_id == account_id
                )
            
            if start_date:
                query = query.filter(Trade.open_time >= start_date)
            
            if end_date:
                query = query.filter(Trade.open_time <= end_date)
            
            trades = query.all()
            
            if not trades:
                return {
                    'total_trades': 0,
                    'winning_trades': 0,
                    'losing_trades': 0,
                    'win_rate': 0.0,
                    'total_profit_loss': 0.0,
                    'average_profit_loss': 0.0
                }
            
            total_trades = len(trades)
            winning_trades = len([t for t in trades if t.status == 'WIN'])
            losing_trades = len([t for t in trades if t.status == 'LOSS'])
            total_profit_loss = sum([t.profit_loss for t in trades])
            
            return {
                'total_trades': total_trades,
                'winning_trades': winning_trades,
                'losing_trades': losing_trades,
                'win_rate': (winning_trades / total_trades) * 100 if total_trades > 0 else 0.0,
                'total_profit_loss': total_profit_loss,
                'average_profit_loss': total_profit_loss / total_trades if total_trades > 0 else 0.0
            }
        finally:
            self.close_session(session)
    
    # Market Data Management
    def save_market_data(self, market_data: List[Dict[str, Any]]):
        """Save market data in batch"""
        session = self.get_session()
        try:
            for data in market_data:
                # Convert indicators dict to JSON string if present
                if 'indicators' in data and isinstance(data['indicators'], dict):
                    data['indicators'] = json.dumps(data['indicators'])
                
                # Check if data already exists
                existing = session.query(MarketData).filter(
                    and_(
                        MarketData.asset_id == data['asset_id'],
                        MarketData.timestamp == data['timestamp'],
                        MarketData.timeframe == data['timeframe']
                    )
                ).first()
                
                if not existing:
                    session.add(MarketData(**data))
            
            session.commit()
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error saving market data: {e}")
            raise
        finally:
            self.close_session(session)
    
    def get_market_data(self, symbol: str, timeframe: str, 
                       start_date: datetime = None, 
                       end_date: datetime = None, 
                       limit: int = 1000) -> List[MarketData]:
        """Get market data for a symbol"""
        session = self.get_session()
        try:
            query = session.query(MarketData).join(Asset).filter(
                and_(
                    Asset.symbol == symbol,
                    MarketData.timeframe == timeframe
                )
            )
            
            if start_date:
                query = query.filter(MarketData.timestamp >= start_date)
            
            if end_date:
                query = query.filter(MarketData.timestamp <= end_date)
            
            return query.order_by(desc(MarketData.timestamp)).limit(limit).all()
        finally:
            self.close_session(session)
    
    def get_latest_market_data(self, symbol: str, timeframe: str) -> Optional[MarketData]:
        """Get latest market data for a symbol"""
        session = self.get_session()
        try:
            return session.query(MarketData).join(Asset).filter(
                and_(
                    Asset.symbol == symbol,
                    MarketData.timeframe == timeframe
                )
            ).order_by(desc(MarketData.timestamp)).first()
        finally:
            self.close_session(session)
    
    # Signal Management
    def create_signal(self, signal_data: Dict[str, Any]) -> TradingSignal:
        """Create a new trading signal"""
        session = self.get_session()
        try:
            # Convert indicators dict to JSON string if present
            if 'indicators_used' in signal_data and isinstance(signal_data['indicators_used'], dict):
                signal_data['indicators_used'] = json.dumps(signal_data['indicators_used'])
            
            signal = TradingSignal(**signal_data)
            session.add(signal)
            session.commit()
            session.refresh(signal)
            return signal
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error creating signal: {e}")
            raise
        finally:
            self.close_session(session)
    
    def get_active_signals(self, account_id: str = None) -> List[TradingSignal]:
        """Get active (not executed) signals"""
        session = self.get_session()
        try:
            query = session.query(TradingSignal).filter(
                TradingSignal.is_executed == False
            )
            
            if account_id:
                query = query.join(TradingAccount).filter(
                    TradingAccount.account_id == account_id
                )
            
            return query.order_by(desc(TradingSignal.generated_at)).all()
        finally:
            self.close_session(session)
    
    def mark_signal_executed(self, signal_id: int, trade_id: str = None):
        """Mark signal as executed"""
        session = self.get_session()
        try:
            signal = session.query(TradingSignal).filter(
                TradingSignal.id == signal_id
            ).first()
            
            if signal:
                signal.is_executed = True
                signal.executed_at = datetime.utcnow()
                session.commit()
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error marking signal as executed: {e}")
            raise
        finally:
            self.close_session(session)
    
    # Strategy Management
    def create_strategy(self, strategy_data: Dict[str, Any]) -> Strategy:
        """Create a new strategy"""
        session = self.get_session()
        try:
            # Convert parameters dict to JSON string if present
            if 'parameters' in strategy_data and isinstance(strategy_data['parameters'], dict):
                strategy_data['parameters'] = json.dumps(strategy_data['parameters'])
            
            strategy = Strategy(**strategy_data)
            session.add(strategy)
            session.commit()
            session.refresh(strategy)
            return strategy
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error creating strategy: {e}")
            raise
        finally:
            self.close_session(session)
    
    def get_active_strategies(self) -> List[Strategy]:
        """Get active strategies"""
        session = self.get_session()
        try:
            return session.query(Strategy).filter(Strategy.is_active == True).all()
        finally:
            self.close_session(session)
    
    def update_strategy_performance(self, strategy_name: str, 
                                  trade_result: str, profit_loss: float):
        """Update strategy performance metrics"""
        session = self.get_session()
        try:
            strategy = session.query(Strategy).filter(
                Strategy.name == strategy_name
            ).first()
            
            if strategy:
                strategy.total_trades += 1
                
                if trade_result == 'WIN':
                    strategy.winning_trades += 1
                elif trade_result == 'LOSS':
                    strategy.losing_trades += 1
                
                strategy.win_rate = (strategy.winning_trades / strategy.total_trades) * 100
                strategy.profit_loss += profit_loss
                strategy.updated_at = datetime.utcnow()
                
                session.commit()
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error updating strategy performance: {e}")
            raise
        finally:
            self.close_session(session)
    
    # ML Model Management
    def create_ml_model(self, model_data: Dict[str, Any]) -> MLModel:
        """Create a new ML model"""
        session = self.get_session()
        try:
            # Convert parameters and features to JSON strings if present
            if 'parameters' in model_data and isinstance(model_data['parameters'], dict):
                model_data['parameters'] = json.dumps(model_data['parameters'])
            
            if 'features' in model_data and isinstance(model_data['features'], list):
                model_data['features'] = json.dumps(model_data['features'])
            
            model = MLModel(**model_data)
            session.add(model)
            session.commit()
            session.refresh(model)
            return model
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error creating ML model: {e}")
            raise
        finally:
            self.close_session(session)
    
    def get_active_ml_models(self) -> List[MLModel]:
        """Get active ML models"""
        session = self.get_session()
        try:
            return session.query(MLModel).filter(MLModel.is_active == True).all()
        finally:
            self.close_session(session)
    
    def save_model_prediction(self, prediction_data: Dict[str, Any]) -> ModelPrediction:
        """Save ML model prediction"""
        session = self.get_session()
        try:
            # Convert features dict to JSON string if present
            if 'features' in prediction_data and isinstance(prediction_data['features'], dict):
                prediction_data['features'] = json.dumps(prediction_data['features'])
            
            prediction = ModelPrediction(**prediction_data)
            session.add(prediction)
            session.commit()
            session.refresh(prediction)
            return prediction
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error saving model prediction: {e}")
            raise
        finally:
            self.close_session(session)
    
    def update_prediction_outcome(self, prediction_id: int, actual_outcome: str):
        """Update prediction with actual outcome"""
        session = self.get_session()
        try:
            prediction = session.query(ModelPrediction).filter(
                ModelPrediction.id == prediction_id
            ).first()
            
            if prediction:
                prediction.actual_outcome = actual_outcome
                prediction.is_correct = (prediction.prediction_type == actual_outcome)
                session.commit()
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error updating prediction outcome: {e}")
            raise
        finally:
            self.close_session(session)
    
    def get_model_accuracy(self, model_id: int, days_back: int = 30) -> float:
        """Calculate model accuracy over specified period"""
        session = self.get_session()
        try:
            cutoff_date = datetime.utcnow() - timedelta(days=days_back)
            
            predictions = session.query(ModelPrediction).filter(
                and_(
                    ModelPrediction.model_id == model_id,
                    ModelPrediction.prediction_time >= cutoff_date,
                    ModelPrediction.actual_outcome.isnot(None)
                )
            ).all()
            
            if not predictions:
                return 0.0
            
            correct_predictions = len([p for p in predictions if p.is_correct])
            return (correct_predictions / len(predictions)) * 100
        finally:
            self.close_session(session)
    
    # Backtest Management
    def save_backtest_result(self, backtest_data: Dict[str, Any]) -> BacktestResult:
        """Save backtest result"""
        session = self.get_session()
        try:
            # Convert lists/dicts to JSON strings if present
            if 'asset_symbols' in backtest_data and isinstance(backtest_data['asset_symbols'], list):
                backtest_data['asset_symbols'] = json.dumps(backtest_data['asset_symbols'])
            
            if 'trade_details' in backtest_data and isinstance(backtest_data['trade_details'], list):
                backtest_data['trade_details'] = json.dumps(backtest_data['trade_details'])
            
            if 'equity_curve' in backtest_data and isinstance(backtest_data['equity_curve'], list):
                backtest_data['equity_curve'] = json.dumps(backtest_data['equity_curve'])
            
            result = BacktestResult(**backtest_data)
            session.add(result)
            session.commit()
            session.refresh(result)
            return result
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error saving backtest result: {e}")
            raise
        finally:
            self.close_session(session)
    
    def get_backtest_results(self, strategy_name: str = None) -> List[BacktestResult]:
        """Get backtest results"""
        session = self.get_session()
        try:
            query = session.query(BacktestResult)
            
            if strategy_name:
                query = query.join(Strategy).filter(Strategy.name == strategy_name)
            
            return query.order_by(desc(BacktestResult.created_at)).all()
        finally:
            self.close_session(session)
    
    # Logging
    def log_event(self, level: str, module: str, message: str, **kwargs):
        """Log system event"""
        session = self.get_session()
        try:
            log_entry = SystemLog(
                level=level,
                module=module,
                message=message,
                **kwargs
            )
            session.add(log_entry)
            session.commit()
        except SQLAlchemyError as e:
            # Don't raise exception for logging errors to avoid infinite loops
            print(f"Error logging event: {e}")
        finally:
            self.close_session(session)
    
    def get_logs(self, level: str = None, module: str = None, 
                 start_date: datetime = None, limit: int = 100) -> List[SystemLog]:
        """Get system logs"""
        session = self.get_session()
        try:
            query = session.query(SystemLog)
            
            if level:
                query = query.filter(SystemLog.level == level)
            
            if module:
                query = query.filter(SystemLog.module == module)
            
            if start_date:
                query = query.filter(SystemLog.timestamp >= start_date)
            
            return query.order_by(desc(SystemLog.timestamp)).limit(limit).all()
        finally:
            self.close_session(session)
    
    # Configuration Management
    def set_config(self, key: str, value: Any, description: str = None, 
                   config_type: str = 'general'):
        """Set configuration value"""
        session = self.get_session()
        try:
            # Convert value to string if it's not already
            if not isinstance(value, str):
                value = json.dumps(value)
            
            config = session.query(SystemConfig).filter(
                SystemConfig.key == key
            ).first()
            
            if config:
                config.value = value
                config.updated_at = datetime.utcnow()
                if description:
                    config.description = description
            else:
                config = SystemConfig(
                    key=key,
                    value=value,
                    description=description,
                    config_type=config_type
                )
                session.add(config)
            
            session.commit()
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error setting config: {e}")
            raise
        finally:
            self.close_session(session)
    
    def get_config(self, key: str, default: Any = None) -> Any:
        """Get configuration value"""
        session = self.get_session()
        try:
            config = session.query(SystemConfig).filter(
                SystemConfig.key == key
            ).first()
            
            if config:
                try:
                    # Try to parse as JSON first
                    return json.loads(config.value)
                except (json.JSONDecodeError, ValueError):
                    # Return as string if not valid JSON
                    return config.value
            
            return default
        finally:
            self.close_session(session)
    
    def get_all_configs(self, config_type: str = None) -> Dict[str, Any]:
        """Get all configuration values"""
        session = self.get_session()
        try:
            query = session.query(SystemConfig)
            
            if config_type:
                query = query.filter(SystemConfig.config_type == config_type)
            
            configs = query.all()
            result = {}
            
            for config in configs:
                try:
                    result[config.key] = json.loads(config.value)
                except (json.JSONDecodeError, ValueError):
                    result[config.key] = config.value
            
            return result
        finally:
            self.close_session(session)
    
    # Data Export/Import
    def export_trades_to_csv(self, filename: str, account_id: str = None, 
                            start_date: datetime = None, end_date: datetime = None):
        """Export trades to CSV file"""
        session = self.get_session()
        try:
            query = session.query(Trade).join(Asset).join(TradingAccount)
            
            if account_id:
                query = query.filter(TradingAccount.account_id == account_id)
            
            if start_date:
                query = query.filter(Trade.open_time >= start_date)
            
            if end_date:
                query = query.filter(Trade.open_time <= end_date)
            
            trades = query.all()
            
            # Convert to DataFrame
            data = []
            for trade in trades:
                data.append({
                    'trade_id': trade.trade_id,
                    'account_id': trade.account.account_id,
                    'symbol': trade.asset.symbol,
                    'direction': trade.direction,
                    'entry_price': trade.entry_price,
                    'exit_price': trade.exit_price,
                    'amount': trade.amount,
                    'open_time': trade.open_time,
                    'close_time': trade.close_time,
                    'expiry_time': trade.expiry_time,
                    'status': trade.status,
                    'profit_loss': trade.profit_loss,
                    'strategy_name': trade.strategy_name
                })
            
            df = pd.DataFrame(data)
            df.to_csv(filename, index=False)
            
            self.logger.info(f"Exported {len(trades)} trades to {filename}")
        finally:
            self.close_session(session)
    
    def get_market_data_df(self, symbol: str, timeframe: str, 
                          start_date: datetime = None, 
                          end_date: datetime = None) -> pd.DataFrame:
        """Get market data as pandas DataFrame"""
        session = self.get_session()
        try:
            query = session.query(MarketData).join(Asset).filter(
                and_(
                    Asset.symbol == symbol,
                    MarketData.timeframe == timeframe
                )
            )
            
            if start_date:
                query = query.filter(MarketData.timestamp >= start_date)
            
            if end_date:
                query = query.filter(MarketData.timestamp <= end_date)
            
            data = query.order_by(MarketData.timestamp).all()
            
            if not data:
                return pd.DataFrame()
            
            # Convert to DataFrame
            df_data = []
            for candle in data:
                row = {
                    'timestamp': candle.timestamp,
                    'open': candle.open_price,
                    'high': candle.high_price,
                    'low': candle.low_price,
                    'close': candle.close_price,
                    'volume': candle.volume
                }
                
                # Add indicators if available
                if candle.indicators:
                    try:
                        indicators = json.loads(candle.indicators)
                        row.update(indicators)
                    except json.JSONDecodeError:
                        pass
                
                df_data.append(row)
            
            df = pd.DataFrame(df_data)
            df.set_index('timestamp', inplace=True)
            return df
        finally:
            self.close_session(session)
    
    # Cleanup and Maintenance
    def cleanup_old_data(self, days_to_keep: int = 90):
        """Clean up old data to manage database size"""
        session = self.get_session()
        try:
            cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)
            
            # Clean up old logs
            old_logs = session.query(SystemLog).filter(
                SystemLog.timestamp < cutoff_date
            ).delete()
            
            # Clean up old market data (keep only daily and higher timeframes)
            old_market_data = session.query(MarketData).filter(
                and_(
                    MarketData.timestamp < cutoff_date,
                    MarketData.timeframe.in_(['1m', '5m', '15m', '30m'])
                )
            ).delete()
            
            # Clean up old predictions
            old_predictions = session.query(ModelPrediction).filter(
                ModelPrediction.prediction_time < cutoff_date
            ).delete()
            
            session.commit()
            
            self.logger.info(f"Cleaned up {old_logs} logs, {old_market_data} market data records, "
                           f"and {old_predictions} predictions older than {days_to_keep} days")
        except SQLAlchemyError as e:
            session.rollback()
            self.logger.error(f"Error cleaning up old data: {e}")
            raise
        finally:
            self.close_session(session)
    
    def get_database_stats(self) -> Dict[str, int]:
        """Get database statistics"""
        session = self.get_session()
        try:
            stats = {}
            
            # Count records in each table
            stats['accounts'] = session.query(TradingAccount).count()
            stats['assets'] = session.query(Asset).count()
            stats['trades'] = session.query(Trade).count()
            stats['market_data'] = session.query(MarketData).count()
            stats['signals'] = session.query(TradingSignal).count()
            stats['strategies'] = session.query(Strategy).count()
            stats['ml_models'] = session.query(MLModel).count()
            stats['predictions'] = session.query(ModelPrediction).count()
            stats['backtest_results'] = session.query(BacktestResult).count()
            stats['logs'] = session.query(SystemLog).count()
            stats['configs'] = session.query(SystemConfig).count()
            
            return stats
        finally:
            self.close_session(session)
    
    def __del__(self):
        """Cleanup on deletion"""
        try:
            self.engine.dispose()
        except:
            pass

================================================================================
FILE: src\database\models.py
DIRECTORY: src\database
SIZE: 12291 bytes
================================================================================

from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, Text, ForeignKey, Index
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime
import json

Base = declarative_base()

class TradingAccount(Base):
    """Trading account information"""
    __tablename__ = 'trading_accounts'
    
    id = Column(Integer, primary_key=True)
    account_id = Column(String(50), unique=True, nullable=False)
    account_name = Column(String(100), nullable=False)
    balance = Column(Float, default=0.0)
    equity = Column(Float, default=0.0)
    margin = Column(Float, default=0.0)
    free_margin = Column(Float, default=0.0)
    currency = Column(String(10), default='USD')
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    trades = relationship("Trade", back_populates="account")
    signals = relationship("TradingSignal", back_populates="account")

class Asset(Base):
    """Trading assets/symbols"""
    __tablename__ = 'assets'
    
    id = Column(Integer, primary_key=True)
    symbol = Column(String(20), unique=True, nullable=False)
    name = Column(String(100), nullable=False)
    category = Column(String(50))  # forex, crypto, commodities, indices
    base_currency = Column(String(10))
    quote_currency = Column(String(10))
    min_trade_amount = Column(Float, default=1.0)
    max_trade_amount = Column(Float, default=1000.0)
    tick_size = Column(Float, default=0.00001)
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    trades = relationship("Trade", back_populates="asset")
    market_data = relationship("MarketData", back_populates="asset")
    signals = relationship("TradingSignal", back_populates="asset")

class Trade(Base):
    """Individual trades"""
    __tablename__ = 'trades'
    
    id = Column(Integer, primary_key=True)
    trade_id = Column(String(50), unique=True, nullable=False)
    account_id = Column(Integer, ForeignKey('trading_accounts.id'), nullable=False)
    asset_id = Column(Integer, ForeignKey('assets.id'), nullable=False)
    
    # Trade details
    direction = Column(String(10), nullable=False)  # CALL, PUT
    entry_price = Column(Float, nullable=False)
    exit_price = Column(Float)
    amount = Column(Float, nullable=False)
    payout_rate = Column(Float, default=0.8)
    
    # Timestamps
    open_time = Column(DateTime, nullable=False)
    close_time = Column(DateTime)
    expiry_time = Column(DateTime, nullable=False)
    
    # Results
    status = Column(String(20), default='PENDING')  # PENDING, WIN, LOSS, DRAW
    profit_loss = Column(Float, default=0.0)
    is_demo = Column(Boolean, default=True)
    
    # Strategy info
    strategy_name = Column(String(50))
    signal_id = Column(Integer, ForeignKey('trading_signals.id'))
    
    # Metadata
    notes = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    # Relationships
    account = relationship("TradingAccount", back_populates="trades")
    asset = relationship("Asset", back_populates="trades")
    signal = relationship("TradingSignal", back_populates="trades")
    
    # Indexes
    __table_args__ = (
        Index('idx_trades_status', 'status'),
        Index('idx_trades_open_time', 'open_time'),
        Index('idx_trades_account_asset', 'account_id', 'asset_id'),
    )

class MarketData(Base):
    """Historical market data"""
    __tablename__ = 'market_data'
    
    id = Column(Integer, primary_key=True)
    asset_id = Column(Integer, ForeignKey('assets.id'), nullable=False)
    
    # OHLCV data
    timestamp = Column(DateTime, nullable=False)
    open_price = Column(Float, nullable=False)
    high_price = Column(Float, nullable=False)
    low_price = Column(Float, nullable=False)
    close_price = Column(Float, nullable=False)
    volume = Column(Float, default=0.0)
    
    # Timeframe
    timeframe = Column(String(10), nullable=False)  # 1m, 5m, 15m, 1h, 4h, 1d
    
    # Indicators (stored as JSON)
    indicators = Column(Text)  # JSON string of calculated indicators
    
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    asset = relationship("Asset", back_populates="market_data")
    
    # Indexes
    __table_args__ = (
        Index('idx_market_data_asset_time', 'asset_id', 'timestamp'),
        Index('idx_market_data_timeframe', 'timeframe'),
    )

class TradingSignal(Base):
    """Trading signals generated by strategies"""
    __tablename__ = 'trading_signals'
    
    id = Column(Integer, primary_key=True)
    account_id = Column(Integer, ForeignKey('trading_accounts.id'), nullable=False)
    asset_id = Column(Integer, ForeignKey('assets.id'), nullable=False)
    
    # Signal details
    signal_type = Column(String(10), nullable=False)  # CALL, PUT
    strength = Column(Float, nullable=False)  # 0.0 to 1.0
    confidence = Column(Float, nullable=False)  # 0.0 to 1.0
    entry_price = Column(Float, nullable=False)
    target_price = Column(Float)
    stop_loss = Column(Float)
    
    # Strategy info
    strategy_name = Column(String(50), nullable=False)
    indicators_used = Column(Text)  # JSON string of indicators
    
    # Timing
    generated_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    expires_at = Column(DateTime)
    
    # Execution
    is_executed = Column(Boolean, default=False)
    executed_at = Column(DateTime)
    
    # Metadata
    notes = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    account = relationship("TradingAccount", back_populates="signals")
    asset = relationship("Asset", back_populates="signals")
    trades = relationship("Trade", back_populates="signal")
    
    # Indexes
    __table_args__ = (
        Index('idx_signals_generated_at', 'generated_at'),
        Index('idx_signals_strategy', 'strategy_name'),
        Index('idx_signals_executed', 'is_executed'),
    )

class Strategy(Base):
    """Trading strategies configuration"""
    __tablename__ = 'strategies'
    
    id = Column(Integer, primary_key=True)
    name = Column(String(50), unique=True, nullable=False)
    description = Column(Text)
    
    # Strategy parameters (stored as JSON)
    parameters = Column(Text, nullable=False)
    
    # Performance metrics
    total_trades = Column(Integer, default=0)
    winning_trades = Column(Integer, default=0)
    losing_trades = Column(Integer, default=0)
    win_rate = Column(Float, default=0.0)
    profit_loss = Column(Float, default=0.0)
    max_drawdown = Column(Float, default=0.0)
    
    # Status
    is_active = Column(Boolean, default=True)
    is_backtested = Column(Boolean, default=False)
    
    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    last_run_at = Column(DateTime)

class BacktestResult(Base):
    """Backtest results"""
    __tablename__ = 'backtest_results'
    
    id = Column(Integer, primary_key=True)
    strategy_id = Column(Integer, ForeignKey('strategies.id'), nullable=False)
    
    # Backtest parameters
    start_date = Column(DateTime, nullable=False)
    end_date = Column(DateTime, nullable=False)
    initial_balance = Column(Float, nullable=False)
    asset_symbols = Column(Text)  # JSON array of symbols
    
    # Results
    final_balance = Column(Float, nullable=False)
    total_trades = Column(Integer, default=0)
    winning_trades = Column(Integer, default=0)
    losing_trades = Column(Integer, default=0)
    win_rate = Column(Float, default=0.0)
    profit_loss = Column(Float, default=0.0)
    max_drawdown = Column(Float, default=0.0)
    sharpe_ratio = Column(Float)
    
    # Detailed results (stored as JSON)
    trade_details = Column(Text)
    equity_curve = Column(Text)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    strategy = relationship("Strategy")

class MLModel(Base):
    """Machine learning models"""
    __tablename__ = 'ml_models'
    
    id = Column(Integer, primary_key=True)
    name = Column(String(100), nullable=False)
    model_type = Column(String(50), nullable=False)  # RandomForest, XGBoost, LSTM, etc.
    version = Column(String(20), nullable=False)
    
    # Model parameters
    parameters = Column(Text)  # JSON string
    features = Column(Text)  # JSON array of feature names
    
    # Performance metrics
    accuracy = Column(Float)
    precision = Column(Float)
    recall = Column(Float)
    f1_score = Column(Float)
    
    # Training info
    training_samples = Column(Integer)
    training_start_date = Column(DateTime)
    training_end_date = Column(DateTime)
    
    # File paths
    model_path = Column(String(255))
    scaler_path = Column(String(255))
    
    # Status
    is_active = Column(Boolean, default=False)
    is_trained = Column(Boolean, default=False)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

class ModelPrediction(Base):
    """ML model predictions"""
    __tablename__ = 'model_predictions'
    
    id = Column(Integer, primary_key=True)
    model_id = Column(Integer, ForeignKey('ml_models.id'), nullable=False)
    asset_id = Column(Integer, ForeignKey('assets.id'), nullable=False)
    
    # Prediction details
    prediction_type = Column(String(10), nullable=False)  # CALL, PUT
    probability = Column(Float, nullable=False)
    confidence = Column(Float, nullable=False)
    
    # Features used (stored as JSON)
    features = Column(Text, nullable=False)
    
    # Timing
    prediction_time = Column(DateTime, nullable=False, default=datetime.utcnow)
    target_time = Column(DateTime, nullable=False)
    
    # Actual outcome (for model evaluation)
    actual_outcome = Column(String(10))  # CALL, PUT
    is_correct = Column(Boolean)
    
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relationships
    model = relationship("MLModel")
    asset = relationship("Asset")
    
    # Indexes
    __table_args__ = (
        Index('idx_predictions_time', 'prediction_time'),
        Index('idx_predictions_model_asset', 'model_id', 'asset_id'),
    )

class SystemLog(Base):
    """System logs and events"""
    __tablename__ = 'system_logs'
    
    id = Column(Integer, primary_key=True)
    level = Column(String(20), nullable=False)  # DEBUG, INFO, WARNING, ERROR, CRITICAL
    module = Column(String(50), nullable=False)
    message = Column(Text, nullable=False)
    
    # Additional context
    user_id = Column(String(50))
    session_id = Column(String(50))
    trade_id = Column(String(50))
    
    # Exception details
    exception_type = Column(String(100))
    exception_message = Column(Text)
    stack_trace = Column(Text)
    
    timestamp = Column(DateTime, default=datetime.utcnow)
    
    # Indexes
    __table_args__ = (
        Index('idx_logs_timestamp', 'timestamp'),
        Index('idx_logs_level', 'level'),
        Index('idx_logs_module', 'module'),
    )

class SystemConfig(Base):
    """System configuration settings"""
    __tablename__ = 'system_config'
    
    id = Column(Integer, primary_key=True)
    key = Column(String(100), unique=True, nullable=False)
    value = Column(Text, nullable=False)
    description = Column(Text)
    config_type = Column(String(50), default='general')  # general, trading, ml, api
    
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

================================================================================
FILE: src\indicators\__init__.py
DIRECTORY: src\indicators
SIZE: 0 bytes
================================================================================



================================================================================
FILE: src\indicators\custom_indicators.py
DIRECTORY: src\indicators
SIZE: 13787 bytes
================================================================================

"""
Custom Technical Indicators for Quotex Trading Bot
This module contains custom technical indicators for advanced trading strategies.
"""

import numpy as np
import pandas as pd
from typing import Union, Tuple, List, Optional
import talib
from scipy.signal import argrelextrema
from scipy.stats import linregress
import warnings
warnings.filterwarnings('ignore')


class CustomIndicators:
    """
    Collection of custom technical indicators for trading analysis.
    """
    
    @staticmethod
    def heikin_ashi(data: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate Heikin Ashi candles for smoother price action analysis.
        
        Args:
            data: DataFrame with OHLC columns
            
        Returns:
            DataFrame with Heikin Ashi OHLC values
        """
        ha_data = pd.DataFrame(index=data.index)
        
        # Calculate Heikin Ashi values
        ha_data['ha_close'] = (data['open'] + data['high'] + data['low'] + data['close']) / 4
        ha_data['ha_open'] = (data['open'].shift(1) + data['close'].shift(1)) / 2
        ha_data['ha_open'].iloc[0] = data['open'].iloc[0]
        
        # Recalculate open based on previous values
        for i in range(1, len(ha_data)):
            ha_data['ha_open'].iloc[i] = (ha_data['ha_open'].iloc[i-1] + ha_data['ha_close'].iloc[i-1]) / 2
        
        ha_data['ha_high'] = np.maximum(data['high'], np.maximum(ha_data['ha_open'], ha_data['ha_close']))
        ha_data['ha_low'] = np.minimum(data['low'], np.minimum(ha_data['ha_open'], ha_data['ha_close']))
        
        return ha_data
    
    @staticmethod
    def supertrend(data: pd.DataFrame, period: int = 10, multiplier: float = 3.0) -> pd.DataFrame:
        """
        Calculate SuperTrend indicator for trend direction and support/resistance.
        
        Args:
            data: DataFrame with OHLC columns
            period: ATR period
            multiplier: ATR multiplier
            
        Returns:
            DataFrame with SuperTrend values and signals
        """
        # Calculate ATR
        atr = talib.ATR(data['high'], data['low'], data['close'], timeperiod=period)
        
        # Calculate basic bands
        hl2 = (data['high'] + data['low']) / 2
        upper_band = hl2 + (multiplier * atr)
        lower_band = hl2 - (multiplier * atr)
        
        # Initialize SuperTrend
        supertrend = pd.Series(index=data.index, dtype=float)
        direction = pd.Series(index=data.index, dtype=int)
        
        # Calculate SuperTrend
        for i in range(1, len(data)):
            if data['close'].iloc[i] > upper_band.iloc[i-1]:
                direction.iloc[i] = 1  # Bullish
            elif data['close'].iloc[i] < lower_band.iloc[i-1]:
                direction.iloc[i] = -1  # Bearish
            else:
                direction.iloc[i] = direction.iloc[i-1]
            
            if direction.iloc[i] == 1:
                supertrend.iloc[i] = lower_band.iloc[i]
            else:
                supertrend.iloc[i] = upper_band.iloc[i]
        
        # Set initial values
        direction.iloc[0] = 1
        supertrend.iloc[0] = lower_band.iloc[0]
        
        result = pd.DataFrame({
            'supertrend': supertrend,
            'direction': direction,
            'upper_band': upper_band,
            'lower_band': lower_band
        })
        
        return result
    
    @staticmethod
    def ichimoku_cloud(data: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate Ichimoku Cloud components.
        
        Args:
            data: DataFrame with OHLC columns
            
        Returns:
            DataFrame with Ichimoku components
        """
        # Tenkan-sen (Conversion Line): 9-period high-low average
        tenkan_sen = (data['high'].rolling(window=9).max() + data['low'].rolling(window=9).min()) / 2
        
        # Kijun-sen (Base Line): 26-period high-low average
        kijun_sen = (data['high'].rolling(window=26).max() + data['low'].rolling(window=26).min()) / 2
        
        # Senkou Span A (Leading Span A): (Tenkan-sen + Kijun-sen) / 2, shifted 26 periods forward
        senkou_span_a = ((tenkan_sen + kijun_sen) / 2).shift(26)
        
        # Senkou Span B (Leading Span B): 52-period high-low average, shifted 26 periods forward
        senkou_span_b = ((data['high'].rolling(window=52).max() + data['low'].rolling(window=52).min()) / 2).shift(26)
        
        # Chikou Span (Lagging Span): Close price shifted 26 periods backward
        chikou_span = data['close'].shift(-26)
        
        return pd.DataFrame({
            'tenkan_sen': tenkan_sen,
            'kijun_sen': kijun_sen,
            'senkou_span_a': senkou_span_a,
            'senkou_span_b': senkou_span_b,
            'chikou_span': chikou_span
        })
    
    @staticmethod
    def vwap(data: pd.DataFrame) -> pd.Series:
        """
        Calculate Volume Weighted Average Price (VWAP).
        
        Args:
            data: DataFrame with OHLC and volume columns
            
        Returns:
            Series with VWAP values
        """
        typical_price = (data['high'] + data['low'] + data['close']) / 3
        return (typical_price * data['volume']).cumsum() / data['volume'].cumsum()
    
    @staticmethod
    def fibonacci_retracement(data: pd.DataFrame, period: int = 50) -> pd.DataFrame:
        """
        Calculate Fibonacci retracement levels.
        
        Args:
            data: DataFrame with OHLC columns
            period: Period to calculate high/low
            
        Returns:
            DataFrame with Fibonacci levels
        """
        high = data['high'].rolling(window=period).max()
        low = data['low'].rolling(window=period).min()
        
        diff = high - low
        
        fib_levels = pd.DataFrame({
            'fib_0': high,
            'fib_236': high - (diff * 0.236),
            'fib_382': high - (diff * 0.382),
            'fib_50': high - (diff * 0.5),
            'fib_618': high - (diff * 0.618),
            'fib_786': high - (diff * 0.786),
            'fib_100': low
        })
        
        return fib_levels
    
    @staticmethod
    def pivot_points(data: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate pivot points and support/resistance levels.
        
        Args:
            data: DataFrame with OHLC columns
            
        Returns:
            DataFrame with pivot points
        """
        # Calculate pivot point
        pivot = (data['high'].shift(1) + data['low'].shift(1) + data['close'].shift(1)) / 3
        
        # Calculate support and resistance levels
        r1 = 2 * pivot - data['low'].shift(1)
        s1 = 2 * pivot - data['high'].shift(1)
        r2 = pivot + (data['high'].shift(1) - data['low'].shift(1))
        s2 = pivot - (data['high'].shift(1) - data['low'].shift(1))
        r3 = data['high'].shift(1) + 2 * (pivot - data['low'].shift(1))
        s3 = data['low'].shift(1) - 2 * (data['high'].shift(1) - pivot)
        
        return pd.DataFrame({
            'pivot': pivot,
            'r1': r1, 'r2': r2, 'r3': r3,
            's1': s1, 's2': s2, 's3': s3
        })
    
    @staticmethod
    def money_flow_index(data: pd.DataFrame, period: int = 14) -> pd.Series:
        """
        Calculate Money Flow Index (MFI).
        
        Args:
            data: DataFrame with OHLC and volume columns
            period: Period for calculation
            
        Returns:
            Series with MFI values
        """
        typical_price = (data['high'] + data['low'] + data['close']) / 3
        money_flow = typical_price * data['volume']
        
        # Calculate positive and negative money flow
        positive_mf = pd.Series(index=data.index, dtype=float)
        negative_mf = pd.Series(index=data.index, dtype=float)
        
        for i in range(1, len(data)):
            if typical_price.iloc[i] > typical_price.iloc[i-1]:
                positive_mf.iloc[i] = money_flow.iloc[i]
                negative_mf.iloc[i] = 0
            elif typical_price.iloc[i] < typical_price.iloc[i-1]:
                positive_mf.iloc[i] = 0
                negative_mf.iloc[i] = money_flow.iloc[i]
            else:
                positive_mf.iloc[i] = 0
                negative_mf.iloc[i] = 0
        
        # Calculate MFI
        positive_mf_sum = positive_mf.rolling(window=period).sum()
        negative_mf_sum = negative_mf.rolling(window=period).sum()
        
        money_ratio = positive_mf_sum / negative_mf_sum
        mfi = 100 - (100 / (1 + money_ratio))
        
        return mfi
    
    @staticmethod
    def accumulation_distribution(data: pd.DataFrame) -> pd.Series:
        """
        Calculate Accumulation/Distribution Line.
        
        Args:
            data: DataFrame with OHLC and volume columns
            
        Returns:
            Series with A/D values
        """
        clv = ((data['close'] - data['low']) - (data['high'] - data['close'])) / (data['high'] - data['low'])
        clv = clv.fillna(0)  # Handle division by zero
        
        ad = (clv * data['volume']).cumsum()
        return ad
    
    @staticmethod
    def chaikin_oscillator(data: pd.DataFrame, fast_period: int = 3, slow_period: int = 10) -> pd.Series:
        """
        Calculate Chaikin Oscillator.
        
        Args:
            data: DataFrame with OHLC and volume columns
            fast_period: Fast EMA period
            slow_period: Slow EMA period
            
        Returns:
            Series with Chaikin Oscillator values
        """
        ad = CustomIndicators.accumulation_distribution(data)
        
        fast_ema = ad.ewm(span=fast_period).mean()
        slow_ema = ad.ewm(span=slow_period).mean()
        
        return fast_ema - slow_ema
    
    @staticmethod
    def support_resistance_levels(data: pd.DataFrame, window: int = 20) -> pd.DataFrame:
        """
        Identify support and resistance levels using local extrema.
        
        Args:
            data: DataFrame with OHLC columns
            window: Window for finding extrema
            
        Returns:
            DataFrame with support and resistance levels
        """
        # Find local maxima and minima
        high_prices = data['high'].values
        low_prices = data['low'].values
        
        # Find peaks (resistance) and valleys (support)
        resistance_indices = argrelextrema(high_prices, np.greater, order=window)[0]
        support_indices = argrelextrema(low_prices, np.less, order=window)[0]
        
        # Create result DataFrame
        result = pd.DataFrame(index=data.index)
        result['resistance'] = np.nan
        result['support'] = np.nan
        
        # Set resistance and support levels
        for idx in resistance_indices:
            if idx < len(result):
                result.iloc[idx, result.columns.get_loc('resistance')] = high_prices[idx]
        
        for idx in support_indices:
            if idx < len(result):
                result.iloc[idx, result.columns.get_loc('support')] = low_prices[idx]
        
        # Forward fill to maintain levels
        result['resistance'] = result['resistance'].fillna(method='ffill')
        result['support'] = result['support'].fillna(method='ffill')
        
        return result
    
    @staticmethod
    def trend_strength(data: pd.DataFrame, period: int = 14) -> pd.Series:
        """
        Calculate trend strength indicator.
        
        Args:
            data: DataFrame with OHLC columns
            period: Period for calculation
            
        Returns:
            Series with trend strength values (-1 to 1)
        """
        # Calculate price momentum
        momentum = data['close'].pct_change(period)
        
        # Calculate volatility
        volatility = data['close'].rolling(window=period).std()
        
        # Calculate trend strength
        trend_strength = momentum / volatility
        
        # Normalize to -1 to 1 range
        trend_strength = np.tanh(trend_strength)
        
        return trend_strength
    
    @staticmethod
    def volume_profile(data: pd.DataFrame, bins: int = 20) -> pd.DataFrame:
        """
        Calculate volume profile for price levels.
        
        Args:
            data: DataFrame with OHLC and volume columns
            bins: Number of price bins
            
        Returns:
            DataFrame with volume profile
        """
        # Create price bins
        price_min = data['low'].min()
        price_max = data['high'].max()
        price_bins = np.linspace(price_min, price_max, bins + 1)
        
        # Calculate volume for each price level
        volume_profile = pd.DataFrame({
            'price_level': (price_bins[:-1] + price_bins[1:]) / 2,
            'volume': 0.0
        })
        
        # Distribute volume across price levels
        for i in range(len(data)):
            typical_price = (data['high'].iloc[i] + data['low'].iloc[i] + data['close'].iloc[i]) / 3
            volume = data['volume'].iloc[i]
            
            # Find appropriate bin
            bin_idx = np.digitize(typical_price, price_bins) - 1
            bin_idx = max(0, min(bin_idx, len(volume_profile) - 1))
            
            volume_profile.loc[bin_idx, 'volume'] += volume
        
        return volume_profile

================================================================================
FILE: src\indicators\signal_generator.py
DIRECTORY: src\indicators
SIZE: 27426 bytes
================================================================================

"""
Signal Generator for Quotex Trading Bot
This module generates trading signals based on technical indicators and custom strategies.
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Union
from enum import Enum
import talib
from .custom_indicators import CustomIndicators
from .technical_indicators import TechnicalIndicators
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SignalType(Enum):
    """Signal types for trading decisions."""
    BUY = "BUY"
    SELL = "SELL"
    HOLD = "HOLD"
    STRONG_BUY = "STRONG_BUY"
    STRONG_SELL = "STRONG_SELL"


class SignalStrength(Enum):
    """Signal strength levels."""
    WEAK = 1
    MODERATE = 2
    STRONG = 3
    VERY_STRONG = 4


class TradingSignal:
    """Class to represent a trading signal."""
    
    def __init__(self, signal_type: SignalType, strength: SignalStrength, 
                 confidence: float, price: float, timestamp: pd.Timestamp,
                 indicators: Dict, reason: str = ""):
        self.signal_type = signal_type
        self.strength = strength
        self.confidence = confidence  # 0.0 to 1.0
        self.price = price
        self.timestamp = timestamp
        self.indicators = indicators
        self.reason = reason
    
    def __repr__(self):
        return f"TradingSignal({self.signal_type.value}, {self.strength.value}, {self.confidence:.2f})"


class SignalGenerator:
    """
    Main signal generator class that combines multiple indicators to generate trading signals.
    """
    
    def __init__(self, config: Dict = None):
        """
        Initialize the signal generator.
        
        Args:
            config: Configuration dictionary with parameters
        """
        self.config = config or self._default_config()
        self.custom_indicators = CustomIndicators()
        self.technical_indicators = TechnicalIndicators()
        
    def _default_config(self) -> Dict:
        """Default configuration for signal generation."""
        return {
            'rsi_period': 14,
            'rsi_oversold': 30,
            'rsi_overbought': 70,
            'macd_fast': 12,
            'macd_slow': 26,
            'macd_signal': 9,
            'bb_period': 20,
            'bb_std': 2,
            'stoch_k': 14,
            'stoch_d': 3,
            'adx_period': 14,
            'adx_threshold': 25,
            'volume_threshold': 1.5,
            'trend_strength_threshold': 0.3,
            'signal_cooldown': 5,  # minutes
            'min_confidence': 0.6
        }
    
    def generate_signals(self, data: pd.DataFrame) -> List[TradingSignal]:
        """
        Generate trading signals from market data.
        
        Args:
            data: DataFrame with OHLC and volume data
            
        Returns:
            List of trading signals
        """
        signals = []
        
        # Calculate all indicators
        indicators = self._calculate_indicators(data)
        
        # Generate signals for each timestamp
        for i in range(len(data)):
            if i < 50:  # Skip first 50 candles for indicator warmup
                continue
                
            signal = self._generate_signal_for_timestamp(data, indicators, i)
            if signal and signal.confidence >= self.config['min_confidence']:
                signals.append(signal)
        
        return signals
    
    def _calculate_indicators(self, data: pd.DataFrame) -> Dict:
        """Calculate all technical indicators."""
        indicators = {}
        
        # Basic indicators
        indicators['rsi'] = talib.RSI(data['close'], timeperiod=self.config['rsi_period'])
        indicators['macd'], indicators['macd_signal'], indicators['macd_hist'] = talib.MACD(
            data['close'], 
            fastperiod=self.config['macd_fast'],
            slowperiod=self.config['macd_slow'],
            signalperiod=self.config['macd_signal']
        )
        
        # Bollinger Bands
        indicators['bb_upper'], indicators['bb_middle'], indicators['bb_lower'] = talib.BBANDS(
            data['close'],
            timeperiod=self.config['bb_period'],
            nbdevup=self.config['bb_std'],
            nbdevdn=self.config['bb_std']
        )
        
        # Stochastic
        indicators['stoch_k'], indicators['stoch_d'] = talib.STOCH(
            data['high'], data['low'], data['close'],
            fastk_period=self.config['stoch_k'],
            slowk_period=self.config['stoch_d'],
            slowd_period=self.config['stoch_d']
        )
        
        # ADX
        indicators['adx'] = talib.ADX(
            data['high'], data['low'], data['close'],
            timeperiod=self.config['adx_period']
        )
        
        # Moving Averages
        indicators['sma_20'] = talib.SMA(data['close'], timeperiod=20)
        indicators['sma_50'] = talib.SMA(data['close'], timeperiod=50)
        indicators['ema_12'] = talib.EMA(data['close'], timeperiod=12)
        indicators['ema_26'] = talib.EMA(data['close'], timeperiod=26)
        
        # Volume indicators
        indicators['volume_sma'] = data['volume'].rolling(window=20).mean()
        indicators['volume_ratio'] = data['volume'] / indicators['volume_sma']
        
        # Custom indicators
        indicators['supertrend'] = self.custom_indicators.supertrend(data)
        indicators['ichimoku'] = self.custom_indicators.ichimoku_cloud(data)
        indicators['heikin_ashi'] = self.custom_indicators.heikin_ashi(data)
        indicators['mfi'] = self.custom_indicators.money_flow_index(data)
        indicators['trend_strength'] = self.custom_indicators.trend_strength(data)
        indicators['support_resistance'] = self.custom_indicators.support_resistance_levels(data)
        
        return indicators
    
    def _generate_signal_for_timestamp(self, data: pd.DataFrame, indicators: Dict, idx: int) -> Optional[TradingSignal]:
        """Generate signal for a specific timestamp."""
        
        # Get current values
        current_price = data['close'].iloc[idx]
        current_time = data.index[idx]
        
        # Calculate individual signal components
        rsi_signal = self._rsi_signal(indicators['rsi'].iloc[idx])
        macd_signal = self._macd_signal(
            indicators['macd'].iloc[idx],
            indicators['macd_signal'].iloc[idx],
            indicators['macd_hist'].iloc[idx]
        )
        bb_signal = self._bollinger_bands_signal(
            current_price,
            indicators['bb_upper'].iloc[idx],
            indicators['bb_middle'].iloc[idx],
            indicators['bb_lower'].iloc[idx]
        )
        stoch_signal = self._stochastic_signal(
            indicators['stoch_k'].iloc[idx],
            indicators['stoch_d'].iloc[idx]
        )
        trend_signal = self._trend_signal(
            indicators['sma_20'].iloc[idx],
            indicators['sma_50'].iloc[idx],
            current_price
        )
        volume_signal = self._volume_signal(indicators['volume_ratio'].iloc[idx])
        supertrend_signal = self._supertrend_signal(indicators['supertrend'], idx)
        
        # Combine signals
        signal_scores = {
            'rsi': rsi_signal,
            'macd': macd_signal,
            'bb': bb_signal,
            'stoch': stoch_signal,
            'trend': trend_signal,
            'volume': volume_signal,
            'supertrend': supertrend_signal
        }
        
        # Calculate overall signal
        overall_signal, confidence = self._combine_signals(signal_scores)
        
        # Determine signal strength
        strength = self._calculate_signal_strength(signal_scores, confidence)
        
        # Create reason string
        reason = self._create_reason_string(signal_scores)
        
        if overall_signal != SignalType.HOLD:
            return TradingSignal(
                signal_type=overall_signal,
                strength=strength,
                confidence=confidence,
                price=current_price,
                timestamp=current_time,
                indicators=signal_scores,
                reason=reason
            )
        
        return None
    
    def _rsi_signal(self, rsi_value: float) -> float:
        """Generate RSI signal (-1 to 1)."""
        if pd.isna(rsi_value):
            return 0.0
        
        if rsi_value <= self.config['rsi_oversold']:
            return 1.0  # Buy signal
        elif rsi_value >= self.config['rsi_overbought']:
            return -1.0  # Sell signal
        else:
            # Gradual transition
            if rsi_value < 50:
                return (50 - rsi_value) / (50 - self.config['rsi_oversold'])
            else:
                return -(rsi_value - 50) / (self.config['rsi_overbought'] - 50)
    
    def _macd_signal(self, macd: float, signal: float, histogram: float) -> float:
        """Generate MACD signal (-1 to 1)."""
        if pd.isna(macd) or pd.isna(signal) or pd.isna(histogram):
            return 0.0
        
        # MACD crossover
        if macd > signal and histogram > 0:
            return min(1.0, histogram * 10)  # Buy signal
        elif macd < signal and histogram < 0:
            return max(-1.0, histogram * 10)  # Sell signal
        else:
            return 0.0
    
    def _bollinger_bands_signal(self, price: float, upper: float, middle: float, lower: float) -> float:
        """Generate Bollinger Bands signal (-1 to 1)."""
        if pd.isna(upper) or pd.isna(lower) or pd.isna(middle):
            return 0.0
        
        band_width = upper - lower
        if band_width == 0:
            return 0.0
        
        # Calculate position within bands
        position = (price - lower) / band_width
        
        if position <= 0.1:  # Near lower band
            return 0.8  # Buy signal
        elif position >= 0.9:  # Near upper band
            return -0.8  # Sell signal
        else:
            return 0.0
    
    def _stochastic_signal(self, k: float, d: float) -> float:
        """Generate Stochastic signal (-1 to 1)."""
        if pd.isna(k) or pd.isna(d):
            return 0.0
        
        if k <= 20 and d <= 20:
            return 0.7  # Buy signal
        elif k >= 80 and d >= 80:
            return -0.7  # Sell signal
        else:
            return 0.0
    
    def _trend_signal(self, sma_20: float, sma_50: float, price: float) -> float:
        """Generate trend signal (-1 to 1)."""
        if pd.isna(sma_20) or pd.isna(sma_50):
            return 0.0
        
        # Moving average crossover
        if sma_20 > sma_50 and price > sma_20:
            return 0.6  # Bullish trend
        elif sma_20 < sma_50 and price < sma_20:
            return -0.6  # Bearish trend
        else:
            return 0.0
    
    def _volume_signal(self, volume_ratio: float) -> float:
        """Generate volume signal (-1 to 1)."""
        if pd.isna(volume_ratio):
            return 0.0
        
        if volume_ratio > self.config['volume_threshold']:
            return 0.3  # High volume confirmation
        else:
            return 0.0
    
    def _supertrend_signal(self, supertrend_data: pd.DataFrame, idx: int) -> float:
        """Generate SuperTrend signal (-1 to 1)."""
        if idx >= len(supertrend_data):
            return 0.0
        
        direction = supertrend_data['direction'].iloc[idx]
        if pd.isna(direction):
            return 0.0
        
        return 0.5 * direction  # 0.5 for buy, -0.5 for sell
    
    def _combine_signals(self, signal_scores: Dict[str, float]) -> Tuple[SignalType, float]:
        """Combine individual signals into overall signal."""
        
        # Weighted combination
        weights = {
            'rsi': 0.2,
            'macd': 0.2,
            'bb': 0.15,
            'stoch': 0.15,
            'trend': 0.15,
            'volume': 0.05,
            'supertrend': 0.1
        }
        
        weighted_score = sum(signal_scores[key] * weights[key] for key in weights.keys())
        
        # Calculate confidence based on signal agreement
        positive_signals = sum(1 for score in signal_scores.values() if score > 0.1)
        negative_signals = sum(1 for score in signal_scores.values() if score < -0.1)
        total_signals = len(signal_scores)
        
        if positive_signals > negative_signals:
            confidence = positive_signals / total_signals
            signal_type = SignalType.STRONG_BUY if weighted_score > 0.5 else SignalType.BUY
        elif negative_signals > positive_signals:
            confidence = negative_signals / total_signals
            signal_type = SignalType.STRONG_SELL if weighted_score < -0.5 else SignalType.SELL
        else:
            confidence = 0.0
            signal_type = SignalType.HOLD
        
        return signal_type, confidence
    
    def _calculate_signal_strength(self, signal_scores: Dict[str, float], confidence: float) -> SignalStrength:
        """Calculate signal strength based on scores and confidence."""
        
        avg_score = abs(np.mean(list(signal_scores.values())))
        
        if confidence >= 0.8 and avg_score >= 0.6:
            return SignalStrength.VERY_STRONG
        elif confidence >= 0.7 and avg_score >= 0.4:
            return SignalStrength.STRONG
        elif confidence >= 0.6 and avg_score >= 0.2:
            return SignalStrength.MODERATE
        else:
            return SignalStrength.WEAK
    
    def _create_reason_string(self, signal_scores: Dict[str, float]) -> str:
        """Create a human-readable reason string for the signal."""
        
        positive_indicators = []
        negative_indicators = []
        
        for indicator, score in signal_scores.items():
            if score > 0.1:
                positive_indicators.append(f"{indicator.upper()}({score:.2f})")
            elif score < -0.1:
                negative_indicators.append(f"{indicator.upper()}({score:.2f})")
        
        reason_parts = []
        if positive_indicators:
            reason_parts.append(f"Bullish: {', '.join(positive_indicators)}")
        if negative_indicators:
            reason_parts.append(f"Bearish: {', '.join(negative_indicators)}")
        
        return " | ".join(reason_parts) if reason_parts else "Neutral signals"
    
    def get_signal_summary(self, signals: List[TradingSignal]) -> Dict:
        """Get summary statistics of generated signals."""
        
        if not signals:
            return {
                'total_signals': 0,
                'buy_signals': 0,
                'sell_signals': 0,
                'strong_signals': 0,
                'avg_confidence': 0.0
            }
        
        buy_count = sum(1 for s in signals if s.signal_type in [SignalType.BUY, SignalType.STRONG_BUY])
        sell_count = sum(1 for s in signals if s.signal_type in [SignalType.SELL, SignalType.STRONG_SELL])
        strong_count = sum(1 for s in signals if s.strength in [SignalStrength.STRONG, SignalStrength.VERY_STRONG])
        avg_confidence = np.mean([s.confidence for s in signals])
        
        return {
            'total_signals': len(signals),
            'buy_signals': buy_count,
            'sell_signals': sell_count,
            'strong_signals': strong_count,
            'avg_confidence': avg_confidence,
            'signal_distribution': {
                'STRONG_BUY': sum(1 for s in signals if s.signal_type == SignalType.STRONG_BUY),
                'BUY': sum(1 for s in signals if s.signal_type == SignalType.BUY),
                'SELL': sum(1 for s in signals if s.signal_type == SignalType.SELL),
                'STRONG_SELL': sum(1 for s in signals if s.signal_type == SignalType.STRONG_SELL)
            }
        }
    
    def filter_signals_by_timeframe(self, signals: List[TradingSignal], 
                                   timeframe: str = '5T') -> List[TradingSignal]:
        """Filter signals to avoid too frequent signals in the same timeframe."""
        
        if not signals:
            return signals
        
        filtered_signals = []
        last_signal_time = None
        
        for signal in signals:
            if last_signal_time is None:
                filtered_signals.append(signal)
                last_signal_time = signal.timestamp
            else:
                # Check if enough time has passed
                time_diff = signal.timestamp - last_signal_time
                if time_diff >= pd.Timedelta(timeframe):
                    filtered_signals.append(signal)
                    last_signal_time = signal.timestamp
        
        return filtered_signals
    
    def get_current_market_sentiment(self, data: pd.DataFrame) -> Dict:
        """Analyze current market sentiment based on recent data."""
        
        if len(data) < 20:
            return {'sentiment': 'NEUTRAL', 'confidence': 0.0}
        
        # Calculate indicators for sentiment analysis
        indicators = self._calculate_indicators(data)
        
        # Get recent values (last 5 candles)
        recent_idx = len(data) - 5
        
        sentiment_scores = []
        
        # RSI sentiment
        rsi_recent = indicators['rsi'].iloc[recent_idx:].mean()
        if rsi_recent < 30:
            sentiment_scores.append(1.0)  # Bullish (oversold)
        elif rsi_recent > 70:
            sentiment_scores.append(-1.0)  # Bearish (overbought)
        else:
            sentiment_scores.append(0.0)
        
        # MACD sentiment
        macd_recent = indicators['macd'].iloc[recent_idx:].mean()
        macd_signal_recent = indicators['macd_signal'].iloc[recent_idx:].mean()
        if macd_recent > macd_signal_recent:
            sentiment_scores.append(0.5)
        else:
            sentiment_scores.append(-0.5)
        
        # Trend sentiment
        sma_20_recent = indicators['sma_20'].iloc[recent_idx:].mean()
        sma_50_recent = indicators['sma_50'].iloc[recent_idx:].mean()
        current_price = data['close'].iloc[-1]
        
        if current_price > sma_20_recent > sma_50_recent:
            sentiment_scores.append(1.0)
        elif current_price < sma_20_recent < sma_50_recent:
            sentiment_scores.append(-1.0)
        else:
            sentiment_scores.append(0.0)
        
        # Volume sentiment
        volume_ratio_recent = indicators['volume_ratio'].iloc[recent_idx:].mean()
        if volume_ratio_recent > 1.5:
            sentiment_scores.append(0.3)  # High volume is generally bullish
        else:
            sentiment_scores.append(0.0)
        
        # Calculate overall sentiment
        avg_sentiment = np.mean(sentiment_scores)
        sentiment_confidence = 1.0 - (np.std(sentiment_scores) / 2.0)  # Lower std = higher confidence
        
        if avg_sentiment > 0.2:
            sentiment = 'BULLISH'
        elif avg_sentiment < -0.2:
            sentiment = 'BEARISH'
        else:
            sentiment = 'NEUTRAL'
        
        return {
            'sentiment': sentiment,
            'confidence': max(0.0, min(1.0, sentiment_confidence)),
            'score': avg_sentiment,
            'components': {
                'rsi': sentiment_scores[0],
                'macd': sentiment_scores[1],
                'trend': sentiment_scores[2],
                'volume': sentiment_scores[3]
            }
        }
    
    def backtest_signals(self, data: pd.DataFrame, signals: List[TradingSignal], 
                        initial_balance: float = 10000.0) -> Dict:
        """Simple backtest of generated signals."""
        
        balance = initial_balance
        position = 0  # 0 = no position, 1 = long, -1 = short
        trades = []
        entry_price = 0.0
        
        for signal in signals:
            current_price = signal.price
            
            if signal.signal_type in [SignalType.BUY, SignalType.STRONG_BUY] and position <= 0:
                # Enter long position
                if position == -1:  # Close short position
                    pnl = entry_price - current_price
                    balance += pnl
                    trades.append({
                        'type': 'SHORT_CLOSE',
                        'price': current_price,
                        'pnl': pnl,
                        'balance': balance,
                        'timestamp': signal.timestamp
                    })
                
                # Enter long
                position = 1
                entry_price = current_price
                trades.append({
                    'type': 'LONG_OPEN',
                    'price': current_price,
                    'pnl': 0,
                    'balance': balance,
                    'timestamp': signal.timestamp
                })
                
            elif signal.signal_type in [SignalType.SELL, SignalType.STRONG_SELL] and position >= 0:
                # Enter short position
                if position == 1:  # Close long position
                    pnl = current_price - entry_price
                    balance += pnl
                    trades.append({
                        'type': 'LONG_CLOSE',
                        'price': current_price,
                        'pnl': pnl,
                        'balance': balance,
                        'timestamp': signal.timestamp
                    })
                
                # Enter short
                position = -1
                entry_price = current_price
                trades.append({
                    'type': 'SHORT_OPEN',
                    'price': current_price,
                    'pnl': 0,
                    'balance': balance,
                    'timestamp': signal.timestamp
                })
        
        # Calculate performance metrics
        total_return = (balance - initial_balance) / initial_balance * 100
        winning_trades = sum(1 for trade in trades if trade['pnl'] > 0)
        losing_trades = sum(1 for trade in trades if trade['pnl'] < 0)
        total_trades = len([trade for trade in trades if trade['pnl'] != 0])
        
        win_rate = (winning_trades / total_trades * 100) if total_trades > 0 else 0
        
        return {
            'initial_balance': initial_balance,
            'final_balance': balance,
            'total_return': total_return,
            'total_trades': total_trades,
            'winning_trades': winning_trades,
            'losing_trades': losing_trades,
            'win_rate': win_rate,
            'trades': trades
        }
    
    def optimize_parameters(self, data: pd.DataFrame, parameter_ranges: Dict) -> Dict:
        """Optimize signal generation parameters using grid search."""
        
        best_params = None
        best_performance = -float('inf')
        
        # Generate parameter combinations
        param_combinations = self._generate_param_combinations(parameter_ranges)
        
        logger.info(f"Testing {len(param_combinations)} parameter combinations...")
        
        for i, params in enumerate(param_combinations):
            if i % 10 == 0:
                logger.info(f"Testing combination {i+1}/{len(param_combinations)}")
            
            # Update configuration
            old_config = self.config.copy()
            self.config.update(params)
            
            try:
                # Generate signals with new parameters
                signals = self.generate_signals(data)
                
                # Backtest signals
                backtest_result = self.backtest_signals(data, signals)
                
                # Use total return as performance metric
                performance = backtest_result['total_return']
                
                if performance > best_performance:
                    best_performance = performance
                    best_params = params.copy()
                    
            except Exception as e:
                logger.warning(f"Error testing parameters {params}: {e}")
                continue
            finally:
                # Restore original configuration
                self.config = old_config
        
        logger.info(f"Best parameters found with {best_performance:.2f}% return")
        
        return {
            'best_params': best_params,
            'best_performance': best_performance,
            'total_combinations_tested': len(param_combinations)
        }
    
    def _generate_param_combinations(self, parameter_ranges: Dict) -> List[Dict]:
        """Generate all combinations of parameters for optimization."""
        
        import itertools
        
        keys = list(parameter_ranges.keys())
        values = list(parameter_ranges.values())
        
        combinations = []
        for combination in itertools.product(*values):
            param_dict = dict(zip(keys, combination))
            combinations.append(param_dict)
        
        return combinations
    
    def export_signals_to_csv(self, signals: List[TradingSignal], filename: str):
        """Export signals to CSV file."""
        
        if not signals:
            logger.warning("No signals to export")
            return
        
        data = []
        for signal in signals:
            data.append({
                'timestamp': signal.timestamp,
                'signal_type': signal.signal_type.value,
                'strength': signal.strength.value,
                'confidence': signal.confidence,
                'price': signal.price,
                'reason': signal.reason
            })
        
        df = pd.DataFrame(data)
        df.to_csv(filename, index=False)
        logger.info(f"Exported {len(signals)} signals to {filename}")
    
    def load_signals_from_csv(self, filename: str) -> List[TradingSignal]:
        """Load signals from CSV file."""
        
        try:
            df = pd.read_csv(filename)
            signals = []
            
            for _, row in df.iterrows():
                signal = TradingSignal(
                    signal_type=SignalType(row['signal_type']),
                    strength=SignalStrength(row['strength']),
                    confidence=row['confidence'],
                    price=row['price'],
                    timestamp=pd.to_datetime(row['timestamp']),
                    indicators={},
                    reason=row['reason']
                )
                signals.append(signal)
            
            logger.info(f"Loaded {len(signals)} signals from {filename}")
            return signals
            
        except Exception as e:
            logger.error(f"Error loading signals from {filename}: {e}")
            return []

================================================================================
FILE: src\indicators\technical_indicators.py
DIRECTORY: src\indicators
SIZE: 16894 bytes
================================================================================

"""
Technical Indicators Module for Quotex Trading Bot
Provides standard technical analysis indicators using numpy and pandas
"""

import numpy as np
import pandas as pd
from typing import Tuple, Optional, Union
import warnings
warnings.filterwarnings('ignore')


class TechnicalIndicators:
    """
    A comprehensive collection of technical indicators for trading analysis.
    All methods are static and can be used independently.
    """
    
    @staticmethod
    def sma(data: pd.Series, period: int = 14) -> pd.Series:
        """
        Simple Moving Average
        
        Args:
            data: Price series (typically close prices)
            period: Period for moving average
            
        Returns:
            pd.Series: Simple moving average values
        """
        return data.rolling(window=period).mean()
    
    @staticmethod
    def ema(data: pd.Series, period: int = 14) -> pd.Series:
        """
        Exponential Moving Average
        
        Args:
            data: Price series (typically close prices)
            period: Period for EMA calculation
            
        Returns:
            pd.Series: Exponential moving average values
        """
        return data.ewm(span=period, adjust=False).mean()
    
    @staticmethod
    def wma(data: pd.Series, period: int = 14) -> pd.Series:
        """
        Weighted Moving Average
        
        Args:
            data: Price series
            period: Period for WMA calculation
            
        Returns:
            pd.Series: Weighted moving average values
        """
        weights = np.arange(1, period + 1)
        return data.rolling(window=period).apply(
            lambda x: np.dot(x, weights) / weights.sum(), raw=True
        )
    
    @staticmethod
    def rsi(data: pd.Series, period: int = 14) -> pd.Series:
        """
        Relative Strength Index
        
        Args:
            data: Price series (typically close prices)
            period: Period for RSI calculation
            
        Returns:
            pd.Series: RSI values (0-100)
        """
        delta = data.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    @staticmethod
    def macd(data: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """
        Moving Average Convergence Divergence
        
        Args:
            data: Price series
            fast: Fast EMA period
            slow: Slow EMA period
            signal: Signal line EMA period
            
        Returns:
            Tuple[pd.Series, pd.Series, pd.Series]: MACD line, Signal line, Histogram
        """
        ema_fast = TechnicalIndicators.ema(data, fast)
        ema_slow = TechnicalIndicators.ema(data, slow)
        macd_line = ema_fast - ema_slow
        signal_line = TechnicalIndicators.ema(macd_line, signal)
        histogram = macd_line - signal_line
        return macd_line, signal_line, histogram
    
    @staticmethod
    def bollinger_bands(data: pd.Series, period: int = 20, std_dev: float = 2.0) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """
        Bollinger Bands
        
        Args:
            data: Price series
            period: Period for moving average
            std_dev: Standard deviation multiplier
            
        Returns:
            Tuple[pd.Series, pd.Series, pd.Series]: Upper band, Middle band (SMA), Lower band
        """
        sma = TechnicalIndicators.sma(data, period)
        std = data.rolling(window=period).std()
        upper_band = sma + (std * std_dev)
        lower_band = sma - (std * std_dev)
        return upper_band, sma, lower_band
    
    @staticmethod
    def stochastic(high: pd.Series, low: pd.Series, close: pd.Series, 
                   k_period: int = 14, d_period: int = 3) -> Tuple[pd.Series, pd.Series]:
        """
        Stochastic Oscillator
        
        Args:
            high: High price series
            low: Low price series
            close: Close price series
            k_period: Period for %K calculation
            d_period: Period for %D calculation
            
        Returns:
            Tuple[pd.Series, pd.Series]: %K, %D
        """
        lowest_low = low.rolling(window=k_period).min()
        highest_high = high.rolling(window=k_period).max()
        k_percent = 100 * ((close - lowest_low) / (highest_high - lowest_low))
        d_percent = k_percent.rolling(window=d_period).mean()
        return k_percent, d_percent
    
    @staticmethod
    def atr(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14) -> pd.Series:
        """
        Average True Range
        
        Args:
            high: High price series
            low: Low price series
            close: Close price series
            period: Period for ATR calculation
            
        Returns:
            pd.Series: ATR values
        """
        tr1 = high - low
        tr2 = abs(high - close.shift(1))
        tr3 = abs(low - close.shift(1))
        true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        return true_range.rolling(window=period).mean()
    
    @staticmethod
    def adx(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14) -> pd.Series:
        """
        Average Directional Index
        
        Args:
            high: High price series
            low: Low price series
            close: Close price series
            period: Period for ADX calculation
            
        Returns:
            pd.Series: ADX values
        """
        tr = TechnicalIndicators.atr(high, low, close, 1)
        
        # Calculate directional movement
        dm_plus = high.diff()
        dm_minus = -low.diff()
        
        # Only keep positive directional movements
        dm_plus = dm_plus.where((dm_plus > dm_minus) & (dm_plus > 0), 0)
        dm_minus = dm_minus.where((dm_minus > dm_plus) & (dm_minus > 0), 0)
        
        # Calculate directional indicators
        di_plus = 100 * (dm_plus.rolling(window=period).sum() / tr.rolling(window=period).sum())
        di_minus = 100 * (dm_minus.rolling(window=period).sum() / tr.rolling(window=period).sum())
        
        # Calculate ADX
        dx = 100 * abs(di_plus - di_minus) / (di_plus + di_minus)
        adx = dx.rolling(window=period).mean()
        
        return adx
    
    @staticmethod
    def cci(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 20) -> pd.Series:
        """
        Commodity Channel Index
        
        Args:
            high: High price series
            low: Low price series
            close: Close price series
            period: Period for CCI calculation
            
        Returns:
            pd.Series: CCI values
        """
        typical_price = (high + low + close) / 3
        sma_tp = typical_price.rolling(window=period).mean()
        mad = typical_price.rolling(window=period).apply(
            lambda x: pd.Series(x).mad(), raw=False
        )
        cci = (typical_price - sma_tp) / (0.015 * mad)
        return cci
    
    @staticmethod
    def williams_r(high: pd.Series, low: pd.Series, close: pd.Series, period: int = 14) -> pd.Series:
        """
        Williams %R
        
        Args:
            high: High price series
            low: Low price series
            close: Close price series
            period: Period for Williams %R calculation
            
        Returns:
            pd.Series: Williams %R values
        """
        highest_high = high.rolling(window=period).max()
        lowest_low = low.rolling(window=period).min()
        williams_r = -100 * (highest_high - close) / (highest_high - lowest_low)
        return williams_r
    
    @staticmethod
    def roc(data: pd.Series, period: int = 12) -> pd.Series:
        """
        Rate of Change
        
        Args:
            data: Price series
            period: Period for ROC calculation
            
        Returns:
            pd.Series: ROC values
        """
        return ((data - data.shift(period)) / data.shift(period)) * 100
    
    @staticmethod
    def momentum(data: pd.Series, period: int = 10) -> pd.Series:
        """
        Momentum
        
        Args:
            data: Price series
            period: Period for momentum calculation
            
        Returns:
            pd.Series: Momentum values
        """
        return data - data.shift(period)
    
    @staticmethod
    def obv(close: pd.Series, volume: pd.Series) -> pd.Series:
        """
        On-Balance Volume
        
        Args:
            close: Close price series
            volume: Volume series
            
        Returns:
            pd.Series: OBV values
        """
        obv = pd.Series(index=close.index, dtype=float)
        obv.iloc[0] = volume.iloc[0]
        
        for i in range(1, len(close)):
            if close.iloc[i] > close.iloc[i-1]:
                obv.iloc[i] = obv.iloc[i-1] + volume.iloc[i]
            elif close.iloc[i] < close.iloc[i-1]:
                obv.iloc[i] = obv.iloc[i-1] - volume.iloc[i]
            else:
                obv.iloc[i] = obv.iloc[i-1]
        
        return obv
    
    @staticmethod
    def mfi(high: pd.Series, low: pd.Series, close: pd.Series, volume: pd.Series, period: int = 14) -> pd.Series:
        """
        Money Flow Index
        
        Args:
            high: High price series
            low: Low price series
            close: Close price series
            volume: Volume series
            period: Period for MFI calculation
            
        Returns:
            pd.Series: MFI values
        """
        typical_price = (high + low + close) / 3
        raw_money_flow = typical_price * volume
        
        positive_flow = raw_money_flow.where(typical_price > typical_price.shift(1), 0)
        negative_flow = raw_money_flow.where(typical_price < typical_price.shift(1), 0)
        
        positive_mf = positive_flow.rolling(window=period).sum()
        negative_mf = negative_flow.rolling(window=period).sum()
        
        mfi = 100 - (100 / (1 + positive_mf / negative_mf))
        return mfi
    
    @staticmethod
    def vwap(high: pd.Series, low: pd.Series, close: pd.Series, volume: pd.Series) -> pd.Series:
        """
        Volume Weighted Average Price
        
        Args:
            high: High price series
            low: Low price series
            close: Close price series
            volume: Volume series
            
        Returns:
            pd.Series: VWAP values
        """
        typical_price = (high + low + close) / 3
        return (typical_price * volume).cumsum() / volume.cumsum()
    
    @staticmethod
    def parabolic_sar(high: pd.Series, low: pd.Series, acceleration: float = 0.02, maximum: float = 0.2) -> pd.Series:
        """
        Parabolic SAR
        
        Args:
            high: High price series
            low: Low price series
            acceleration: Acceleration factor
            maximum: Maximum acceleration factor
            
        Returns:
            pd.Series: Parabolic SAR values
        """
        sar = pd.Series(index=high.index, dtype=float)
        trend = pd.Series(index=high.index, dtype=int)
        af = pd.Series(index=high.index, dtype=float)
        ep = pd.Series(index=high.index, dtype=float)
        
        # Initialize
        sar.iloc[0] = low.iloc[0]
        trend.iloc[0] = 1  # 1 for uptrend, -1 for downtrend
        af.iloc[0] = acceleration
        ep.iloc[0] = high.iloc[0]
        
        for i in range(1, len(high)):
            if trend.iloc[i-1] == 1:  # Uptrend
                sar.iloc[i] = sar.iloc[i-1] + af.iloc[i-1] * (ep.iloc[i-1] - sar.iloc[i-1])
                
                if low.iloc[i] <= sar.iloc[i]:
                    trend.iloc[i] = -1
                    sar.iloc[i] = ep.iloc[i-1]
                    af.iloc[i] = acceleration
                    ep.iloc[i] = low.iloc[i]
                else:
                    trend.iloc[i] = 1
                    if high.iloc[i] > ep.iloc[i-1]:
                        ep.iloc[i] = high.iloc[i]
                        af.iloc[i] = min(af.iloc[i-1] + acceleration, maximum)
                    else:
                        ep.iloc[i] = ep.iloc[i-1]
                        af.iloc[i] = af.iloc[i-1]
            else:  # Downtrend
                sar.iloc[i] = sar.iloc[i-1] - af.iloc[i-1] * (sar.iloc[i-1] - ep.iloc[i-1])
                
                if high.iloc[i] >= sar.iloc[i]:
                    trend.iloc[i] = 1
                    sar.iloc[i] = ep.iloc[i-1]
                    af.iloc[i] = acceleration
                    ep.iloc[i] = high.iloc[i]
                else:
                    trend.iloc[i] = -1
                    if low.iloc[i] < ep.iloc[i-1]:
                        ep.iloc[i] = low.iloc[i]
                        af.iloc[i] = min(af.iloc[i-1] + acceleration, maximum)
                    else:
                        ep.iloc[i] = ep.iloc[i-1]
                        af.iloc[i] = af.iloc[i-1]
        
        return sar
    
    @staticmethod
    def fibonacci_retracement(high: float, low: float) -> dict:
        """
        Calculate Fibonacci retracement levels
        
        Args:
            high: High price
            low: Low price
            
        Returns:
            dict: Fibonacci levels
        """
        diff = high - low
        return {
            '0%': high,
            '23.6%': high - 0.236 * diff,
            '38.2%': high - 0.382 * diff,
            '50%': high - 0.5 * diff,
            '61.8%': high - 0.618 * diff,
            '78.6%': high - 0.786 * diff,
            '100%': low
        }
    
    @staticmethod
    def pivot_points(high: float, low: float, close: float) -> dict:
        """
        Calculate pivot points and support/resistance levels
        
        Args:
            high: Previous period high
            low: Previous period low
            close: Previous period close
            
        Returns:
            dict: Pivot points and levels
        """
        pivot = (high + low + close) / 3
        
        return {
            'pivot': pivot,
            'r1': 2 * pivot - low,
            'r2': pivot + (high - low),
            'r3': high + 2 * (pivot - low),
            's1': 2 * pivot - high,
            's2': pivot - (high - low),
            's3': low - 2 * (high - pivot)
        }
    
    @staticmethod
    def ichimoku_cloud(high: pd.Series, low: pd.Series, close: pd.Series,
                      conversion_period: int = 9, base_period: int = 26,
                      leading_span_b_period: int = 52, displacement: int = 26) -> dict:
        """
        Ichimoku Cloud components
        
        Args:
            high: High price series
            low: Low price series
            close: Close price series
            conversion_period: Conversion line period
            base_period: Base line period
            leading_span_b_period: Leading span B period
            displacement: Displacement for leading spans
            
        Returns:
            dict: Ichimoku components
        """
        # Conversion Line (Tenkan-sen)
        conversion_line = (high.rolling(window=conversion_period).max() + 
                          low.rolling(window=conversion_period).min()) / 2
        
        # Base Line (Kijun-sen)
        base_line = (high.rolling(window=base_period).max() + 
                    low.rolling(window=base_period).min()) / 2
        
        # Leading Span A (Senkou Span A)
        leading_span_a = ((conversion_line + base_line) / 2).shift(displacement)
        
        # Leading Span B (Senkou Span B)
        leading_span_b = ((high.rolling(window=leading_span_b_period).max() + 
                          low.rolling(window=leading_span_b_period).min()) / 2).shift(displacement)
        
        # Lagging Span (Chikou Span)
        lagging_span = close.shift(-displacement)
        
        return {
            'conversion_line': conversion_line,
            'base_line': base_line,
            'leading_span_a': leading_span_a,
            'leading_span_b': leading_span_b,
            'lagging_span': lagging_span
        }

================================================================================
FILE: src\ml\__init__.py
DIRECTORY: src\ml
SIZE: 0 bytes
================================================================================



================================================================================
FILE: src\ml\feature_engineering.py
DIRECTORY: src\ml
SIZE: 21698 bytes
================================================================================

"""
Model Trainer for Quotex Trading Bot
Handles training, validation, and management of ML models for trading predictions
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import joblib
import os
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import json

from ..utils.logger import setup_logger
from .feature_engineering import FeatureEngineer
from ..database.database_manager import DatabaseManager


class ModelTrainer:
    """
    Advanced model trainer for trading predictions with multiple ML algorithms
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the ModelTrainer
        
        Args:
            config: Configuration dictionary containing model parameters
        """
        self.config = config
        self.logger = setup_logger(__name__)
        self.feature_engineer = FeatureEngineer(config)
        self.db_manager = DatabaseManager(config)
        
        # Model storage paths
        self.model_dir = config.get('model_dir', 'data/models')
        self.scaler_dir = config.get('scaler_dir', 'data/scalers')
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs(self.scaler_dir, exist_ok=True)
        
        # Initialize models
        self.models = self._initialize_models()
        self.scalers = {}
        self.trained_models = {}
        self.model_metrics = {}
        
        # Training parameters
        self.test_size = config.get('test_size', 0.2)
        self.validation_size = config.get('validation_size', 0.2)
        self.random_state = config.get('random_state', 42)
        self.cv_folds = config.get('cv_folds', 5)
        
    def _initialize_models(self) -> Dict[str, Any]:
        """Initialize ML models with default parameters"""
        return {
            'random_forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=self.random_state,
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=self.random_state
            ),
            'logistic_regression': LogisticRegression(
                random_state=self.random_state,
                max_iter=1000,
                solver='liblinear'
            ),
            'svm': SVC(
                kernel='rbf',
                C=1.0,
                gamma='scale',
                random_state=self.random_state,
                probability=True
            ),
            'neural_network': MLPClassifier(
                hidden_layer_sizes=(100, 50),
                max_iter=1000,
                random_state=self.random_state,
                early_stopping=True,
                validation_fraction=0.1
            )
        }
    
    def prepare_training_data(self, 
                            symbol: str, 
                            timeframe: str, 
                            start_date: Optional[str] = None,
                            end_date: Optional[str] = None) -> Tuple[pd.DataFrame, pd.Series]:
        """
        Prepare training data with features and labels
        
        Args:
            symbol: Trading symbol
            timeframe: Timeframe for data
            start_date: Start date for data (optional)
            end_date: End date for data (optional)
            
        Returns:
            Tuple of (features_df, labels_series)
        """
        try:
            # Get historical data
            historical_data = self.db_manager.get_historical_data(
                symbol=symbol,
                timeframe=timeframe,
                start_date=start_date,
                end_date=end_date
            )
            
            if historical_data.empty:
                raise ValueError(f"No historical data found for {symbol} {timeframe}")
            
            # Generate features
            features_df = self.feature_engineer.generate_features(historical_data)
            
            # Generate labels (1 for up, 0 for down)
            labels = self._generate_labels(historical_data)
            
            # Remove NaN values
            combined_df = features_df.join(labels, how='inner')
            combined_df = combined_df.dropna()
            
            features_df = combined_df.iloc[:, :-1]
            labels_series = combined_df.iloc[:, -1]
            
            self.logger.info(f"Prepared training data: {len(features_df)} samples, {len(features_df.columns)} features")
            
            return features_df, labels_series
            
        except Exception as e:
            self.logger.error(f"Error preparing training data: {str(e)}")
            raise
    
    def _generate_labels(self, data: pd.DataFrame, 
                        lookforward: int = 1) -> pd.Series:
        """
        Generate trading labels based on future price movement
        
        Args:
            data: Historical price data
            lookforward: Number of periods to look forward
            
        Returns:
            Series of labels (1 for up, 0 for down)
        """
        try:
            # Calculate future returns
            future_returns = data['close'].pct_change(lookforward).shift(-lookforward)
            
            # Generate binary labels
            labels = (future_returns > 0).astype(int)
            labels.name = 'label'
            
            return labels
            
        except Exception as e:
            self.logger.error(f"Error generating labels: {str(e)}")
            raise
    
    def train_model(self, 
                   model_name: str,
                   features: pd.DataFrame,
                   labels: pd.Series,
                   optimize_hyperparameters: bool = False) -> Dict[str, Any]:
        """
        Train a specific model
        
        Args:
            model_name: Name of the model to train
            features: Feature matrix
            labels: Target labels
            optimize_hyperparameters: Whether to perform hyperparameter optimization
            
        Returns:
            Dictionary containing training results
        """
        try:
            if model_name not in self.models:
                raise ValueError(f"Model {model_name} not available")
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                features, labels, 
                test_size=self.test_size, 
                random_state=self.random_state,
                stratify=labels
            )
            
            # Scale features
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Store scaler
            self.scalers[model_name] = scaler
            
            # Get model
            model = self.models[model_name]
            
            # Hyperparameter optimization
            if optimize_hyperparameters:
                model = self._optimize_hyperparameters(model_name, X_train_scaled, y_train)
            
            # Train model
            self.logger.info(f"Training {model_name} model...")
            model.fit(X_train_scaled, y_train)
            
            # Make predictions
            y_pred = model.predict(X_test_scaled)
            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None
            
            # Calculate metrics
            accuracy = accuracy_score(y_test, y_pred)
            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=self.cv_folds)
            
            # Store trained model
            self.trained_models[model_name] = model
            
            # Prepare results
            results = {
                'model_name': model_name,
                'accuracy': accuracy,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'classification_report': classification_report(y_test, y_pred, output_dict=True),
                'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),
                'feature_importance': self._get_feature_importance(model, features.columns),
                'training_samples': len(X_train),
                'test_samples': len(X_test),
                'timestamp': datetime.now().isoformat()
            }
            
            # Store metrics
            self.model_metrics[model_name] = results
            
            self.logger.info(f"Model {model_name} trained successfully. Accuracy: {accuracy:.4f}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"Error training model {model_name}: {str(e)}")
            raise
    
    def _optimize_hyperparameters(self, 
                                 model_name: str, 
                                 X_train: np.ndarray, 
                                 y_train: np.ndarray) -> Any:
        """
        Optimize hyperparameters using GridSearchCV
        
        Args:
            model_name: Name of the model
            X_train: Training features
            y_train: Training labels
            
        Returns:
            Optimized model
        """
        try:
            param_grids = {
                'random_forest': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [5, 10, 15],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                },
                'gradient_boosting': {
                    'n_estimators': [50, 100, 200],
                    'learning_rate': [0.01, 0.1, 0.2],
                    'max_depth': [3, 6, 9],
                    'min_samples_split': [2, 5, 10]
                },
                'logistic_regression': {
                    'C': [0.1, 1.0, 10.0],
                    'penalty': ['l1', 'l2'],
                    'solver': ['liblinear', 'lbfgs']
                },
                'svm': {
                    'C': [0.1, 1.0, 10.0],
                    'kernel': ['linear', 'rbf'],
                    'gamma': ['scale', 'auto']
                },
                'neural_network': {
                    'hidden_layer_sizes': [(50,), (100,), (100, 50)],
                    'learning_rate_init': [0.001, 0.01, 0.1],
                    'alpha': [0.0001, 0.001, 0.01]
                }
            }
            
            if model_name not in param_grids:
                return self.models[model_name]
            
            self.logger.info(f"Optimizing hyperparameters for {model_name}...")
            
            grid_search = GridSearchCV(
                self.models[model_name],
                param_grids[model_name],
                cv=3,
                scoring='accuracy',
                n_jobs=-1,
                verbose=1
            )
            
            grid_search.fit(X_train, y_train)
            
            self.logger.info(f"Best parameters for {model_name}: {grid_search.best_params_}")
            
            return grid_search.best_estimator_
            
        except Exception as e:
            self.logger.error(f"Error optimizing hyperparameters for {model_name}: {str(e)}")
            return self.models[model_name]
    
    def _get_feature_importance(self, model: Any, feature_names: List[str]) -> Dict[str, float]:
        """
        Get feature importance from trained model
        
        Args:
            model: Trained model
            feature_names: List of feature names
            
        Returns:
            Dictionary of feature importance scores
        """
        try:
            importance_dict = {}
            
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
                for name, importance in zip(feature_names, importances):
                    importance_dict[name] = float(importance)
            elif hasattr(model, 'coef_'):
                importances = np.abs(model.coef_[0])
                for name, importance in zip(feature_names, importances):
                    importance_dict[name] = float(importance)
            
            # Sort by importance
            importance_dict = dict(sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))
            
            return importance_dict
            
        except Exception as e:
            self.logger.error(f"Error getting feature importance: {str(e)}")
            return {}
    
    def train_all_models(self, 
                        features: pd.DataFrame,
                        labels: pd.Series,
                        optimize_hyperparameters: bool = False) -> Dict[str, Dict[str, Any]]:
        """
        Train all available models
        
        Args:
            features: Feature matrix
            labels: Target labels
            optimize_hyperparameters: Whether to optimize hyperparameters
            
        Returns:
            Dictionary of all training results
        """
        try:
            all_results = {}
            
            for model_name in self.models.keys():
                self.logger.info(f"Training {model_name}...")
                results = self.train_model(
                    model_name, 
                    features, 
                    labels, 
                    optimize_hyperparameters
                )
                all_results[model_name] = results
            
            # Find best model
            best_model = max(all_results.keys(), key=lambda x: all_results[x]['accuracy'])
            self.logger.info(f"Best performing model: {best_model} with accuracy: {all_results[best_model]['accuracy']:.4f}")
            
            return all_results
            
        except Exception as e:
            self.logger.error(f"Error training all models: {str(e)}")
            raise
    
    def save_model(self, model_name: str, symbol: str, timeframe: str) -> str:
        """
        Save trained model and scaler to disk
        
        Args:
            model_name: Name of the model to save
            symbol: Trading symbol
            timeframe: Timeframe
            
        Returns:
            Path to saved model
        """
        try:
            if model_name not in self.trained_models:
                raise ValueError(f"Model {model_name} not trained yet")
            
            # Create filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_filename = f"{model_name}_{symbol}_{timeframe}_{timestamp}.pkl"
            scaler_filename = f"{model_name}_{symbol}_{timeframe}_{timestamp}_scaler.pkl"
            
            # Save model
            model_path = os.path.join(self.model_dir, model_filename)
            joblib.dump(self.trained_models[model_name], model_path)
            
            # Save scaler
            scaler_path = os.path.join(self.scaler_dir, scaler_filename)
            joblib.dump(self.scalers[model_name], scaler_path)
            
            # Save metadata
            metadata = {
                'model_name': model_name,
                'symbol': symbol,
                'timeframe': timeframe,
                'model_path': model_path,
                'scaler_path': scaler_path,
                'metrics': self.model_metrics.get(model_name, {}),
                'timestamp': datetime.now().isoformat()
            }
            
            metadata_filename = f"{model_name}_{symbol}_{timeframe}_{timestamp}_metadata.json"
            metadata_path = os.path.join(self.model_dir, metadata_filename)
            
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            self.logger.info(f"Model saved to: {model_path}")
            
            return model_path
            
        except Exception as e:
            self.logger.error(f"Error saving model {model_name}: {str(e)}")
            raise
    
    def load_model(self, model_path: str, scaler_path: str) -> Tuple[Any, Any]:
        """
        Load saved model and scaler
        
        Args:
            model_path: Path to saved model
            scaler_path: Path to saved scaler
            
        Returns:
            Tuple of (model, scaler)
        """
        try:
            model = joblib.load(model_path)
            scaler = joblib.load(scaler_path)
            
            self.logger.info(f"Model loaded from: {model_path}")
            
            return model, scaler
            
        except Exception as e:
            self.logger.error(f"Error loading model: {str(e)}")
            raise
    
    def evaluate_model(self, 
                      model_name: str,
                      test_features: pd.DataFrame,
                      test_labels: pd.Series) -> Dict[str, Any]:
        """
        Evaluate trained model on test data
        
        Args:
            model_name: Name of the model to evaluate
            test_features: Test features
            test_labels: Test labels
            
        Returns:
            Dictionary of evaluation metrics
        """
        try:
            if model_name not in self.trained_models:
                raise ValueError(f"Model {model_name} not trained yet")
            
            model = self.trained_models[model_name]
            scaler = self.scalers[model_name]
            
            # Scale test features
            test_features_scaled = scaler.transform(test_features)
            
            # Make predictions
            predictions = model.predict(test_features_scaled)
            prediction_probabilities = model.predict_proba(test_features_scaled)[:, 1] if hasattr(model, 'predict_proba') else None
            
            # Calculate metrics
            accuracy = accuracy_score(test_labels, predictions)
            
            evaluation_results = {
                'model_name': model_name,
                'accuracy': accuracy,
                'classification_report': classification_report(test_labels, predictions, output_dict=True),
                'confusion_matrix': confusion_matrix(test_labels, predictions).tolist(),
                'test_samples': len(test_features),
                'timestamp': datetime.now().isoformat()
            }
            
            self.logger.info(f"Model {model_name} evaluation completed. Accuracy: {accuracy:.4f}")
            
            return evaluation_results
            
        except Exception as e:
            self.logger.error(f"Error evaluating model {model_name}: {str(e)}")
            raise
    
    def get_model_comparison(self) -> pd.DataFrame:
        """
        Get comparison of all trained models
        
        Returns:
            DataFrame with model comparison metrics
        """
        try:
            if not self.model_metrics:
                return pd.DataFrame()
            
            comparison_data = []
            for model_name, metrics in self.model_metrics.items():
                comparison_data.append({
                    'model_name': model_name,
                    'accuracy': metrics['accuracy'],
                    'cv_mean': metrics['cv_mean'],
                    'cv_std': metrics['cv_std'],
                    'training_samples': metrics['training_samples'],
                    'test_samples': metrics['test_samples']
                })
            
            comparison_df = pd.DataFrame(comparison_data)
            comparison_df = comparison_df.sort_values('accuracy', ascending=False)
            
            return comparison_df
            
        except Exception as e:
            self.logger.error(f"Error creating model comparison: {str(e)}")
            return pd.DataFrame()
    
    def get_best_model(self) -> Optional[str]:
        """
        Get the name of the best performing model
        
        Returns:
            Name of the best model or None if no models trained
        """
        try:
            if not self.model_metrics:
                return None
            
            best_model = max(self.model_metrics.keys(), 
                           key=lambda x: self.model_metrics[x]['accuracy'])
            
            return best_model
            
        except Exception as e:
            self.logger.error(f"Error getting best model: {str(e)}")
            return None

================================================================================
FILE: src\ml\model_trainer.py
DIRECTORY: src\ml
SIZE: 19349 bytes
================================================================================

"""
Model Trainer for Quotex Trading Bot
Handles training of various ML models for price prediction and signal generation
"""

import os
import pickle
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
import joblib
import warnings
warnings.filterwarnings('ignore')

from ..utils.logger import setup_logger
from config import settings
from .feature_engineering import FeatureEngineer


class ModelTrainer:
    """
    Comprehensive model training class for trading predictions
    """
    
    def __init__(self, model_dir: str = "data/models"):
        """
        Initialize the model trainer
        
        Args:
            model_dir: Directory to save trained models
        """
        self.model_dir = model_dir
        self.logger = setup_logger(__name__)
        self.feature_engineer = FeatureEngineer()
        
        # Create model directory if it doesn't exist
        os.makedirs(model_dir, exist_ok=True)
        
        # Initialize models dictionary
        self.models = {
            'random_forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=100,
                max_depth=6,
                random_state=42
            ),
            'logistic_regression': LogisticRegression(
                random_state=42,
                max_iter=1000
            ),
            'svm': SVC(
                kernel='rbf',
                random_state=42,
                probability=True
            ),
            'neural_network': MLPClassifier(
                hidden_layer_sizes=(100, 50),
                max_iter=1000,
                random_state=42
            )
        }
        
        # Model performance tracking
        self.model_performance = {}
        self.best_model = None
        self.scaler = StandardScaler()
        
    def prepare_data(self, data: pd.DataFrame, target_column: str = 'signal') -> Tuple[np.ndarray, np.ndarray]:
        """
        Prepare data for training
        
        Args:
            data: Input DataFrame with features and target
            target_column: Name of the target column
            
        Returns:
            Tuple of (features, target)
        """
        try:
            # Generate features using feature engineering
            features_df = self.feature_engineer.generate_features(data)
            
            # Handle missing values
            features_df = features_df.fillna(method='ffill').fillna(method='bfill')
            
            # Prepare target variable
            if target_column not in features_df.columns:
                # Generate trading signals based on price movement
                features_df['signal'] = self._generate_signals(features_df)
                target_column = 'signal'
            
            # Separate features and target
            X = features_df.drop(columns=[target_column])
            y = features_df[target_column]
            
            # Handle categorical variables
            for col in X.select_dtypes(include=['object']).columns:
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))
            
            # Scale features
            X_scaled = self.scaler.fit_transform(X)
            
            self.logger.info(f"Data prepared: {X_scaled.shape[0]} samples, {X_scaled.shape[1]} features")
            return X_scaled, y.values
            
        except Exception as e:
            self.logger.error(f"Error preparing data: {str(e)}")
            raise
    
    def _generate_signals(self, data: pd.DataFrame) -> np.ndarray:
        """
        Generate trading signals based on price movement
        
        Args:
            data: DataFrame with OHLCV data
            
        Returns:
            Array of trading signals (0: sell, 1: buy, 2: hold)
        """
        try:
            # Calculate future returns
            future_return = data['close'].pct_change().shift(-1)
            
            # Define signal thresholds
            buy_threshold = 0.001  # 0.1% positive return
            sell_threshold = -0.001  # -0.1% negative return
            
            # Generate signals
            signals = np.where(future_return > buy_threshold, 1,
                             np.where(future_return < sell_threshold, 0, 2))
            
            return signals[:-1]  # Remove last element (no future data)
            
        except Exception as e:
            self.logger.error(f"Error generating signals: {str(e)}")
            return np.zeros(len(data) - 1)
    
    def train_model(self, model_name: str, X_train: np.ndarray, y_train: np.ndarray) -> Dict[str, Any]:
        """
        Train a specific model
        
        Args:
            model_name: Name of the model to train
            X_train: Training features
            y_train: Training targets
            
        Returns:
            Dictionary with model performance metrics
        """
        try:
            if model_name not in self.models:
                raise ValueError(f"Model {model_name} not found")
            
            model = self.models[model_name]
            
            # Train the model
            self.logger.info(f"Training {model_name}...")
            model.fit(X_train, y_train)
            
            # Cross-validation
            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
            
            # Calculate metrics
            y_pred = model.predict(X_train)
            metrics = {
                'accuracy': accuracy_score(y_train, y_pred),
                'precision': precision_score(y_train, y_pred, average='weighted'),
                'recall': recall_score(y_train, y_pred, average='weighted'),
                'f1_score': f1_score(y_train, y_pred, average='weighted'),
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            
            self.logger.info(f"{model_name} training completed - Accuracy: {metrics['accuracy']:.4f}")
            return metrics
            
        except Exception as e:
            self.logger.error(f"Error training {model_name}: {str(e)}")
            return {}
    
    def hyperparameter_tuning(self, model_name: str, X_train: np.ndarray, y_train: np.ndarray) -> Dict[str, Any]:
        """
        Perform hyperparameter tuning for a model
        
        Args:
            model_name: Name of the model
            X_train: Training features
            y_train: Training targets
            
        Returns:
            Best parameters and performance
        """
        try:
            param_grids = {
                'random_forest': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [5, 10, 15],
                    'min_samples_split': [2, 5, 10]
                },
                'gradient_boosting': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [3, 6, 9],
                    'learning_rate': [0.01, 0.1, 0.2]
                },
                'logistic_regression': {
                    'C': [0.1, 1, 10],
                    'penalty': ['l1', 'l2'],
                    'solver': ['liblinear', 'saga']
                },
                'svm': {
                    'C': [0.1, 1, 10],
                    'kernel': ['rbf', 'linear'],
                    'gamma': ['scale', 'auto']
                },
                'neural_network': {
                    'hidden_layer_sizes': [(50,), (100,), (100, 50)],
                    'alpha': [0.0001, 0.001, 0.01],
                    'learning_rate': ['constant', 'adaptive']
                }
            }
            
            if model_name not in param_grids:
                self.logger.warning(f"No parameter grid defined for {model_name}")
                return {}
            
            # Perform grid search
            grid_search = GridSearchCV(
                self.models[model_name],
                param_grids[model_name],
                cv=3,
                scoring='accuracy',
                n_jobs=-1,
                verbose=1
            )
            
            self.logger.info(f"Starting hyperparameter tuning for {model_name}...")
            grid_search.fit(X_train, y_train)
            
            # Update model with best parameters
            self.models[model_name] = grid_search.best_estimator_
            
            results = {
                'best_params': grid_search.best_params_,
                'best_score': grid_search.best_score_,
                'cv_results': grid_search.cv_results_
            }
            
            self.logger.info(f"Best parameters for {model_name}: {grid_search.best_params_}")
            return results
            
        except Exception as e:
            self.logger.error(f"Error in hyperparameter tuning for {model_name}: {str(e)}")
            return {}
    
    def train_all_models(self, data: pd.DataFrame, target_column: str = 'signal', 
                        test_size: float = 0.2, tune_hyperparameters: bool = False) -> Dict[str, Any]:
        """
        Train all available models
        
        Args:
            data: Training data
            target_column: Target column name
            test_size: Test set size ratio
            tune_hyperparameters: Whether to perform hyperparameter tuning
            
        Returns:
            Dictionary with all model performances
        """
        try:
            # Prepare data
            X, y = self.prepare_data(data, target_column)
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=42, stratify=y
            )
            
            results = {}
            
            for model_name in self.models.keys():
                self.logger.info(f"Processing {model_name}...")
                
                # Hyperparameter tuning
                if tune_hyperparameters:
                    tuning_results = self.hyperparameter_tuning(model_name, X_train, y_train)
                    results[f"{model_name}_tuning"] = tuning_results
                
                # Train model
                train_metrics = self.train_model(model_name, X_train, y_train)
                
                # Test model
                test_metrics = self.evaluate_model(model_name, X_test, y_test)
                
                # Combine results
                results[model_name] = {
                    'train_metrics': train_metrics,
                    'test_metrics': test_metrics,
                    'model_object': self.models[model_name]
                }
            
            # Find best model
            self.best_model = self._find_best_model(results)
            self.model_performance = results
            
            self.logger.info(f"Best model: {self.best_model}")
            return results
            
        except Exception as e:
            self.logger.error(f"Error training all models: {str(e)}")
            return {}
    
    def evaluate_model(self, model_name: str, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:
        """
        Evaluate a trained model
        
        Args:
            model_name: Name of the model
            X_test: Test features
            y_test: Test targets
            
        Returns:
            Dictionary with evaluation metrics
        """
        try:
            model = self.models[model_name]
            y_pred = model.predict(X_test)
            
            metrics = {
                'accuracy': accuracy_score(y_test, y_pred),
                'precision': precision_score(y_test, y_pred, average='weighted'),
                'recall': recall_score(y_test, y_pred, average='weighted'),
                'f1_score': f1_score(y_test, y_pred, average='weighted')
            }
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"Error evaluating {model_name}: {str(e)}")
            return {}
    
    def _find_best_model(self, results: Dict[str, Any]) -> str:
        """
        Find the best performing model
        
        Args:
            results: Dictionary with model results
            
        Returns:
            Name of the best model
        """
        best_model = None
        best_score = -1
        
        for model_name, model_results in results.items():
            if 'test_metrics' in model_results:
                score = model_results['test_metrics'].get('f1_score', 0)
                if score > best_score:
                    best_score = score
                    best_model = model_name
        
        return best_model
    
    def save_model(self, model_name: str, filename: Optional[str] = None) -> bool:
        """
        Save a trained model
        
        Args:
            model_name: Name of the model to save
            filename: Optional filename (default: model_name.pkl)
            
        Returns:
            True if successful, False otherwise
        """
        try:
            if model_name not in self.models:
                raise ValueError(f"Model {model_name} not found")
            
            if filename is None:
                filename = f"{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            
            filepath = os.path.join(self.model_dir, filename)
            
            # Save model and scaler
            model_data = {
                'model': self.models[model_name],
                'scaler': self.scaler,
                'feature_engineer': self.feature_engineer,
                'performance': self.model_performance.get(model_name, {}),
                'timestamp': datetime.now().isoformat()
            }
            
            joblib.dump(model_data, filepath)
            self.logger.info(f"Model {model_name} saved to {filepath}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error saving model {model_name}: {str(e)}")
            return False
    
    def load_model(self, filepath: str) -> bool:
        """
        Load a trained model
        
        Args:
            filepath: Path to the model file
            
        Returns:
            True if successful, False otherwise
        """
        try:
            model_data = joblib.load(filepath)
            
            # Extract components
            model_name = os.path.basename(filepath).split('_')[0]
            self.models[model_name] = model_data['model']
            self.scaler = model_data['scaler']
            self.feature_engineer = model_data['feature_engineer']
            
            self.logger.info(f"Model loaded from {filepath}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error loading model from {filepath}: {str(e)}")
            return False
    
    def predict(self, data: pd.DataFrame, model_name: Optional[str] = None) -> np.ndarray:
        """
        Make predictions using a trained model
        
        Args:
            data: Input data for prediction
            model_name: Name of the model to use (default: best model)
            
        Returns:
            Array of predictions
        """
        try:
            if model_name is None:
                model_name = self.best_model
            
            if model_name not in self.models:
                raise ValueError(f"Model {model_name} not found")
            
            # Prepare features
            features_df = self.feature_engineer.generate_features(data)
            features_df = features_df.fillna(method='ffill').fillna(method='bfill')
            
            # Handle categorical variables
            for col in features_df.select_dtypes(include=['object']).columns:
                le = LabelEncoder()
                features_df[col] = le.fit_transform(features_df[col].astype(str))
            
            # Scale features
            X_scaled = self.scaler.transform(features_df)
            
            # Make predictions
            predictions = self.models[model_name].predict(X_scaled)
            
            return predictions
            
        except Exception as e:
            self.logger.error(f"Error making predictions: {str(e)}")
            return np.array([])
    
    def get_feature_importance(self, model_name: str) -> pd.DataFrame:
        """
        Get feature importance from a trained model
        
        Args:
            model_name: Name of the model
            
        Returns:
            DataFrame with feature importance
        """
        try:
            model = self.models[model_name]
            
            if hasattr(model, 'feature_importances_'):
                # Tree-based models
                importances = model.feature_importances_
            elif hasattr(model, 'coef_'):
                # Linear models
                importances = np.abs(model.coef_[0])
            else:
                self.logger.warning(f"Model {model_name} doesn't support feature importance")
                return pd.DataFrame()
            
            # Create DataFrame
            feature_names = [f"feature_{i}" for i in range(len(importances))]
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importances
            }).sort_values('importance', ascending=False)
            
            return importance_df
            
        except Exception as e:
            self.logger.error(f"Error getting feature importance: {str(e)}")
            return pd.DataFrame()
    
    def get_model_summary(self) -> Dict[str, Any]:
        """
        Get summary of all trained models
        
        Returns:
            Dictionary with model summary
        """
        summary = {
            'total_models': len(self.models),
            'best_model': self.best_model,
            'model_performance': self.model_performance,
            'training_timestamp': datetime.now().isoformat()
        }
        
        return summary

================================================================================
FILE: src\ml\prediction_engine.py
DIRECTORY: src\ml
SIZE: 24712 bytes
================================================================================

"""
Prediction Engine for Quotex Trading Bot
Advanced ML-based price prediction system with ensemble methods
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib
import logging
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Local imports
from ..utils.logger import setup_logger
from ..utils.helpers import validate_data, calculate_metrics
from .feature_engineering import FeatureEngineer


class PredictionEngine:
    """
    Advanced prediction engine using ensemble ML models for price forecasting
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Initialize the prediction engine
        
        Args:
            config: Configuration dictionary containing model parameters
        """
        self.config = config
        self.logger = setup_logger(__name__)
        self.feature_engineer = FeatureEngineer()
        
        # Model parameters
        self.lookback_window = config.get('lookback_window', 60)
        self.prediction_horizon = config.get('prediction_horizon', 5)
        self.model_type = config.get('model_type', 'ensemble')
        
        # Initialize models
        self.models = {}
        self.scalers = {}
        self.model_weights = {}
        self.is_trained = False
        
        # Performance tracking
        self.performance_metrics = {}
        self.prediction_history = []
        
        self._initialize_models()
    
    def _initialize_models(self):
        """Initialize ML models with optimized parameters"""
        try:
            # Random Forest
            self.models['rf'] = RandomForestRegressor(
                n_estimators=self.config.get('rf_n_estimators', 100),
                max_depth=self.config.get('rf_max_depth', 10),
                min_samples_split=self.config.get('rf_min_samples_split', 5),
                min_samples_leaf=self.config.get('rf_min_samples_leaf', 2),
                random_state=42,
                n_jobs=-1
            )
            
            # Gradient Boosting
            self.models['gb'] = GradientBoostingRegressor(
                n_estimators=self.config.get('gb_n_estimators', 100),
                learning_rate=self.config.get('gb_learning_rate', 0.1),
                max_depth=self.config.get('gb_max_depth', 6),
                random_state=42
            )
            
            # Support Vector Regression
            self.models['svr'] = SVR(
                kernel=self.config.get('svr_kernel', 'rbf'),
                C=self.config.get('svr_C', 1.0),
                gamma=self.config.get('svr_gamma', 'scale')
            )
            
            # Linear models
            self.models['linear'] = LinearRegression()
            self.models['ridge'] = Ridge(
                alpha=self.config.get('ridge_alpha', 1.0),
                random_state=42
            )
            
            # Initialize scalers
            for model_name in self.models.keys():
                self.scalers[model_name] = StandardScaler()
            
            # Set initial weights (will be updated during training)
            num_models = len(self.models)
            for model_name in self.models.keys():
                self.model_weights[model_name] = 1.0 / num_models
            
            self.logger.info(f"Initialized {len(self.models)} models")
            
        except Exception as e:
            self.logger.error(f"Error initializing models: {str(e)}")
            raise
    
    def prepare_training_data(self, market_data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        Prepare training data with features and targets
        
        Args:
            market_data: Historical market data
            
        Returns:
            Tuple of (features, targets)
        """
        try:
            # Validate input data
            if not validate_data(market_data):
                raise ValueError("Invalid market data provided")
            
            # Generate features
            features_df = self.feature_engineer.generate_features(market_data)
            
            # Create target variable (future price movement)
            targets = self._create_targets(market_data)
            
            # Align features and targets
            min_length = min(len(features_df), len(targets))
            features = features_df.iloc[:min_length].values
            targets = targets[:min_length]
            
            # Remove NaN values
            valid_indices = ~(np.isnan(features).any(axis=1) | np.isnan(targets))
            features = features[valid_indices]
            targets = targets[valid_indices]
            
            self.logger.info(f"Prepared training data: {features.shape[0]} samples, {features.shape[1]} features")
            
            return features, targets
            
        except Exception as e:
            self.logger.error(f"Error preparing training data: {str(e)}")
            raise
    
    def _create_targets(self, market_data: pd.DataFrame) -> np.ndarray:
        """
        Create target variables for prediction
        
        Args:
            market_data: Historical market data
            
        Returns:
            Target array
        """
        try:
            prices = market_data['close'].values
            targets = np.zeros(len(prices))
            
            # Calculate future price movement
            for i in range(len(prices) - self.prediction_horizon):
                future_price = prices[i + self.prediction_horizon]
                current_price = prices[i]
                
                # Calculate percentage change
                targets[i] = (future_price - current_price) / current_price * 100
            
            return targets
            
        except Exception as e:
            self.logger.error(f"Error creating targets: {str(e)}")
            raise
    
    def train_models(self, market_data: pd.DataFrame) -> Dict[str, float]:
        """
        Train all models and calculate performance metrics
        
        Args:
            market_data: Historical market data for training
            
        Returns:
            Dictionary of model performance scores
        """
        try:
            # Prepare training data
            features, targets = self.prepare_training_data(market_data)
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                features, targets, test_size=0.2, random_state=42
            )
            
            model_scores = {}
            
            # Train each model
            for model_name, model in self.models.items():
                self.logger.info(f"Training {model_name} model...")
                
                # Scale features
                X_train_scaled = self.scalers[model_name].fit_transform(X_train)
                X_test_scaled = self.scalers[model_name].transform(X_test)
                
                # Train model
                model.fit(X_train_scaled, y_train)
                
                # Make predictions
                y_pred = model.predict(X_test_scaled)
                
                # Calculate metrics
                mse = mean_squared_error(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                
                # Cross-validation score
                cv_scores = cross_val_score(
                    model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error'
                )
                cv_score = -cv_scores.mean()
                
                model_scores[model_name] = {
                    'mse': mse,
                    'mae': mae,
                    'r2': r2,
                    'cv_score': cv_score
                }
                
                self.logger.info(f"{model_name} - MSE: {mse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}")
            
            # Update model weights based on performance
            self._update_model_weights(model_scores)
            
            # Store performance metrics
            self.performance_metrics = model_scores
            self.is_trained = True
            
            self.logger.info("All models trained successfully")
            
            return model_scores
            
        except Exception as e:
            self.logger.error(f"Error training models: {str(e)}")
            raise
    
    def _update_model_weights(self, model_scores: Dict[str, Dict[str, float]]):
        """
        Update model weights based on performance scores
        
        Args:
            model_scores: Dictionary of model performance metrics
        """
        try:
            # Use inverse of MSE for weights (lower MSE = higher weight)
            total_inverse_mse = 0
            inverse_mse_scores = {}
            
            for model_name, scores in model_scores.items():
                inverse_mse = 1.0 / (scores['mse'] + 1e-8)  # Add small epsilon to avoid division by zero
                inverse_mse_scores[model_name] = inverse_mse
                total_inverse_mse += inverse_mse
            
            # Normalize weights
            for model_name in inverse_mse_scores:
                self.model_weights[model_name] = inverse_mse_scores[model_name] / total_inverse_mse
            
            self.logger.info(f"Updated model weights: {self.model_weights}")
            
        except Exception as e:
            self.logger.error(f"Error updating model weights: {str(e)}")
            raise
    
    def predict(self, market_data: pd.DataFrame) -> Dict[str, Any]:
        """
        Make price predictions using ensemble of models
        
        Args:
            market_data: Recent market data for prediction
            
        Returns:
            Dictionary containing predictions and confidence metrics
        """
        try:
            if not self.is_trained:
                raise ValueError("Models must be trained before making predictions")
            
            # Generate features for latest data
            features_df = self.feature_engineer.generate_features(market_data)
            
            # Get the latest feature vector
            latest_features = features_df.iloc[-1].values.reshape(1, -1)
            
            # Check for NaN values
            if np.isnan(latest_features).any():
                self.logger.warning("NaN values detected in features, using last valid values")
                latest_features = np.nan_to_num(latest_features)
            
            predictions = {}
            scaled_predictions = []
            
            # Get predictions from each model
            for model_name, model in self.models.items():
                # Scale features
                scaled_features = self.scalers[model_name].transform(latest_features)
                
                # Make prediction
                pred = model.predict(scaled_features)[0]
                predictions[model_name] = pred
                
                # Weight the prediction
                weighted_pred = pred * self.model_weights[model_name]
                scaled_predictions.append(weighted_pred)
            
            # Ensemble prediction
            ensemble_prediction = sum(scaled_predictions)
            
            # Calculate prediction confidence
            pred_values = list(predictions.values())
            prediction_std = np.std(pred_values)
            confidence = max(0, 1 - (prediction_std / (abs(ensemble_prediction) + 1e-8)))
            
            # Determine signal direction
            signal = self._generate_signal(ensemble_prediction, confidence)
            
            # Create prediction result
            result = {
                'ensemble_prediction': ensemble_prediction,
                'individual_predictions': predictions,
                'confidence': confidence,
                'signal': signal,
                'timestamp': datetime.now(),
                'model_weights': self.model_weights.copy()
            }
            
            # Store prediction history
            self.prediction_history.append(result)
            
            # Keep only last 1000 predictions
            if len(self.prediction_history) > 1000:
                self.prediction_history = self.prediction_history[-1000:]
            
            self.logger.info(f"Prediction: {ensemble_prediction:.4f}, Confidence: {confidence:.4f}, Signal: {signal}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error making prediction: {str(e)}")
            raise
    
    def _generate_signal(self, prediction: float, confidence: float) -> str:
        """
        Generate trading signal based on prediction and confidence
        
        Args:
            prediction: Ensemble prediction value
            confidence: Prediction confidence score
            
        Returns:
            Trading signal ('BUY', 'SELL', 'HOLD')
        """
        try:
            min_confidence = self.config.get('min_confidence', 0.6)
            min_prediction_threshold = self.config.get('min_prediction_threshold', 0.1)
            
            # Check if confidence is sufficient
            if confidence < min_confidence:
                return 'HOLD'
            
            # Generate signal based on prediction magnitude
            if prediction > min_prediction_threshold:
                return 'BUY'
            elif prediction < -min_prediction_threshold:
                return 'SELL'
            else:
                return 'HOLD'
                
        except Exception as e:
            self.logger.error(f"Error generating signal: {str(e)}")
            return 'HOLD'
    
    def get_model_performance(self) -> Dict[str, Any]:
        """
        Get comprehensive model performance metrics
        
        Returns:
            Dictionary of performance metrics
        """
        try:
            if not self.performance_metrics:
                return {"error": "No performance metrics available. Train models first."}
            
            # Calculate additional metrics
            performance_summary = {
                'individual_models': self.performance_metrics,
                'model_weights': self.model_weights,
                'is_trained': self.is_trained,
                'prediction_history_length': len(self.prediction_history)
            }
            
            # Calculate recent prediction accuracy if available
            if len(self.prediction_history) > 10:
                recent_predictions = self.prediction_history[-10:]
                recent_confidence = np.mean([p['confidence'] for p in recent_predictions])
                performance_summary['recent_avg_confidence'] = recent_confidence
            
            return performance_summary
            
        except Exception as e:
            self.logger.error(f"Error getting model performance: {str(e)}")
            return {"error": str(e)}
    
    def save_models(self, filepath: str):
        """
        Save trained models to disk
        
        Args:
            filepath: Path to save models
        """
        try:
            if not self.is_trained:
                raise ValueError("No trained models to save")
            
            model_data = {
                'models': self.models,
                'scalers': self.scalers,
                'model_weights': self.model_weights,
                'performance_metrics': self.performance_metrics,
                'config': self.config
            }
            
            joblib.dump(model_data, filepath)
            self.logger.info(f"Models saved to {filepath}")
            
        except Exception as e:
            self.logger.error(f"Error saving models: {str(e)}")
            raise
    
    def load_models(self, filepath: str):
        """
        Load trained models from disk
        
        Args:
            filepath: Path to load models from
        """
        try:
            model_data = joblib.load(filepath)
            
            self.models = model_data['models']
            self.scalers = model_data['scalers']
            self.model_weights = model_data['model_weights']
            self.performance_metrics = model_data['performance_metrics']
            self.config.update(model_data['config'])
            
            self.is_trained = True
            self.logger.info(f"Models loaded from {filepath}")
            
        except Exception as e:
            self.logger.error(f"Error loading models: {str(e)}")
            raise
    
    def retrain_models(self, market_data: pd.DataFrame):
        """
        Retrain models with new data
        
        Args:
            market_data: New market data for retraining
        """
        try:
            self.logger.info("Retraining models with new data...")
            
            # Reset models
            self._initialize_models()
            
            # Train with new data
            self.train_models(market_data)
            
            self.logger.info("Models retrained successfully")
            
        except Exception as e:
            self.logger.error(f"Error retraining models: {str(e)}")
            raise
    
    def optimize_parameters(self, market_data: pd.DataFrame, param_ranges: Dict[str, Any]):
        """
        Optimize model parameters using grid search
        
        Args:
            market_data: Training data
            param_ranges: Dictionary of parameter ranges to search
        """
        try:
            from sklearn.model_selection import GridSearchCV
            
            self.logger.info("Starting parameter optimization...")
            
            # Prepare data
            features, targets = self.prepare_training_data(market_data)
            
            best_params = {}
            
            # Optimize each model
            for model_name, model in self.models.items():
                if model_name in param_ranges:
                    self.logger.info(f"Optimizing {model_name} parameters...")
                    
                    # Scale features
                    scaled_features = self.scalers[model_name].fit_transform(features)
                    
                    # Grid search
                    grid_search = GridSearchCV(
                        model, param_ranges[model_name], 
                        cv=5, scoring='neg_mean_squared_error', n_jobs=-1
                    )
                    grid_search.fit(scaled_features, targets)
                    
                    # Update model with best parameters
                    best_params[model_name] = grid_search.best_params_
                    self.models[model_name] = grid_search.best_estimator_
                    
                    self.logger.info(f"{model_name} best params: {grid_search.best_params_}")
            
            # Update config with best parameters
            self.config['optimized_params'] = best_params
            
            self.logger.info("Parameter optimization completed")
            
            return best_params
            
        except Exception as e:
            self.logger.error(f"Error optimizing parameters: {str(e)}")
            raise
    
    def get_feature_importance(self) -> Dict[str, np.ndarray]:
        """
        Get feature importance from tree-based models
        
        Returns:
            Dictionary of feature importance arrays
        """
        try:
            if not self.is_trained:
                raise ValueError("Models must be trained first")
            
            importance_dict = {}
            
            # Get feature importance from tree-based models
            for model_name, model in self.models.items():
                if hasattr(model, 'feature_importances_'):
                    importance_dict[model_name] = model.feature_importances_
            
            return importance_dict
            
        except Exception as e:
            self.logger.error(f"Error getting feature importance: {str(e)}")
            return {}
    
    def validate_prediction_accuracy(self, market_data: pd.DataFrame, 
                                   validation_days: int = 30) -> Dict[str, float]:
        """
        Validate prediction accuracy on recent data
        
        Args:
            market_data: Recent market data for validation
            validation_days: Number of days to validate
            
        Returns:
            Dictionary of accuracy metrics
        """
        try:
            if not self.is_trained:
                raise ValueError("Models must be trained first")
            
            # Get recent data for validation
            recent_data = market_data.tail(validation_days * 24)  # Assuming hourly data
            
            actual_movements = []
            predicted_movements = []
            
            # Generate predictions for each time step
            for i in range(len(recent_data) - self.prediction_horizon):
                # Get data up to current point
                current_data = recent_data.iloc[:i+self.lookback_window]
                
                if len(current_data) < self.lookback_window:
                    continue
                
                # Make prediction
                try:
                    prediction_result = self.predict(current_data)
                    predicted_movement = prediction_result['ensemble_prediction']
                    
                    # Calculate actual movement
                    current_price = recent_data.iloc[i]['close']
                    future_price = recent_data.iloc[i + self.prediction_horizon]['close']
                    actual_movement = (future_price - current_price) / current_price * 100
                    
                    actual_movements.append(actual_movement)
                    predicted_movements.append(predicted_movement)
                    
                except Exception as e:
                    self.logger.warning(f"Error in validation step {i}: {str(e)}")
                    continue
            
            if len(actual_movements) < 10:
                return {"error": "Insufficient data for validation"}
            
            # Calculate accuracy metrics
            actual_movements = np.array(actual_movements)
            predicted_movements = np.array(predicted_movements)
            
            # Direction accuracy
            actual_directions = np.sign(actual_movements)
            predicted_directions = np.sign(predicted_movements)
            direction_accuracy = np.mean(actual_directions == predicted_directions)
            
            # Magnitude accuracy
            mse = mean_squared_error(actual_movements, predicted_movements)
            mae = mean_absolute_error(actual_movements, predicted_movements)
            
            # Correlation
            correlation = np.corrcoef(actual_movements, predicted_movements)[0, 1]
            
            validation_metrics = {
                'direction_accuracy': direction_accuracy,
                'mse': mse,
                'mae': mae,
                'correlation': correlation,
                'sample_size': len(actual_movements)
            }
            
            self.logger.info(f"Validation metrics: {validation_metrics}")
            
            return validation_metrics
            
        except Exception as e:
            self.logger.error(f"Error validating prediction accuracy: {str(e)}")
            return {"error": str(e)}

================================================================================
FILE: src\utils\__init__.py
DIRECTORY: src\utils
SIZE: 10748 bytes
================================================================================

"""
Utils package for the Quotex trading bot.
Provides logging, data processing, and general utility functions.
"""

# Version information
__version__ = "1.0.0"
__author__ = "Trading Bot Team"

# Import main classes and functions for easy access
try:
    from .logger import (
        TradingBotLogger,
        setup_logging,
        get_logger,
        get_trading_logger,
        log_trade,
        log_signal,
        ContextLogger,
        log_execution_time
    )
except ImportError:
    # Fallback for direct execution
    try:
        from logger import (
            TradingBotLogger,
            setup_logging,
            get_logger,
            get_trading_logger,
            log_trade,
            log_signal,
            ContextLogger,
            log_execution_time
        )
    except ImportError as e:
        print(f"Warning: Could not import logger module: {e}")
        # Create dummy functions to prevent crashes
        def setup_logging(*args, **kwargs): pass
        def get_logger(*args, **kwargs): 
            import logging
            return logging.getLogger("dummy")
        def get_trading_logger(*args, **kwargs): 
            import logging
            return logging.getLogger("dummy_trading")
        def log_trade(*args, **kwargs): pass
        def log_signal(*args, **kwargs): pass

try:
    from .data_processor import (
        DataProcessor,
        detect_market_regime,
        calculate_fractal_dimension,
        validate_data_quality
    )
except ImportError:
    # Fallback for direct execution
    try:
        from data_processor import (
            DataProcessor,
            detect_market_regime,
            calculate_fractal_dimension,
            validate_data_quality
        )
    except ImportError as e:
        print(f"Warning: Could not import data_processor module: {e}")
        # Create dummy class
        class DataProcessor:
            def __init__(self, *args, **kwargs): pass
        def detect_market_regime(*args, **kwargs): pass
        def calculate_fractal_dimension(*args, **kwargs): return 0.5
        def validate_data_quality(*args, **kwargs): return {}

try:
    from .helpers import (
        # Time and date utilities
        get_current_timestamp,
        timestamp_to_datetime,
        datetime_to_timestamp,
        is_market_open,
        get_next_market_open,
        sleep_until,
        
        # Financial calculations
        calculate_pip_value,
        calculate_position_size,
        calculate_profit_loss,
        calculate_margin_required,
        
        # Number and string utilities
        round_to_decimals,
        format_currency,
        format_percentage,
        safe_divide,
        clamp,
        normalize_symbol,
        
        # File and data utilities
        save_json,
        load_json,
        save_pickle,
        load_pickle,
        ensure_directory,
        get_file_hash,
        
        # Decorators
        retry,
        timing,
        rate_limit,
        cache_result,
        
        # Threading utilities
        ThreadSafeCounter,
        thread_pool_executor,
        run_parallel,
        
        # Validation utilities
        validate_email,
        validate_url,
        validate_symbol_format,
        validate_price,
        validate_config,
        
        # Utility classes
        PerformanceMonitor,
        ConfigManager,
        performance_monitor
    )
except ImportError:
    # Fallback for direct execution
    try:
        from helpers import (
            get_current_timestamp,
            timestamp_to_datetime,
            datetime_to_timestamp,
            is_market_open,
            get_next_market_open,
            sleep_until,
            calculate_pip_value,
            calculate_position_size,
            calculate_profit_loss,
            calculate_margin_required,
            round_to_decimals,
            format_currency,
            format_percentage,
            safe_divide,
            clamp,
            normalize_symbol,
            save_json,
            load_json,
            save_pickle,
            load_pickle,
            ensure_directory,
            get_file_hash,
            retry,
            timing,
            rate_limit,
            cache_result,
            ThreadSafeCounter,
            thread_pool_executor,
            run_parallel,
            validate_email,
            validate_url,
            validate_symbol_format,
            validate_price,
            validate_config,
            PerformanceMonitor,
            ConfigManager,
            performance_monitor
        )
    except ImportError as e:
        print(f"Warning: Could not import helpers module: {e}")
        # Create dummy functions
        def get_current_timestamp(): 
            from datetime import datetime, timezone
            return datetime.now(timezone.utc).isoformat()
        def is_market_open(*args, **kwargs): return True
        def calculate_position_size(*args, **kwargs): return 1.0
        def safe_divide(a, b, default=0): return a/b if b != 0 else default
        def retry(*args, **kwargs): 
            def decorator(func): return func
            return decorator
        def timing(func): return func
        class ConfigManager:
            def __init__(self, *args): pass
            def get(self, *args, **kwargs): return None
            def set(self, *args, **kwargs): pass

# Package-level exports - only include what was successfully imported
__all__ = []

# Add successfully imported items to __all__
for item_name in [
    # Logger exports
    "TradingBotLogger", "setup_logging", "get_logger", "get_trading_logger",
    "log_trade", "log_signal", "ContextLogger", "log_execution_time",
    
    # Data processor exports
    "DataProcessor", "detect_market_regime", "calculate_fractal_dimension", 
    "validate_data_quality",
    
    # Helper function exports
    "get_current_timestamp", "timestamp_to_datetime", "datetime_to_timestamp",
    "is_market_open", "get_next_market_open", "sleep_until",
    "calculate_pip_value", "calculate_position_size", "calculate_profit_loss",
    "calculate_margin_required", "round_to_decimals", "format_currency",
    "format_percentage", "safe_divide", "clamp", "normalize_symbol",
    "save_json", "load_json", "save_pickle", "load_pickle",
    "ensure_directory", "get_file_hash", "retry", "timing",
    "rate_limit", "cache_result", "ThreadSafeCounter", "thread_pool_executor",
    "run_parallel", "validate_email", "validate_url", "validate_symbol_format",
    "validate_price", "validate_config", "PerformanceMonitor", "ConfigManager",
    "performance_monitor"
]:
    if item_name in globals():
        __all__.append(item_name)

# Package initialization
def initialize_utils(log_level: str = "INFO", log_dir: str = "data/logs"):
    """
    Initialize the utils package with logging configuration.
    
    Args:
        log_level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_dir: Directory for log files
    
    Returns:
        TradingBotLogger instance or None if setup fails
    """
    try:
        return setup_logging(log_level, log_dir)
    except Exception as e:
        print(f"Failed to initialize logging: {e}")
        return None


def get_package_info() -> dict:
    """Get package information."""
    return {
        "name": "quotex_trading_bot.utils",
        "version": __version__,
        "author": __author__,
        "modules": ["logger", "data_processor", "helpers"],
        "exports_count": len(__all__)
    }


# Convenience functions for common operations
def quick_setup(log_level: str = "INFO", config_file: str = "config/settings.json") -> tuple:
    """
    Quick setup for common utils functionality.
    
    Args:
        log_level: Logging level
        config_file: Configuration file path
    
    Returns:
        Tuple of (logger, config_manager, data_processor)
    """
    try:
        # Setup logging
        logger_instance = setup_logging(log_level)
        
        # Setup configuration manager
        config_manager = ConfigManager(config_file)
        
        # Setup data processor
        data_processor = DataProcessor()
        
        # Get main logger
        logger = get_logger("main")
        logger.info("Utils package initialized successfully")
        
        return logger, config_manager, data_processor
    except Exception as e:
        print(f"Failed to setup utils: {e}")
        import logging
        return logging.getLogger("fallback"), None, None


# Common configurations
DEFAULT_LOG_CONFIG = {
    "level": "INFO",
    "format": "%(asctime)s | %(levelname)-8s | %(name)s | %(message)s",
    "date_format": "%Y-%m-%d %H:%M:%S",
    "file_rotation": {
        "max_bytes": 10 * 1024 * 1024,  # 10MB
        "backup_count": 5
    }
}

DEFAULT_DATA_CONFIG = {
    "scaler_type": "standard",
    "missing_data_method": "forward_fill",
    "outlier_threshold": 5.0,
    "validation_enabled": True
}

DEFAULT_TRADING_CONFIG = {
    "risk_percentage": 2.0,
    "max_positions": 5,
    "leverage": 100,
    "pip_value": 10.0,
    "minimum_lot_size": 0.01
}

# Export configurations
CONFIGS = {
    "logging": DEFAULT_LOG_CONFIG,
    "data": DEFAULT_DATA_CONFIG,
    "trading": DEFAULT_TRADING_CONFIG
}


def print_package_info():
    """Print package information."""
    print(f"\nQuotex Trading Bot Utils v{__version__}")
    print(f"Author: {__author__}")
    print(f"Successfully imported: {len(__all__)} items")
    print(f"Available modules: logger, data_processor, helpers")


# Simple test function
def test_imports():
    """Test if all imports are working."""
    print("Testing utils package imports...")
    
    # Test basic functions
    try:
        timestamp = get_current_timestamp()
        print(f"✓ Timestamp function works: {timestamp}")
    except Exception as e:
        print(f"✗ Timestamp function failed: {e}")
    
    try:
        market_status = is_market_open()
        print(f"✓ Market status function works: {market_status}")
    except Exception as e:
        print(f"✗ Market status function failed: {e}")
    
    try:
        logger = get_logger("test")
        print("✓ Logger function works")
    except Exception as e:
        print(f"✗ Logger function failed: {e}")
    
    print(f"Total exported items: {len(__all__)}")


if __name__ == "__main__":
    print_package_info()
    test_imports()

================================================================================
FILE: src\utils\data_processor.py
DIRECTORY: src\utils
SIZE: 20784 bytes
================================================================================

"""
Data processing utilities for the Quotex trading bot.
Handles data cleaning, transformation, normalization, and preprocessing.
"""

import numpy as np
import pandas as pd
from typing import Union, List, Dict, Optional, Tuple, Any
from datetime import datetime, timedelta
import warnings
from scipy import stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.impute import SimpleImputer, KNNImputer

from .logger import get_logger

logger = get_logger(__name__)


class DataProcessor:
    """Main data processing class for trading data."""
    
    def __init__(self, scaler_type: str = "standard"):
        """
        Initialize DataProcessor.
        
        Args:
            scaler_type: Type of scaler to use ('standard', 'minmax', 'robust')
        """
        self.scaler_type = scaler_type
        self.scaler = self._get_scaler(scaler_type)
        self.imputer = None
        self.is_fitted = False
        
    def _get_scaler(self, scaler_type: str):
        """Get the specified scaler."""
        scalers = {
            'standard': StandardScaler(),
            'minmax': MinMaxScaler(),
            'robust': RobustScaler()
        }
        return scalers.get(scaler_type, StandardScaler())
    
    def clean_ohlc_data(self, df: pd.DataFrame, validate: bool = True) -> pd.DataFrame:
        """
        Clean OHLC (Open, High, Low, Close) trading data.
        
        Args:
            df: DataFrame with OHLC data
            validate: Whether to validate OHLC relationships
        
        Returns:
            Cleaned DataFrame
        """
        logger.info("Cleaning OHLC data")
        df_clean = df.copy()
        
        # Ensure required columns exist
        required_cols = ['open', 'high', 'low', 'close']
        missing_cols = [col for col in required_cols if col not in df_clean.columns]
        if missing_cols:
            raise ValueError(f"Missing required columns: {missing_cols}")
        
        # Convert to numeric
        for col in required_cols + ['volume'] if 'volume' in df_clean.columns else required_cols:
            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
        
        # Remove rows with all NaN OHLC values
        df_clean = df_clean.dropna(subset=required_cols, how='all')
        
        # Validate OHLC relationships
        if validate:
            df_clean = self._validate_ohlc_relationships(df_clean)
        
        # Handle extreme outliers
        df_clean = self._handle_price_outliers(df_clean)
        
        # Ensure datetime index
        df_clean = self._ensure_datetime_index(df_clean)
        
        # Sort by timestamp
        df_clean = df_clean.sort_index()
        
        logger.info(f"Cleaned OHLC data: {len(df_clean)} rows remaining")
        return df_clean
    
    def _validate_ohlc_relationships(self, df: pd.DataFrame) -> pd.DataFrame:
        """Validate and fix OHLC price relationships."""
        logger.debug("Validating OHLC relationships")
        
        # Check if high >= max(open, close) and low <= min(open, close)
        invalid_high = df['high'] < df[['open', 'close']].max(axis=1)
        invalid_low = df['low'] > df[['open', 'close']].min(axis=1)
        
        if invalid_high.any():
            logger.warning(f"Found {invalid_high.sum()} rows with invalid high prices")
            df.loc[invalid_high, 'high'] = df.loc[invalid_high, ['open', 'close']].max(axis=1)
        
        if invalid_low.any():
            logger.warning(f"Found {invalid_low.sum()} rows with invalid low prices")
            df.loc[invalid_low, 'low'] = df.loc[invalid_low, ['open', 'close']].min(axis=1)
        
        return df
    
    def _handle_price_outliers(self, df: pd.DataFrame, z_threshold: float = 5.0) -> pd.DataFrame:
        """Handle extreme price outliers using Z-score method."""
        logger.debug("Handling price outliers")
        
        price_cols = ['open', 'high', 'low', 'close']
        
        for col in price_cols:
            if col in df.columns:
                z_scores = np.abs(stats.zscore(df[col].dropna()))
                outliers = z_scores > z_threshold
                
                if outliers.any():
                    logger.warning(f"Found {outliers.sum()} outliers in {col}")
                    # Replace outliers with median
                    median_val = df[col].median()
                    df.loc[df[col].index[outliers], col] = median_val
        
        return df
    
    def _ensure_datetime_index(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ensure DataFrame has datetime index."""
        if not isinstance(df.index, pd.DatetimeIndex):
            if 'timestamp' in df.columns:
                df.index = pd.to_datetime(df['timestamp'])
                df = df.drop('timestamp', axis=1)
            elif 'time' in df.columns:
                df.index = pd.to_datetime(df['time'])
                df = df.drop('time', axis=1)
            else:
                logger.warning("No timestamp column found, using sequential index")
        
        return df
    
    def resample_data(self, df: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """
        Resample OHLC data to different timeframe.
        
        Args:
            df: DataFrame with OHLC data
            timeframe: Target timeframe ('1min', '5min', '15min', '1H', '4H', '1D')
        
        Returns:
            Resampled DataFrame
        """
        logger.info(f"Resampling data to {timeframe}")
        
        if not isinstance(df.index, pd.DatetimeIndex):
            raise ValueError("DataFrame must have datetime index for resampling")
        
        agg_dict = {
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last'
        }
        
        # Add volume if present
        if 'volume' in df.columns:
            agg_dict['volume'] = 'sum'
        
        # Add other numeric columns with mean aggregation
        for col in df.columns:
            if col not in agg_dict and pd.api.types.is_numeric_dtype(df[col]):
                agg_dict[col] = 'mean'
        
        resampled = df.resample(timeframe).agg(agg_dict).dropna()
        
        logger.info(f"Resampled data: {len(resampled)} rows")
        return resampled
    
    def add_returns(self, df: pd.DataFrame, periods: List[int] = [1]) -> pd.DataFrame:
        """
        Add return columns to the DataFrame.
        
        Args:
            df: DataFrame with price data
            periods: List of periods for return calculation
        
        Returns:
            DataFrame with return columns
        """
        logger.debug(f"Adding returns for periods: {periods}")
        df_returns = df.copy()
        
        for period in periods:
            # Simple returns
            df_returns[f'return_{period}'] = df_returns['close'].pct_change(period)
            
            # Log returns
            df_returns[f'log_return_{period}'] = np.log(df_returns['close'] / df_returns['close'].shift(period))
        
        return df_returns
    
    def add_volatility(self, df: pd.DataFrame, window: int = 20) -> pd.DataFrame:
        """
        Add volatility measures to the DataFrame.
        
        Args:
            df: DataFrame with return data
            window: Rolling window for volatility calculation
        
        Returns:
            DataFrame with volatility columns
        """
        logger.debug(f"Adding volatility with window: {window}")
        df_vol = df.copy()
        
        # True Range
        df_vol['tr'] = np.maximum(
            df_vol['high'] - df_vol['low'],
            np.maximum(
                np.abs(df_vol['high'] - df_vol['close'].shift(1)),
                np.abs(df_vol['low'] - df_vol['close'].shift(1))
            )
        )
        
        # Average True Range
        df_vol['atr'] = df_vol['tr'].rolling(window=window).mean()
        
        # Price volatility (standard deviation of returns)
        if 'return_1' in df_vol.columns:
            df_vol['volatility'] = df_vol['return_1'].rolling(window=window).std()
        
        # Realized volatility (sum of squared returns)
        if 'return_1' in df_vol.columns:
            df_vol['realized_vol'] = np.sqrt(
                df_vol['return_1'].rolling(window=window).apply(lambda x: (x**2).sum())
            )
        
        return df_vol
    
    def handle_missing_data(self, df: pd.DataFrame, method: str = "forward_fill", 
                          max_gap: Optional[int] = None) -> pd.DataFrame:
        """
        Handle missing data in the DataFrame.
        
        Args:
            df: DataFrame with missing data
            method: Method to handle missing data ('forward_fill', 'backward_fill', 'interpolate', 'knn')
            max_gap: Maximum gap size to fill
        
        Returns:
            DataFrame with missing data handled
        """
        logger.info(f"Handling missing data using method: {method}")
        df_filled = df.copy()
        
        # Log missing data info
        missing_counts = df_filled.isnull().sum()
        if missing_counts.any():
            logger.info(f"Missing data counts: {missing_counts[missing_counts > 0].to_dict()}")
        
        if method == "forward_fill":
            if max_gap:
                df_filled = df_filled.fillna(method='ffill', limit=max_gap)
            else:
                df_filled = df_filled.fillna(method='ffill')
                
        elif method == "backward_fill":
            if max_gap:
                df_filled = df_filled.fillna(method='bfill', limit=max_gap)
            else:
                df_filled = df_filled.fillna(method='bfill')
                
        elif method == "interpolate":
            df_filled = df_filled.interpolate(method='time' if isinstance(df_filled.index, pd.DatetimeIndex) else 'linear')
            
        elif method == "knn":
            numeric_cols = df_filled.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                imputer = KNNImputer(n_neighbors=5)
                df_filled[numeric_cols] = imputer.fit_transform(df_filled[numeric_cols])
        
        # Final check and remove rows with remaining NaN values
        remaining_na = df_filled.isnull().sum().sum()
        if remaining_na > 0:
            logger.warning(f"Removing {remaining_na} remaining NaN values")
            df_filled = df_filled.dropna()
        
        return df_filled
    
    def normalize_features(self, df: pd.DataFrame, columns: Optional[List[str]] = None,
                          fit: bool = True) -> pd.DataFrame:
        """
        Normalize/scale feature columns.
        
        Args:
            df: DataFrame to normalize
            columns: Columns to normalize (if None, all numeric columns)
            fit: Whether to fit the scaler
        
        Returns:
            DataFrame with normalized features
        """
        logger.debug(f"Normalizing features using {self.scaler_type} scaler")
        df_norm = df.copy()
        
        if columns is None:
            columns = df_norm.select_dtypes(include=[np.number]).columns.tolist()
        
        if fit:
            df_norm[columns] = self.scaler.fit_transform(df_norm[columns])
            self.is_fitted = True
        else:
            if not self.is_fitted:
                raise ValueError("Scaler must be fitted before transform")
            df_norm[columns] = self.scaler.transform(df_norm[columns])
        
        return df_norm
    
    def inverse_normalize(self, df: pd.DataFrame, columns: Optional[List[str]] = None) -> pd.DataFrame:
        """
        Inverse normalize features back to original scale.
        
        Args:
            df: Normalized DataFrame
            columns: Columns to inverse normalize
        
        Returns:
            DataFrame with original scale
        """
        if not self.is_fitted:
            raise ValueError("Scaler must be fitted before inverse transform")
        
        df_inv = df.copy()
        
        if columns is None:
            columns = df_inv.select_dtypes(include=[np.number]).columns.tolist()
        
        df_inv[columns] = self.scaler.inverse_transform(df_inv[columns])
        
        return df_inv
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Create additional features from price data.
        
        Args:
            df: DataFrame with OHLC data
        
        Returns:
            DataFrame with additional features
        """
        logger.info("Creating additional features")
        df_features = df.copy()
        
        # Price-based features
        df_features['hl_avg'] = (df_features['high'] + df_features['low']) / 2
        df_features['hlc_avg'] = (df_features['high'] + df_features['low'] + df_features['close']) / 3
        df_features['ohlc_avg'] = (df_features['open'] + df_features['high'] + 
                                  df_features['low'] + df_features['close']) / 4
        
        # Price ranges
        df_features['hl_range'] = df_features['high'] - df_features['low']
        df_features['oc_range'] = abs(df_features['open'] - df_features['close'])
        
        # Gap features
        df_features['gap'] = df_features['open'] - df_features['close'].shift(1)
        df_features['gap_pct'] = df_features['gap'] / df_features['close'].shift(1)
        
        # Time-based features (if datetime index)
        if isinstance(df_features.index, pd.DatetimeIndex):
            df_features['hour'] = df_features.index.hour
            df_features['day_of_week'] = df_features.index.dayofweek
            df_features['month'] = df_features.index.month
            df_features['quarter'] = df_features.index.quarter
        
        # Volume features (if volume column exists)
        if 'volume' in df_features.columns:
            df_features['volume_sma'] = df_features['volume'].rolling(20).mean()
            df_features['volume_ratio'] = df_features['volume'] / df_features['volume_sma']
            df_features['price_volume'] = df_features['close'] * df_features['volume']
        
        logger.info(f"Created {len(df_features.columns) - len(df.columns)} new features")
        return df_features
    
    def split_sequences(self, data: np.ndarray, n_steps_in: int, n_steps_out: int = 1) -> Tuple[np.ndarray, np.ndarray]:
        """
        Split a multivariate sequence into samples for ML models.
        
        Args:
            data: Input data array
            n_steps_in: Number of time steps for input
            n_steps_out: Number of time steps for output
        
        Returns:
            Tuple of (X, y) arrays
        """
        X, y = [], []
        
        for i in range(len(data)):
            # Find the end of this pattern
            end_ix = i + n_steps_in
            out_end_ix = end_ix + n_steps_out
            
            # Check if we are beyond the dataset
            if out_end_ix > len(data):
                break
                
            # Gather input and output parts of the pattern
            seq_x = data[i:end_ix]
            seq_y = data[end_ix:out_end_ix]
            X.append(seq_x)
            y.append(seq_y)
        
        return np.array(X), np.array(y)


def detect_market_regime(df: pd.DataFrame, window: int = 50) -> pd.DataFrame:
    """
    Detect market regime (trending vs ranging).
    
    Args:
        df: DataFrame with price data
        window: Window for regime detection
    
    Returns:
        DataFrame with regime indicators
    """
    logger.debug(f"Detecting market regime with window: {window}")
    df_regime = df.copy()
    
    # Calculate price efficiency ratio
    price_change = abs(df_regime['close'] - df_regime['close'].shift(window))
    price_volatility = df_regime['close'].diff().abs().rolling(window).sum()
    
    df_regime['efficiency_ratio'] = price_change / price_volatility
    
    # Classify regime
    df_regime['market_regime'] = np.where(
        df_regime['efficiency_ratio'] > 0.3, 'trending', 'ranging'
    )
    
    return df_regime


def calculate_fractal_dimension(series: pd.Series, max_k: int = 10) -> float:
    """
    Calculate fractal dimension of a time series.
    
    Args:
        series: Time series data
        max_k: Maximum k value for calculation
    
    Returns:
        Fractal dimension
    """
    n = len(series)
    rs_values = []
    
    for k in range(2, min(max_k + 1, n // 2)):
        # Split series into k subseries
        subseries_length = n // k
        rs_list = []
        
        for i in range(k):
            start_idx = i * subseries_length
            end_idx = start_idx + subseries_length
            subseries = series.iloc[start_idx:end_idx]
            
            # Calculate R/S statistic
            mean_val = subseries.mean()
            deviations = subseries - mean_val
            cumulative_deviations = deviations.cumsum()
            
            R = cumulative_deviations.max() - cumulative_deviations.min()
            S = subseries.std()
            
            if S > 0:
                rs_list.append(R / S)
        
        if rs_list:
            rs_values.append(np.mean(rs_list))
    
    if len(rs_values) < 2:
        return 0.5  # Random walk
    
    # Calculate Hurst exponent
    log_rs = np.log(rs_values)
    log_n = np.log(range(2, len(rs_values) + 2))
    
    hurst_exponent = np.polyfit(log_n, log_rs, 1)[0]
    
    # Convert to fractal dimension
    fractal_dimension = 2 - hurst_exponent
    
    return fractal_dimension


# Utility functions
def validate_data_quality(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Validate data quality and return report.
    
    Args:
        df: DataFrame to validate
    
    Returns:
        Data quality report
    """
    logger.info("Validating data quality")
    
    report = {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'missing_data': df.isnull().sum().to_dict(),
        'duplicate_rows': df.duplicated().sum(),
        'data_types': df.dtypes.astype(str).to_dict(),
        'memory_usage': df.memory_usage(deep=True).sum(),
        'date_range': None
    }
    
    # Date range info
    if isinstance(df.index, pd.DatetimeIndex):
        report['date_range'] = {
            'start': df.index.min().isoformat(),
            'end': df.index.max().isoformat(),
            'frequency': pd.infer_freq(df.index)
        }
    
    # Numeric columns statistics
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        report['numeric_summary'] = df[numeric_cols].describe().to_dict()
    
    logger.info(f"Data quality report generated for {len(df)} rows")
    return report


if __name__ == "__main__":
    # Example usage
    logger = get_logger("data_processor_test")
    
    # Create sample data
    dates = pd.date_range('2023-01-01', periods=1000, freq='1H')
    np.random.seed(42)
    
    # Generate sample OHLC data
    close_prices = 100 + np.cumsum(np.random.randn(1000) * 0.01)
    
    sample_data = pd.DataFrame({
        'open': close_prices + np.random.randn(1000) * 0.1,
        'high': close_prices + np.abs(np.random.randn(1000) * 0.2),
        'low': close_prices - np.abs(np.random.randn(1000) * 0.2),
        'close': close_prices,
        'volume': np.random.randint(1000, 10000, 1000)
    }, index=dates)
    
    # Test data processor
    processor = DataProcessor()
    
    # Clean data
    clean_data = processor.clean_ohlc_data(sample_data)
    logger.info(f"Original data: {len(sample_data)} rows")
    logger.info(f"Clean data: {len(clean_data)} rows")
    
    # Add features
    feature_data = processor.create_features(clean_data)
    logger.info(f"Features added: {len(feature_data.columns) - len(clean_data.columns)}")
    
    # Validate data quality
    quality_report = validate_data_quality(feature_data)
    logger.info(f"Data quality report: {quality_report['total_rows']} rows, "
               f"{quality_report['total_columns']} columns")
    
    print("Data processor test completed successfully!")

================================================================================
FILE: src\utils\helpers.py
DIRECTORY: src\utils
SIZE: 22102 bytes
================================================================================

"""
General utility functions and helpers for the Quotex trading bot.
Contains common functions used across different modules.
"""

import json
import os
import time
import hashlib
import pickle
import gzip
from typing import Any, Dict, List, Optional, Union, Callable, Tuple
from datetime import datetime, timedelta, timezone
from decimal import Decimal, ROUND_HALF_UP
from pathlib import Path
import functools
import threading
from contextlib import contextmanager
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed

import pandas as pd
import numpy as np
from urllib.parse import urljoin, urlparse

from .logger import get_logger

logger = get_logger(__name__)


# === Time and Date Utilities ===

def get_current_timestamp() -> str:
    """Get current timestamp in ISO format."""
    return datetime.now(timezone.utc).isoformat()


def timestamp_to_datetime(timestamp: Union[int, float, str]) -> datetime:
    """
    Convert timestamp to datetime object.
    
    Args:
        timestamp: Timestamp in various formats (unix, string, etc.)
    
    Returns:
        datetime object
    """
    if isinstance(timestamp, str):
        return pd.to_datetime(timestamp)
    elif isinstance(timestamp, (int, float)):
        # Assume unix timestamp
        if timestamp > 1e10:  # Milliseconds
            timestamp = timestamp / 1000
        return datetime.fromtimestamp(timestamp, tz=timezone.utc)
    else:
        raise ValueError(f"Unsupported timestamp format: {type(timestamp)}")


def datetime_to_timestamp(dt: datetime) -> int:
    """Convert datetime to unix timestamp."""
    return int(dt.timestamp())


def is_market_open(current_time: Optional[datetime] = None, 
                  market_timezone: str = "America/New_York") -> bool:
    """
    Check if market is open (basic implementation for forex/crypto).
    
    Args:
        current_time: Time to check (default: current time)
        market_timezone: Market timezone
    
    Returns:
        True if market is considered open
    """
    if current_time is None:
        current_time = datetime.now(timezone.utc)
    
    # Forex market is open 24/5 (closed on weekends)
    weekday = current_time.weekday()
    
    # Monday 00:00 UTC to Friday 23:59 UTC (simplified)
    if weekday < 5:  # Monday to Friday
        return True
    elif weekday == 6 and current_time.hour >= 22:  # Sunday after 22:00 UTC
        return True
    else:
        return False


def get_next_market_open(current_time: Optional[datetime] = None) -> datetime:
    """Get the next market opening time."""
    if current_time is None:
        current_time = datetime.now(timezone.utc)
    
    if is_market_open(current_time):
        return current_time
    
    # Find next Monday 00:00 UTC
    days_until_monday = (7 - current_time.weekday()) % 7
    if days_until_monday == 0:  # It's Sunday
        days_until_monday = 1
    
    next_open = current_time.replace(hour=0, minute=0, second=0, microsecond=0)
    next_open += timedelta(days=days_until_monday)
    
    return next_open


def sleep_until(target_time: datetime):
    """Sleep until a specific datetime."""
    now = datetime.now(timezone.utc)
    if target_time > now:
        sleep_seconds = (target_time - now).total_seconds()
        time.sleep(sleep_seconds)


# === Financial Calculations ===

def calculate_pip_value(symbol: str, lot_size: float = 1.0, 
                       account_currency: str = "USD") -> float:
    """
    Calculate pip value for a currency pair.
    
    Args:
        symbol: Currency pair (e.g., "EURUSD")
        lot_size: Lot size
        account_currency: Account currency
    
    Returns:
        Pip value
    """
    # Simplified calculation - in practice, you'd need current exchange rates
    major_pairs = ["EURUSD", "GBPUSD", "AUDUSD", "NZDUSD", "USDCAD", "USDCHF", "USDJPY"]
    
    if symbol in major_pairs:
        if symbol.endswith("USD"):
            return 10 * lot_size  # For pairs where USD is quote currency
        elif symbol.startswith("USD"):
            return 10 * lot_size  # Simplified for USD base pairs
    
    return 10 * lot_size  # Default value


def calculate_position_size(account_balance: float, risk_percentage: float,
                          stop_loss_pips: float, pip_value: float) -> float:
    """
    Calculate position size based on risk management.
    
    Args:
        account_balance: Account balance
        risk_percentage: Risk percentage (0-100)
        stop_loss_pips: Stop loss in pips
        pip_value: Value per pip
    
    Returns:
        Position size in lots
    """
    risk_amount = account_balance * (risk_percentage / 100)
    position_size = risk_amount / (stop_loss_pips * pip_value)
    
    return round(position_size, 2)


def calculate_profit_loss(entry_price: float, exit_price: float,
                         position_size: float, pip_value: float,
                         position_type: str = "buy") -> float:
    """
    Calculate profit/loss for a trade.
    
    Args:
        entry_price: Entry price
        exit_price: Exit price
        position_size: Position size in lots
        pip_value: Value per pip
        position_type: "buy" or "sell"
    
    Returns:
        Profit/loss amount
    """
    pip_difference = (exit_price - entry_price) * 10000  # Assuming 4-decimal pairs
    
    if position_type.lower() == "sell":
        pip_difference = -pip_difference
    
    return pip_difference * pip_value * position_size


def calculate_margin_required(position_size: float, current_price: float,
                            leverage: int = 100) -> float:
    """
    Calculate margin required for a position.
    
    Args:
        position_size: Position size in lots
        current_price: Current market price
        leverage: Leverage ratio
    
    Returns:
        Required margin
    """
    notional_value = position_size * 100000 * current_price  # 100000 = standard lot size
    return notional_value / leverage


# === Number and String Utilities ===

def round_to_decimals(value: float, decimals: int) -> float:
    """Round value to specified decimal places."""
    if decimals < 0:
        return value
    
    decimal_value = Decimal(str(value))
    rounded = decimal_value.quantize(
        Decimal('0.' + '0' * decimals), rounding=ROUND_HALF_UP
    )
    return float(rounded)


def format_currency(amount: float, currency: str = "USD", decimals: int = 2) -> str:
    """Format amount as currency string."""
    formatted = f"{amount:,.{decimals}f}"
    return f"{formatted} {currency}"


def format_percentage(value: float, decimals: int = 2) -> str:
    """Format value as percentage."""
    return f"{value:.{decimals}f}%"


def safe_divide(numerator: float, denominator: float, default: float = 0.0) -> float:
    """Safe division that handles division by zero."""
    if denominator == 0:
        return default
    return numerator / denominator


def clamp(value: float, min_val: float, max_val: float) -> float:
    """Clamp value between min and max."""
    return max(min_val, min(value, max_val))


def normalize_symbol(symbol: str) -> str:
    """Normalize trading symbol format."""
    # Remove common separators and convert to uppercase
    normalized = symbol.replace("/", "").replace("-", "").replace("_", "").upper()
    return normalized


# === File and Data Utilities ===

def save_json(data: Dict[str, Any], filepath: Union[str, Path], 
              indent: int = 2, compress: bool = False) -> bool:
    """
    Save data as JSON file.
    
    Args:
        data: Data to save
        filepath: File path
        indent: JSON indentation
        compress: Whether to compress the file
    
    Returns:
        True if successful
    """
    try:
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        json_str = json.dumps(data, indent=indent, default=str)
        
        if compress:
            with gzip.open(f"{filepath}.gz", 'wt', encoding='utf-8') as f:
                f.write(json_str)
        else:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(json_str)
        
        logger.debug(f"Saved JSON data to {filepath}")
        return True
    
    except Exception as e:
        logger.error(f"Failed to save JSON data: {e}")
        return False


def load_json(filepath: Union[str, Path], compressed: bool = False) -> Optional[Dict[str, Any]]:
    """
    Load data from JSON file.
    
    Args:
        filepath: File path
        compressed: Whether file is compressed
    
    Returns:
        Loaded data or None if failed
    """
    try:
        filepath = Path(filepath)
        
        if compressed:
            with gzip.open(f"{filepath}.gz", 'rt', encoding='utf-8') as f:
                data = json.load(f)
        else:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
        
        logger.debug(f"Loaded JSON data from {filepath}")
        return data
    
    except Exception as e:
        logger.error(f"Failed to load JSON data: {e}")
        return None


def save_pickle(data: Any, filepath: Union[str, Path], compress: bool = True) -> bool:
    """Save data using pickle."""
    try:
        filepath = Path(filepath)
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        if compress:
            with gzip.open(f"{filepath}.pkl.gz", 'wb') as f:
                pickle.dump(data, f)
        else:
            with open(f"{filepath}.pkl", 'wb') as f:
                pickle.dump(data, f)
        
        logger.debug(f"Saved pickle data to {filepath}")
        return True
    
    except Exception as e:
        logger.error(f"Failed to save pickle data: {e}")
        return False


def load_pickle(filepath: Union[str, Path], compressed: bool = True) -> Optional[Any]:
    """Load data from pickle file."""
    try:
        filepath = Path(filepath)
        
        if compressed:
            with gzip.open(f"{filepath}.pkl.gz", 'rb') as f:
                data = pickle.load(f)
        else:
            with open(f"{filepath}.pkl", 'rb') as f:
                data = pickle.load(f)
        
        logger.debug(f"Loaded pickle data from {filepath}")
        return data
    
    except Exception as e:
        logger.error(f"Failed to load pickle data: {e}")
        return None


def ensure_directory(directory: Union[str, Path]) -> Path:
    """Ensure directory exists, create if not."""
    dir_path = Path(directory)
    dir_path.mkdir(parents=True, exist_ok=True)
    return dir_path


def get_file_hash(filepath: Union[str, Path], algorithm: str = "md5") -> str:
    """Get hash of file contents."""
    hash_algo = hashlib.new(algorithm)
    
    with open(filepath, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_algo.update(chunk)
    
    return hash_algo.hexdigest()


# === Decorators ===

def retry(max_attempts: int = 3, delay: float = 1.0, 
          exponential_backoff: bool = True, exceptions: Tuple = (Exception,)):
    """
    Retry decorator for functions.
    
    Args:
        max_attempts: Maximum retry attempts
        delay: Delay between attempts
        exponential_backoff: Whether to use exponential backoff
        exceptions: Exception types to catch
    """
    def decorator(func: Callable):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    
                    if attempt < max_attempts - 1:
                        wait_time = delay * (2 ** attempt) if exponential_backoff else delay
                        logger.warning(f"Attempt {attempt + 1} failed: {e}. Retrying in {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"All {max_attempts} attempts failed")
            
            raise last_exception
        
        return wrapper
    return decorator


def timing(func: Callable):
    """Timing decorator to measure function execution time."""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        execution_time = end_time - start_time
        logger.debug(f"{func.__name__} executed in {execution_time:.4f} seconds")
        
        return result
    
    return wrapper


def rate_limit(calls_per_second: float):
    """Rate limiting decorator."""
    min_interval = 1.0 / calls_per_second
    last_called = [0.0]
    
    def decorator(func: Callable):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            left_to_wait = min_interval - elapsed
            
            if left_to_wait > 0:
                time.sleep(left_to_wait)
            
            ret = func(*args, **kwargs)
            last_called[0] = time.time()
            return ret
        
        return wrapper
    return decorator


def cache_result(ttl_seconds: int = 300):
    """Simple caching decorator with TTL."""
    def decorator(func: Callable):
        cache = {}
        
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # Create cache key
            key = str(args) + str(sorted(kwargs.items()))
            current_time = time.time()
            
            # Check if cached result is still valid
            if key in cache:
                result, timestamp = cache[key]
                if current_time - timestamp < ttl_seconds:
                    return result
            
            # Call function and cache result
            result = func(*args, **kwargs)
            cache[key] = (result, current_time)
            
            return result
        
        return wrapper
    return decorator


# === Threading and Async Utilities ===

class ThreadSafeCounter:
    """Thread-safe counter."""
    
    def __init__(self, initial_value: int = 0):
        self._value = initial_value
        self._lock = threading.Lock()
    
    def increment(self, amount: int = 1) -> int:
        with self._lock:
            self._value += amount
            return self._value
    
    def decrement(self, amount: int = 1) -> int:
        with self._lock:
            self._value -= amount
            return self._value
    
    @property
    def value(self) -> int:
        with self._lock:
            return self._value


@contextmanager
def thread_pool_executor(max_workers: int = None):
    """Context manager for ThreadPoolExecutor."""
    executor = ThreadPoolExecutor(max_workers=max_workers)
    try:
        yield executor
    finally:
        executor.shutdown(wait=True)


def run_parallel(functions: List[Callable], max_workers: int = None, 
                timeout: Optional[float] = None) -> List[Any]:
    """
    Run functions in parallel using ThreadPoolExecutor.
    
    Args:
        functions: List of functions to run
        max_workers: Maximum number of worker threads
        timeout: Timeout for each function
    
    Returns:
        List of results
    """
    results = []
    
    with thread_pool_executor(max_workers) as executor:
        # Submit all functions
        future_to_func = {executor.submit(func): func for func in functions}
        
        # Collect results
        for future in as_completed(future_to_func, timeout=timeout):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                logger.error(f"Function failed: {e}")
                results.append(None)
    
    return results


# === Validation Utilities ===

def validate_email(email: str) -> bool:
    """Validate email format."""
    import re
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    return re.match(pattern, email) is not None


def validate_url(url: str) -> bool:
    """Validate URL format."""
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except:
        return False


def validate_symbol_format(symbol: str) -> bool:
    """Validate trading symbol format."""
    # Basic validation for forex pairs
    symbol = symbol.upper()
    if len(symbol) == 6 and symbol.isalpha():
        return True
    return False


def validate_price(price: Union[int, float]) -> bool:
    """Validate price value."""
    return isinstance(price, (int, float)) and price > 0


def validate_config(config: Dict[str, Any], required_keys: List[str]) -> Tuple[bool, List[str]]:
    """
    Validate configuration dictionary.
    
    Args:
        config: Configuration dictionary
        required_keys: List of required keys
    
    Returns:
        Tuple of (is_valid, missing_keys)
    """
    missing_keys = [key for key in required_keys if key not in config]
    return len(missing_keys) == 0, missing_keys


# === Performance Monitoring ===

class PerformanceMonitor:
    """Monitor performance metrics."""
    
    def __init__(self):
        self.metrics = {}
        self.start_times = {}
    
    def start_timer(self, name: str):
        """Start timing an operation."""
        self.start_times[name] = time.time()
    
    def end_timer(self, name: str) -> float:
        """End timing and return duration."""
        if name not in self.start_times:
            return 0.0
        
        duration = time.time() - self.start_times[name]
        
        if name not in self.metrics:
            self.metrics[name] = []
        
        self.metrics[name].append(duration)
        return duration
    
    def get_stats(self, name: str) -> Dict[str, float]:
        """Get statistics for a metric."""
        if name not in self.metrics:
            return {}
        
        values = self.metrics[name]
        return {
            'count': len(values),
            'total': sum(values),
            'average': sum(values) / len(values),
            'min': min(values),
            'max': max(values)
        }
    
    def reset(self):
        """Reset all metrics."""
        self.metrics.clear()
        self.start_times.clear()


# Global performance monitor
performance_monitor = PerformanceMonitor()


# === Utility Classes ===

class ConfigManager:
    """Simple configuration manager."""
    
    def __init__(self, config_file: Union[str, Path]):
        self.config_file = Path(config_file)
        self.config = self.load_config()
    
    def load_config(self) -> Dict[str, Any]:
        """Load configuration from file."""
        if self.config_file.exists():
            return load_json(self.config_file) or {}
        return {}
    
    def save_config(self) -> bool:
        """Save configuration to file."""
        return save_json(self.config, self.config_file)
    
    def get(self, key: str, default: Any = None) -> Any:
        """Get configuration value."""
        keys = key.split('.')
        value = self.config
        
        for k in keys:
            if isinstance(value, dict) and k in value:
                value = value[k]
            else:
                return default
        
        return value
    
    def set(self, key: str, value: Any):
        """Set configuration value."""
        keys = key.split('.')
        config = self.config
        
        for k in keys[:-1]:
            if k not in config:
                config[k] = {}
            config = config[k]
        
        config[keys[-1]] = value
    
    def update(self, updates: Dict[str, Any]):
        """Update multiple configuration values."""
        for key, value in updates.items():
            self.set(key, value)


if __name__ == "__main__":
    # Example usage and tests
    logger = get_logger("helpers_test")
    
    # Test timestamp functions
    current_time = get_current_timestamp()
    logger.info(f"Current timestamp: {current_time}")
    
    # Test market status
    market_open = is_market_open()
    logger.info(f"Market is open: {market_open}")
    
    # Test financial calculations
    position_size = calculate_position_size(10000, 2, 50, 10)
    logger.info(f"Calculated position size: {position_size}")
    
    # Test performance monitor
    performance_monitor.start_timer("test_operation")
    time.sleep(0.1)
    duration = performance_monitor.end_timer("test_operation")
    logger.info(f"Test operation took: {duration:.4f} seconds")
    
    # Test retry decorator
    @retry(max_attempts=3, delay=0.1)
    def failing_function():
        import random
        if random.random() < 0.7:
            raise Exception("Random failure")
        return "Success"
    
    try:
        result = failing_function()
        logger.info(f"Retry test result: {result}")
    except:
        logger.info("Retry test failed after all attempts")
    
    # Test config manager
    config_mgr = ConfigManager("test_config.json")
    config_mgr.set("trading.risk_percentage", 2.0)
    config_mgr.set("api.timeout", 30)
    logger.info(f"Config test: {config_mgr.get('trading.risk_percentage')}")
    
    print("Helpers test completed successfully!")

================================================================================
FILE: src\utils\logger.py
DIRECTORY: src\utils
SIZE: 9840 bytes
================================================================================

"""
Logging configuration and utilities for the Quotex trading bot.
Provides centralized logging with different levels and output formats.
"""

import logging
import logging.handlers
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional


class TradingBotLogger:
    """Custom logger class for the trading bot with multiple handlers and formatters."""
    
    def __init__(self, name: str = "quotex_trading_bot", log_dir: str = "data/logs"):
        self.name = name
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # Create logger
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.DEBUG)
        
        # Prevent duplicate handlers
        if not self.logger.handlers:
            self._setup_handlers()
    
    def _setup_handlers(self):
        """Set up different handlers for console and file logging."""
        
        # Console handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter(
            '%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        console_handler.setFormatter(console_formatter)
        
        # File handler for general logs
        general_log_file = self.log_dir / f"{self.name}.log"
        file_handler = logging.handlers.RotatingFileHandler(
            general_log_file,
            maxBytes=10*1024*1024,  # 10MB
            backupCount=5
        )
        file_handler.setLevel(logging.DEBUG)
        file_formatter = logging.Formatter(
            '%(asctime)s | %(levelname)-8s | %(name)s | %(funcName)s:%(lineno)d | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(file_formatter)
        
        # Error file handler
        error_log_file = self.log_dir / f"{self.name}_errors.log"
        error_handler = logging.handlers.RotatingFileHandler(
            error_log_file,
            maxBytes=5*1024*1024,  # 5MB
            backupCount=3
        )
        error_handler.setLevel(logging.ERROR)
        error_handler.setFormatter(file_formatter)
        
        # Trading activities handler
        trading_log_file = self.log_dir / f"trading_activities.log"
        trading_handler = logging.handlers.RotatingFileHandler(
            trading_log_file,
            maxBytes=20*1024*1024,  # 20MB
            backupCount=10
        )
        trading_handler.setLevel(logging.INFO)
        trading_formatter = logging.Formatter(
            '%(asctime)s | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        trading_handler.setFormatter(trading_formatter)
        
        # Add handlers to logger
        self.logger.addHandler(console_handler)
        self.logger.addHandler(file_handler)
        self.logger.addHandler(error_handler)
        
        # Create separate logger for trading activities
        self.trading_logger = logging.getLogger(f"{self.name}.trading")
        self.trading_logger.setLevel(logging.INFO)
        self.trading_logger.addHandler(trading_handler)
        self.trading_logger.propagate = False
    
    def get_logger(self, module_name: Optional[str] = None) -> logging.Logger:
        """Get a logger instance for a specific module."""
        if module_name:
            return logging.getLogger(f"{self.name}.{module_name}")
        return self.logger
    
    def get_trading_logger(self) -> logging.Logger:
        """Get the specialized trading activities logger."""
        return self.trading_logger


# Global logger instance
_bot_logger = None


def setup_logging(log_level: str = "INFO", log_dir: str = "data/logs") -> TradingBotLogger:
    """
    Set up logging for the entire application.
    
    Args:
        log_level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_dir: Directory to store log files
    
    Returns:
        TradingBotLogger instance
    """
    global _bot_logger
    
    if _bot_logger is None:
        _bot_logger = TradingBotLogger(log_dir=log_dir)
        
        # Set log level
        numeric_level = getattr(logging, log_level.upper(), None)
        if not isinstance(numeric_level, int):
            raise ValueError(f'Invalid log level: {log_level}')
        
        _bot_logger.logger.setLevel(numeric_level)
    
    return _bot_logger


def get_logger(module_name: Optional[str] = None) -> logging.Logger:
    """
    Get a logger instance.
    
    Args:
        module_name: Name of the module requesting the logger
    
    Returns:
        Logger instance
    """
    global _bot_logger
    
    if _bot_logger is None:
        _bot_logger = setup_logging()
    
    return _bot_logger.get_logger(module_name)


def get_trading_logger() -> logging.Logger:
    """
    Get the specialized trading activities logger.
    
    Returns:
        Trading logger instance
    """
    global _bot_logger
    
    if _bot_logger is None:
        _bot_logger = setup_logging()
    
    return _bot_logger.get_trading_logger()


def log_trade(action: str, symbol: str, amount: float, price: float, 
              trade_id: Optional[str] = None, profit: Optional[float] = None, **kwargs):
    """
    Log trading activities with structured format.
    
    Args:
        action: Trade action (BUY, SELL, OPEN, CLOSE)
        symbol: Trading symbol
        amount: Trade amount
        price: Entry/exit price
        trade_id: Unique trade identifier
        profit: Profit/loss amount
        **kwargs: Additional trade parameters
    """
    trading_logger = get_trading_logger()
    
    trade_info = {
        'action': action,
        'symbol': symbol,
        'amount': amount,
        'price': price,
        'timestamp': datetime.now().isoformat()
    }
    
    if trade_id:
        trade_info['trade_id'] = trade_id
    if profit is not None:
        trade_info['profit'] = profit
    
    trade_info.update(kwargs)
    
    # Format log message
    log_parts = [f"{k}={v}" for k, v in trade_info.items()]
    log_message = " | ".join(log_parts)
    
    trading_logger.info(log_message)


def log_signal(signal_type: str, symbol: str, confidence: float, 
               indicators: dict, **kwargs):
    """
    Log trading signals with detailed information.
    
    Args:
        signal_type: Type of signal (BUY, SELL, HOLD)
        symbol: Trading symbol
        confidence: Signal confidence (0-1)
        indicators: Technical indicators values
        **kwargs: Additional signal parameters
    """
    logger = get_logger("signals")
    
    signal_info = {
        'signal': signal_type,
        'symbol': symbol,
        'confidence': f"{confidence:.4f}",
        'timestamp': datetime.now().isoformat()
    }
    
    # Add indicator values
    for indicator, value in indicators.items():
        if isinstance(value, float):
            signal_info[indicator] = f"{value:.6f}"
        else:
            signal_info[indicator] = str(value)
    
    signal_info.update(kwargs)
    
    # Format log message
    log_parts = [f"{k}={v}" for k, v in signal_info.items()]
    log_message = " | ".join(log_parts)
    
    logger.info(f"SIGNAL: {log_message}")


class ContextLogger:
    """Context manager for logging function execution time and errors."""
    
    def __init__(self, logger: logging.Logger, operation: str, level: int = logging.INFO):
        self.logger = logger
        self.operation = operation
        self.level = level
        self.start_time = None
    
    def __enter__(self):
        self.start_time = datetime.now()
        self.logger.log(self.level, f"Starting {self.operation}")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        duration = datetime.now() - self.start_time
        
        if exc_type is None:
            self.logger.log(self.level, 
                          f"Completed {self.operation} in {duration.total_seconds():.3f}s")
        else:
            self.logger.error(
                f"Failed {self.operation} after {duration.total_seconds():.3f}s: {exc_val}"
            )
        
        return False  # Don't suppress exceptions


def log_execution_time(operation: str, level: int = logging.INFO):
    """
    Decorator to log function execution time.
    
    Args:
        operation: Description of the operation
        level: Logging level
    """
    def decorator(func):
        def wrapper(*args, **kwargs):
            logger = get_logger(func.__module__)
            with ContextLogger(logger, f"{operation} ({func.__name__})", level):
                return func(*args, **kwargs)
        return wrapper
    return decorator


# Example usage
if __name__ == "__main__":
    # Setup logging
    setup_logging("DEBUG")
    
    # Get loggers
    logger = get_logger("test")
    trading_logger = get_trading_logger()
    
    # Test logging
    logger.info("Testing logger functionality")
    logger.warning("This is a warning message")
    logger.error("This is an error message")
    
    # Test trading logger
    log_trade("BUY", "EURUSD", 100.0, 1.1234, trade_id="T001")
    log_signal("BUY", "EURUSD", 0.85, {"rsi": 30.5, "macd": 0.001})
    
    # Test context logger
    logger = get_logger("context_test")
    with ContextLogger(logger, "test operation"):
        import time
        time.sleep(0.1)
    
    print("Logging test completed. Check data/logs/ directory for log files.")

================================================================================
FILE: tests\__init__.py
DIRECTORY: tests
SIZE: 0 bytes
================================================================================



================================================================================
FILE: tests\test_bot.py
DIRECTORY: tests
SIZE: 20876 bytes
================================================================================

"""
Test suite for the bot package.
Tests trading bot core functionality, strategies, and risk management.
"""

import unittest
import pytest
import tempfile
import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from unittest.mock import Mock, patch, MagicMock
import asyncio
import time

# Add src to path for testing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

# Mock classes for testing when actual modules aren't available
class MockTradingBot:
    """Mock trading bot for testing."""
    
    def __init__(self, config=None):
        self.config = config or {}
        self.is_running = False
        self.positions = []
        self.balance = 10000.0
        self.trades = []
        
    def start(self):
        self.is_running = True
        return True
        
    def stop(self):
        self.is_running = False
        return True
        
    def place_order(self, symbol, order_type, amount, price=None):
        order = {
            'id': f"ORDER_{len(self.trades)}",
            'symbol': symbol,
            'type': order_type,
            'amount': amount,
            'price': price or 1.0,
            'timestamp': datetime.now(),
            'status': 'filled'
        }
        self.trades.append(order)
        return order
        
    def get_balance(self):
        return self.balance
        
    def get_positions(self):
        return self.positions.copy()


class MockRiskManager:
    """Mock risk manager for testing."""
    
    def __init__(self, max_risk_per_trade=0.02, max_total_risk=0.1):
        self.max_risk_per_trade = max_risk_per_trade
        self.max_total_risk = max_total_risk
        
    def calculate_position_size(self, account_balance, risk_percentage, stop_loss_pips, pip_value):
        return min(account_balance * risk_percentage / (stop_loss_pips * pip_value), 
                  account_balance * self.max_risk_per_trade / (stop_loss_pips * pip_value))
        
    def validate_trade(self, trade_params):
        # Simple validation - always approve for testing
        return True, "Trade approved"
        
    def check_exposure(self, symbol, proposed_amount):
        return True  # Always allow for testing


class MockStrategyManager:
    """Mock strategy manager for testing."""
    
    def __init__(self):
        self.strategies = {}
        self.active_strategy = None
        
    def add_strategy(self, name, strategy):
        self.strategies[name] = strategy
        
    def set_active_strategy(self, name):
        if name in self.strategies:
            self.active_strategy = name
            return True
        return False
        
    def get_signal(self, data):
        if self.active_strategy:
            return {"signal": "BUY", "confidence": 0.75, "price": 1.1234}
        return {"signal": "HOLD", "confidence": 0.5, "price": 1.1234}
        
    def backtest_strategy(self, strategy_name, data, start_date, end_date):
        return {
            "total_return": 0.15,
            "sharpe_ratio": 1.2,
            "max_drawdown": 0.08,
            "win_rate": 0.65,
            "total_trades": 100
        }


class TestTradingBot(unittest.TestCase):
    """Test cases for the main trading bot."""
    
    def setUp(self):
        """Set up test environment."""
        self.config = {
            'api': {
                'timeout': 30,
                'max_retries': 3
            },
            'trading': {
                'risk_percentage': 2.0,
                'max_positions': 5,
                'leverage': 100
            },
            'symbols': ['EURUSD', 'GBPUSD', 'USDJPY']
        }
        self.bot = MockTradingBot(self.config)
    
    def test_bot_initialization(self):
        """Test bot initialization."""
        self.assertIsNotNone(self.bot)
        self.assertEqual(self.bot.config, self.config)
        self.assertFalse(self.bot.is_running)
        self.assertEqual(self.bot.balance, 10000.0)
    
    def test_bot_start_stop(self):
        """Test bot start and stop functionality."""
        # Test start
        result = self.bot.start()
        self.assertTrue(result)
        self.assertTrue(self.bot.is_running)
        
        # Test stop
        result = self.bot.stop()
        self.assertTrue(result)
        self.assertFalse(self.bot.is_running)
    
    def test_place_order(self):
        """Test order placement."""
        order = self.bot.place_order("EURUSD", "BUY", 100.0, 1.1234)
        
        self.assertIsNotNone(order)
        self.assertEqual(order['symbol'], "EURUSD")
        self.assertEqual(order['type'], "BUY")
        self.assertEqual(order['amount'], 100.0)
        self.assertEqual(order['price'], 1.1234)
        self.assertEqual(order['status'], 'filled')
        self.assertIn('id', order)
        self.assertIn('timestamp', order)
    
    def test_multiple_orders(self):
        """Test placing multiple orders."""
        orders = []
        symbols = ['EURUSD', 'GBPUSD', 'USDJPY']
        
        for i, symbol in enumerate(symbols):
            order = self.bot.place_order(symbol, "BUY", 100.0 * (i + 1), 1.0 + i * 0.1)
            orders.append(order)
        
        self.assertEqual(len(orders), 3)
        self.assertEqual(len(self.bot.trades), 3)
        
        # Check order IDs are unique
        order_ids = [order['id'] for order in orders]
        self.assertEqual(len(set(order_ids)), 3)
    
    def test_get_balance(self):
        """Test balance retrieval."""
        balance = self.bot.get_balance()
        self.assertEqual(balance, 10000.0)
        self.assertIsInstance(balance, float)
    
    def test_get_positions(self):
        """Test position retrieval."""
        positions = self.bot.get_positions()
        self.assertIsInstance(positions, list)
        # Initially should be empty
        self.assertEqual(len(positions), 0)


class TestRiskManager(unittest.TestCase):
    """Test cases for risk management."""
    
    def setUp(self):
        """Set up test environment."""
        self.risk_manager = MockRiskManager(max_risk_per_trade=0.02, max_total_risk=0.1)
    
    def test_position_size_calculation(self):
        """Test position size calculation."""
        account_balance = 10000.0
        risk_percentage = 0.02  # 2%
        stop_loss_pips = 50
        pip_value = 10
        
        position_size = self.risk_manager.calculate_position_size(
            account_balance, risk_percentage, stop_loss_pips, pip_value
        )
        
        self.assertIsInstance(position_size, float)
        self.assertGreater(position_size, 0)
        
        # Check that position size respects risk limits
        max_loss = position_size * stop_loss_pips * pip_value
        max_allowed_loss = account_balance * risk_percentage
        self.assertLessEqual(max_loss, max_allowed_loss * 1.01)  # Allow small rounding error
    
    def test_zero_risk_position_size(self):
        """Test position size with zero risk."""
        position_size = self.risk_manager.calculate_position_size(10000.0, 0, 50, 10)
        self.assertEqual(position_size, 0)
    
    def test_trade_validation(self):
        """Test trade validation."""
        trade_params = {
            'symbol': 'EURUSD',
            'amount': 100.0,
            'type': 'BUY',
            'stop_loss': 50,
            'take_profit': 100
        }
        
        is_valid, message = self.risk_manager.validate_trade(trade_params)
        self.assertTrue(is_valid)
        self.assertIsInstance(message, str)
    
    def test_exposure_check(self):
        """Test exposure checking."""
        result = self.risk_manager.check_exposure('EURUSD', 1000.0)
        self.assertIsInstance(result, bool)


class TestStrategyManager(unittest.TestCase):
    """Test cases for strategy management."""
    
    def setUp(self):
        """Set up test environment."""
        self.strategy_manager = MockStrategyManager()
        
        # Mock strategy
        self.mock_strategy = Mock()
        self.mock_strategy.name = "TestStrategy"
        self.mock_strategy.get_signal.return_value = {"signal": "BUY", "confidence": 0.8}
    
    def test_add_strategy(self):
        """Test adding strategies."""
        self.strategy_manager.add_strategy("test_strategy", self.mock_strategy)
        self.assertIn("test_strategy", self.strategy_manager.strategies)
        self.assertEqual(self.strategy_manager.strategies["test_strategy"], self.mock_strategy)
    
    def test_set_active_strategy(self):
        """Test setting active strategy."""
        # Add strategy first
        self.strategy_manager.add_strategy("test_strategy", self.mock_strategy)
        
        # Set as active
        result = self.strategy_manager.set_active_strategy("test_strategy")
        self.assertTrue(result)
        self.assertEqual(self.strategy_manager.active_strategy, "test_strategy")
        
        # Try to set non-existent strategy
        result = self.strategy_manager.set_active_strategy("nonexistent")
        self.assertFalse(result)
    
    def test_get_signal(self):
        """Test signal generation."""
        # Test without active strategy
        signal = self.strategy_manager.get_signal(None)
        self.assertEqual(signal["signal"], "HOLD")
        
        # Test with active strategy
        self.strategy_manager.add_strategy("test_strategy", self.mock_strategy)
        self.strategy_manager.set_active_strategy("test_strategy")
        
        signal = self.strategy_manager.get_signal(None)
        self.assertIn("signal", signal)
        self.assertIn("confidence", signal)
        self.assertIn("price", signal)
    
    def test_backtest_strategy(self):
        """Test strategy backtesting."""
        self.strategy_manager.add_strategy("test_strategy", self.mock_strategy)
        
        # Mock data
        data = pd.DataFrame({
            'close': [1.1, 1.2, 1.15, 1.25, 1.2],
            'volume': [1000, 1200, 1100, 1300, 1150]
        })
        
        results = self.strategy_manager.backtest_strategy(
            "test_strategy", data, "2023-01-01", "2023-12-31"
        )
        
        self.assertIsInstance(results, dict)
        expected_keys = ["total_return", "sharpe_ratio", "max_drawdown", "win_rate", "total_trades"]
        for key in expected_keys:
            self.assertIn(key, results)


class TestTradingIntegration(unittest.TestCase):
    """Integration tests for trading components."""
    
    def setUp(self):
        """Set up integration test environment."""
        self.bot = MockTradingBot()
        self.risk_manager = MockRiskManager()
        self.strategy_manager = MockStrategyManager()
        
        # Add a test strategy
        mock_strategy = Mock()
        mock_strategy.get_signal.return_value = {"signal": "BUY", "confidence": 0.8, "price": 1.1234}
        self.strategy_manager.add_strategy("test_strategy", mock_strategy)
        self.strategy_manager.set_active_strategy("test_strategy")
    
    def test_complete_trading_workflow(self):
        """Test complete trading workflow from signal to execution."""
        # Step 1: Get signal from strategy
        market_data = pd.DataFrame({
            'open': [1.1200, 1.1210, 1.1220],
            'high': [1.1250, 1.1260, 1.1270],
            'low': [1.1180, 1.1190, 1.1200],
            'close': [1.1230, 1.1240, 1.1250],
            'volume': [10000, 12000, 11000]
        })
        
        signal = self.strategy_manager.get_signal(market_data)
        self.assertEqual(signal["signal"], "BUY")
        
        # Step 2: Risk management validation
        trade_params = {
            'symbol': 'EURUSD',
            'amount': 100.0,
            'type': signal["signal"],
            'price': signal["price"]
        }
        
        is_valid, message = self.risk_manager.validate_trade(trade_params)
        self.assertTrue(is_valid)
        
        # Step 3: Calculate position size
        position_size = self.risk_manager.calculate_position_size(
            self.bot.get_balance(), 0.02, 50, 10
        )
        self.assertGreater(position_size, 0)
        
        # Step 4: Execute trade
        order = self.bot.place_order(
            trade_params['symbol'], 
            trade_params['type'], 
            position_size, 
            trade_params['price']
        )
        
        self.assertIsNotNone(order)
        self.assertEqual(order['status'], 'filled')
    
    def test_risk_management_integration(self):
        """Test risk management across multiple trades."""
        symbols = ['EURUSD', 'GBPUSD', 'USDJPY']
        orders = []
        
        for symbol in symbols:
            # Check exposure before trade
            exposure_ok = self.risk_manager.check_exposure(symbol, 100.0)
            self.assertTrue(exposure_ok)
            
            # Calculate position size
            position_size = self.risk_manager.calculate_position_size(
                self.bot.get_balance(), 0.01, 50, 10  # 1% risk per trade
            )
            
            # Validate trade
            trade_params = {
                'symbol': symbol,
                'amount': position_size,
                'type': 'BUY'
            }
            is_valid, _ = self.risk_manager.validate_trade(trade_params)
            self.assertTrue(is_valid)
            
            # Execute if valid
            if is_valid:
                order = self.bot.place_order(symbol, 'BUY', position_size, 1.0)
                orders.append(order)
        
        # Check that all trades were executed
        self.assertEqual(len(orders), 3)
        self.assertEqual(len(self.bot.trades), 3)


class TestAsyncTradingBot(unittest.TestCase):
    """Test cases for asynchronous trading bot operations."""
    
    def setUp(self):
        """Set up async test environment."""
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
    
    def tearDown(self):
        """Clean up async test environment."""
        self.loop.close()
    
    async def async_market_data_feed(self):
        """Mock async market data feed."""
        for i in range(5):
            yield {
                'symbol': 'EURUSD',
                'price': 1.1200 + i * 0.0001,
                'timestamp': datetime.now(),
                'volume': 1000 + i * 100
            }
            await asyncio.sleep(0.01)  # Simulate real-time delay
    
    def test_async_data_processing(self):
        """Test asynchronous data processing."""
        async def test_coroutine():
            data_points = []
            async for data in self.async_market_data_feed():
                data_points.append(data)
            return data_points
        
        # Run the async test
        result = self.loop.run_until_complete(test_coroutine())
        
        self.assertEqual(len(result), 5)
        self.assertEqual(result[0]['symbol'], 'EURUSD')
        self.assertAlmostEqual(result[0]['price'], 1.1200, places=4)
        self.assertAlmostEqual(result[-1]['price'], 1.1204, places=4)


class TestPerformanceMetrics(unittest.TestCase):
    """Test cases for trading performance metrics."""
    
    def setUp(self):
        """Set up performance test data."""
        # Create sample trade history
        np.random.seed(42)
        self.trades = []
        
        for i in range(100):
            # Random P&L between -100 and +150 (slight positive bias)
            pnl = np.random.normal(5, 50)  # Mean profit of 5, std dev of 50
            
            trade = {
                'id': f'T{i:03d}',
                'symbol': np.random.choice(['EURUSD', 'GBPUSD', 'USDJPY']),
                'type': np.random.choice(['BUY', 'SELL']),
                'amount': np.random.uniform(0.1, 2.0),
                'entry_price': np.random.uniform(1.0, 1.5),
                'exit_price': np.random.uniform(1.0, 1.5),
                'pnl': pnl,
                'duration': np.random.uniform(1, 24),  # hours
                'timestamp': datetime.now() - timedelta(days=i)
            }
            self.trades.append(trade)
    
    def calculate_performance_metrics(self, trades):
        """Calculate performance metrics from trades."""
        if not trades:
            return {}
        
        pnls = [trade['pnl'] for trade in trades]
        
        metrics = {
            'total_trades': len(trades),
            'total_pnl': sum(pnls),
            'average_pnl': np.mean(pnls),
            'win_rate': len([p for p in pnls if p > 0]) / len(pnls),
            'profit_factor': abs(sum([p for p in pnls if p > 0])) / abs(sum([p for p in pnls if p < 0])) if any(p < 0 for p in pnls) else float('inf'),
            'max_win': max(pnls),
            'max_loss': min(pnls),
            'sharpe_ratio': np.mean(pnls) / np.std(pnls) if np.std(pnls) > 0 else 0
        }
        
        return metrics
    
    def test_performance_calculation(self):
        """Test performance metrics calculation."""
        metrics = self.calculate_performance_metrics(self.trades)
        
        # Verify all expected metrics are present
        expected_keys = [
            'total_trades', 'total_pnl', 'average_pnl', 'win_rate', 
            'profit_factor', 'max_win', 'max_loss', 'sharpe_ratio'
        ]
        
        for key in expected_keys:
            self.assertIn(key, metrics)
        
        # Verify metric ranges
        self.assertEqual(metrics['total_trades'], 100)
        self.assertGreaterEqual(metrics['win_rate'], 0)
        self.assertLessEqual(metrics['win_rate'], 1)
        self.assertGreater(metrics['max_win'], 0)
        self.assertLess(metrics['max_loss'], 0)
    
    def test_empty_trades_performance(self):
        """Test performance calculation with no trades."""
        metrics = self.calculate_performance_metrics([])
        self.assertEqual(metrics, {})
    
    def test_single_trade_performance(self):
        """Test performance calculation with single trade."""
        single_trade = [self.trades[0]]
        metrics = self.calculate_performance_metrics(single_trade)
        
        self.assertEqual(metrics['total_trades'], 1)
        self.assertEqual(metrics['total_pnl'], single_trade[0]['pnl'])
        self.assertEqual(metrics['average_pnl'], single_trade[0]['pnl'])


class TestBotConfiguration(unittest.TestCase):
    """Test cases for bot configuration management."""
    
    def test_config_validation(self):
        """Test configuration validation."""
        valid_config = {
            'api': {'timeout': 30, 'url': 'https://api.example.com'},
            'trading': {'risk_percentage': 2.0, 'max_positions': 5},
            'symbols': ['EURUSD', 'GBPUSD']
        }
        
        # Test valid config
        bot = MockTradingBot(valid_config)
        self.assertEqual(bot.config, valid_config)
    
    def test_config_defaults(self):
        """Test default configuration handling."""
        bot = MockTradingBot()
        self.assertEqual(bot.config, {})
        
        # Should still function with empty config
        self.assertIsNotNone(bot.get_balance())
        self.assertIsNotNone(bot.get_positions())


if __name__ == "__main__":
    # Run specific test suites
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # Add test cases
    suite.addTest(loader.loadTestsFromTestCase(TestTradingBot))
    suite.addTest(loader.loadTestsFromTestCase(TestRiskManager))
    suite.addTest(loader.loadTestsFromTestCase(TestStrategyManager))
    suite.addTest(loader.loadTestsFromTestCase(TestTradingIntegration))
    suite.addTest(loader.loadTestsFromTestCase(TestPerformanceMetrics))
    suite.addTest(loader.loadTestsFromTestCase(TestBotConfiguration))
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2, buffer=True)
    result = runner.run(suite)
    
    # Print summary
    print(f"\nTests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    
    if result.failures:
        print("\nFailures:")
        for test, traceback in result.failures:
            print(f"- {test}: {traceback}")
    
    if result.errors:
        print("\nErrors:")
        for test, traceback in result.errors:
            print(f"- {test}: {traceback}")

================================================================================
FILE: tests\test_indicators.py
DIRECTORY: tests
SIZE: 25368 bytes
================================================================================

"""
Test suite for technical indicators.
Tests all technical analysis indicators and signal generation.
"""

import unittest
import pytest
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import sys
import os

# Add src to path for testing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))


class MockTechnicalIndicators:
    """Mock technical indicators for testing when actual module isn't available."""
    
    @staticmethod
    def sma(data, period=20):
        """Simple Moving Average."""
        if isinstance(data, pd.Series):
            return data.rolling(window=period).mean()
        elif isinstance(data, (list, np.ndarray)):
            data = pd.Series(data)
            return data.rolling(window=period).mean()
        else:
            raise ValueError("Data must be pandas Series, list, or numpy array")
    
    @staticmethod
    def ema(data, period=20):
        """Exponential Moving Average."""
        if isinstance(data, pd.Series):
            return data.ewm(span=period).mean()
        elif isinstance(data, (list, np.ndarray)):
            data = pd.Series(data)
            return data.ewm(span=period).mean()
        else:
            raise ValueError("Data must be pandas Series, list, or numpy array")
    
    @staticmethod
    def rsi(data, period=14):
        """Relative Strength Index."""
        if isinstance(data, (list, np.ndarray)):
            data = pd.Series(data)
        
        delta = data.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    @staticmethod
    def macd(data, fast=12, slow=26, signal=9):
        """MACD Indicator."""
        if isinstance(data, (list, np.ndarray)):
            data = pd.Series(data)
        
        ema_fast = data.ewm(span=fast).mean()
        ema_slow = data.ewm(span=slow).mean()
        macd_line = ema_fast - ema_slow
        signal_line = macd_line.ewm(span=signal).mean()
        histogram = macd_line - signal_line
        
        return pd.DataFrame({
            'macd': macd_line,
            'signal': signal_line,
            'histogram': histogram
        })
    
    @staticmethod
    def bollinger_bands(data, period=20, std_dev=2):
        """Bollinger Bands."""
        if isinstance(data, (list, np.ndarray)):
            data = pd.Series(data)
        
        sma = data.rolling(window=period).mean()
        std = data.rolling(window=period).std()
        
        upper_band = sma + (std * std_dev)
        lower_band = sma - (std * std_dev)
        
        return pd.DataFrame({
            'upper': upper_band,
            'middle': sma,
            'lower': lower_band
        })
    
    @staticmethod
    def stochastic(high, low, close, k_period=14, d_period=3):
        """Stochastic Oscillator."""
        if isinstance(high, (list, np.ndarray)):
            high = pd.Series(high)
        if isinstance(low, (list, np.ndarray)):
            low = pd.Series(low)
        if isinstance(close, (list, np.ndarray)):
            close = pd.Series(close)
        
        lowest_low = low.rolling(window=k_period).min()
        highest_high = high.rolling(window=k_period).max()
        
        k_percent = 100 * ((close - lowest_low) / (highest_high - lowest_low))
        d_percent = k_percent.rolling(window=d_period).mean()
        
        return pd.DataFrame({
            'k': k_percent,
            'd': d_percent
        })
    
    @staticmethod
    def atr(high, low, close, period=14):
        """Average True Range."""
        if isinstance(high, (list, np.ndarray)):
            high = pd.Series(high)
        if isinstance(low, (list, np.ndarray)):
            low = pd.Series(low)
        if isinstance(close, (list, np.ndarray)):
            close = pd.Series(close)
        
        tr1 = high - low
        tr2 = abs(high - close.shift(1))
        tr3 = abs(low - close.shift(1))
        
        true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        atr = true_range.rolling(window=period).mean()
        
        return atr


class MockSignalGenerator:
    """Mock signal generator for testing."""
    
    def __init__(self):
        self.indicators = MockTechnicalIndicators()
    
    def generate_signals(self, data):
        """Generate trading signals from market data."""
        signals = pd.DataFrame(index=data.index)
        signals['signal'] = 'HOLD'
        signals['confidence'] = 0.5
        
        # Simple RSI-based signals
        rsi = self.indicators.rsi(data['close'])
        signals.loc[rsi < 30, 'signal'] = 'BUY'
        signals.loc[rsi < 30, 'confidence'] = 0.8
        signals.loc[rsi > 70, 'signal'] = 'SELL'
        signals.loc[rsi > 70, 'confidence'] = 0.8
        
        return signals
    
    def crossover_signal(self, fast_ma, slow_ma):
        """Generate crossover signals."""
        signals = pd.Series('HOLD', index=fast_ma.index)
        
        # Golden cross (fast MA crosses above slow MA)
        golden_cross = (fast_ma > slow_ma) & (fast_ma.shift(1) <= slow_ma.shift(1))
        signals.loc[golden_cross] = 'BUY'
        
        # Death cross (fast MA crosses below slow MA)
        death_cross = (fast_ma < slow_ma) & (fast_ma.shift(1) >= slow_ma.shift(1))
        signals.loc[death_cross] = 'SELL'
        
        return signals


class TestTechnicalIndicators(unittest.TestCase):
    """Test cases for technical indicators."""
    
    def setUp(self):
        """Set up test data."""
        # Create sample price data
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=100, freq='D')
        
        # Generate realistic price data
        price_changes = np.random.normal(0, 0.01, 100)
        prices = 100 * np.exp(np.cumsum(price_changes))
        
        self.sample_data = pd.DataFrame({
            'open': prices + np.random.normal(0, 0.1, 100),
            'high': prices + np.abs(np.random.normal(0, 0.3, 100)),
            'low': prices - np.abs(np.random.normal(0, 0.3, 100)),
            'close': prices,
            'volume': np.random.randint(1000, 10000, 100)
        }, index=dates)
        
        self.indicators = MockTechnicalIndicators()
    
    def test_sma_calculation(self):
        """Test Simple Moving Average calculation."""
        sma_20 = self.indicators.sma(self.sample_data['close'], 20)
        
        # Check basic properties
        self.assertIsInstance(sma_20, pd.Series)
        self.assertEqual(len(sma_20), len(self.sample_data))
        
        # First 19 values should be NaN
        self.assertTrue(sma_20.iloc[:19].isna().all())
        
        # Check manual calculation for a specific point
        manual_sma = self.sample_data['close'].iloc[19:40].mean()
        calculated_sma = sma_20.iloc[39]
        self.assertAlmostEqual(manual_sma, calculated_sma, places=10)
        
        # SMA should smooth the data (less volatile)
        sma_volatility = sma_20.dropna().std()
        price_volatility = self.sample_data['close'].std()
        self.assertLess(sma_volatility, price_volatility)
    
    def test_ema_calculation(self):
        """Test Exponential Moving Average calculation."""
        ema_20 = self.indicators.ema(self.sample_data['close'], 20)
        
        # Check basic properties
        self.assertIsInstance(ema_20, pd.Series)
        self.assertEqual(len(ema_20), len(self.sample_data))
        
        # EMA should not have initial NaN values (unlike SMA)
        self.assertFalse(ema_20.iloc[0:].isna().all())
        
        # EMA should be more responsive than SMA
        sma_20 = self.indicators.sma(self.sample_data['close'], 20)
        
        # Compare responsiveness (EMA should change more quickly)
        ema_changes = ema_20.diff().abs().mean()
        sma_changes = sma_20.diff().abs().mean()
        self.assertGreater(ema_changes, sma_changes)
    
    def test_rsi_calculation(self):
        """Test Relative Strength Index calculation."""
        rsi = self.indicators.rsi(self.sample_data['close'], 14)
        
        # Check basic properties
        self.assertIsInstance(rsi, pd.Series)
        self.assertEqual(len(rsi), len(self.sample_data))
        
        # RSI should be between 0 and 100
        rsi_values = rsi.dropna()
        self.assertTrue((rsi_values >= 0).all())
        self.assertTrue((rsi_values <= 100).all())
        
        # Check RSI properties
        self.assertGreater(rsi_values.max(), 50)  # Should have some variation
        self.assertLess(rsi_values.min(), 50)
    
    def test_macd_calculation(self):
        """Test MACD calculation."""
        macd_data = self.indicators.macd(self.sample_data['close'])
        
        # Check structure
        self.assertIsInstance(macd_data, pd.DataFrame)
        expected_columns = ['macd', 'signal', 'histogram']
        for col in expected_columns:
            self.assertIn(col, macd_data.columns)
        
        # Check relationships
        # Histogram should be MACD - Signal
        calculated_histogram = macd_data['macd'] - macd_data['signal']
        np.testing.assert_array_almost_equal(
            macd_data['histogram'].dropna(), 
            calculated_histogram.dropna(), 
            decimal=10
        )
        
        # Signal line should be smoother than MACD line
        macd_volatility = macd_data['macd'].std()
        signal_volatility = macd_data['signal'].std()
        self.assertLess(signal_volatility, macd_volatility)
    
    def test_bollinger_bands_calculation(self):
        """Test Bollinger Bands calculation."""
        bb = self.indicators.bollinger_bands(self.sample_data['close'], period=20, std_dev=2)
        
        # Check structure
        self.assertIsInstance(bb, pd.DataFrame)
        expected_columns = ['upper', 'middle', 'lower']
        for col in expected_columns:
            self.assertIn(col, bb.columns)
        
        # Check relationships
        bb_clean = bb.dropna()
        
        # Upper band should be above middle, middle above lower
        self.assertTrue((bb_clean['upper'] >= bb_clean['middle']).all())
        self.assertTrue((bb_clean['middle'] >= bb_clean['lower']).all())
        
        # Middle band should be SMA
        sma_20 = self.indicators.sma(self.sample_data['close'], 20)
        np.testing.assert_array_almost_equal(
            bb['middle'].dropna(), 
            sma_20.dropna(), 
            decimal=10
        )
        
        # Most prices should be within the bands
        prices = self.sample_data['close']
        within_bands = ((prices >= bb['lower']) & (prices <= bb['upper'])).sum()
        total_valid = len(bb.dropna())
        within_percentage = within_bands / total_valid
        self.assertGreater(within_percentage, 0.8)  # Should be around 95% normally
    
    def test_stochastic_calculation(self):
        """Test Stochastic Oscillator calculation."""
        stoch = self.indicators.stochastic(
            self.sample_data['high'], 
            self.sample_data['low'], 
            self.sample_data['close']
        )
        
        # Check structure
        self.assertIsInstance(stoch, pd.DataFrame)
        expected_columns = ['k', 'd']
        for col in expected_columns:
            self.assertIn(col, stoch.columns)
        
        # Check value ranges
        stoch_clean = stoch.dropna()
        
        # K and D should be between 0 and 100
        self.assertTrue((stoch_clean['k'] >= 0).all())
        self.assertTrue((stoch_clean['k'] <= 100).all())
        self.assertTrue((stoch_clean['d'] >= 0).all())
        self.assertTrue((stoch_clean['d'] <= 100).all())
        
        # D should be smoother than K (it's a moving average of K)
        k_volatility = stoch_clean['k'].std()
        d_volatility = stoch_clean['d'].std()
        self.assertLessEqual(d_volatility, k_volatility)
    
    def test_atr_calculation(self):
        """Test Average True Range calculation."""
        atr = self.indicators.atr(
            self.sample_data['high'], 
            self.sample_data['low'], 
            self.sample_data['close']
        )
        
        # Check basic properties
        self.assertIsInstance(atr, pd.Series)
        self.assertEqual(len(atr), len(self.sample_data))
        
        # ATR should be positive
        atr_values = atr.dropna()
        self.assertTrue((atr_values > 0).all())
        
        # ATR should be reasonable compared to price range
        price_range = self.sample_data['close'].max() - self.sample_data['close'].min()
        avg_atr = atr_values.mean()
        self.assertLess(avg_atr, price_range)  # ATR shouldn't exceed total price range
    
    def test_indicator_with_insufficient_data(self):
        """Test indicators with insufficient data."""
        short_data = self.sample_data['close'].iloc[:5]  # Only 5 data points
        
        # Should still work but return mostly NaN
        sma_20 = self.indicators.sma(short_data, 20)
        self.assertTrue(sma_20.isna().all())
        
        rsi_14 = self.indicators.rsi(short_data, 14)
        self.assertTrue(rsi_14.isna().all())
    
    def test_indicator_with_list_input(self):
        """Test indicators with list input instead of pandas Series."""
        price_list = self.sample_data['close'].tolist()
        
        sma_result = self.indicators.sma(price_list, 10)
        self.assertIsInstance(sma_result, pd.Series)
        
        rsi_result = self.indicators.rsi(price_list, 14)
        self.assertIsInstance(rsi_result, pd.Series)
    
    def test_indicator_with_numpy_input(self):
        """Test indicators with numpy array input."""
        price_array = self.sample_data['close'].values
        
        sma_result = self.indicators.sma(price_array, 10)
        self.assertIsInstance(sma_result, pd.Series)
        
        rsi_result = self.indicators.rsi(price_array, 14)
        self.assertIsInstance(rsi_result, pd.Series)


class TestSignalGenerator(unittest.TestCase):
    """Test cases for signal generation."""
    
    def setUp(self):
        """Set up test data."""
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=100, freq='D')
        
        # Create trending data for better signal testing
        trend = np.linspace(100, 110, 100)
        noise = np.random.normal(0, 0.5, 100)
        prices = trend + noise
        
        self.sample_data = pd.DataFrame({
            'open': prices + np.random.normal(0, 0.1, 100),
            'high': prices + np.abs(np.random.normal(0, 0.3, 100)),
            'low': prices - np.abs(np.random.normal(0, 0.3, 100)),
            'close': prices,
            'volume': np.random.randint(1000, 10000, 100)
        }, index=dates)
        
        self.signal_generator = MockSignalGenerator()
    
    def test_signal_generation(self):
        """Test basic signal generation."""
        signals = self.signal_generator.generate_signals(self.sample_data)
        
        # Check structure
        self.assertIsInstance(signals, pd.DataFrame)
        self.assertIn('signal', signals.columns)
        self.assertIn('confidence', signals.columns)
        self.assertEqual(len(signals), len(self.sample_data))
        
        # Check signal values
        valid_signals = ['BUY', 'SELL', 'HOLD']
        unique_signals = signals['signal'].unique()
        for signal in unique_signals:
            self.assertIn(signal, valid_signals)
        
        # Check confidence values
        self.assertTrue((signals['confidence'] >= 0).all())
        self.assertTrue((signals['confidence'] <= 1).all())
    
    def test_crossover_signals(self):
        """Test moving average crossover signals."""
        # Create clear crossover scenario
        fast_ma = pd.Series([10, 11, 12, 13, 14, 15, 16, 15, 14, 13], 
                           index=range(10))
        slow_ma = pd.Series([12, 12, 12, 12, 12, 12, 12, 12, 12, 12], 
                           index=range(10))
        
        signals = self.signal_generator.crossover_signal(fast_ma, slow_ma)
        
        # Should generate BUY signal when fast crosses above slow
        # Should generate SELL signal when fast crosses below slow
        self.assertIn('BUY', signals.values)
        self.assertIn('SELL', signals.values)
        self.assertIn('HOLD', signals.values)
    
    def test_rsi_signals(self):
        """Test RSI-based signals."""
        # Create data that will generate extreme RSI values
        declining_prices = pd.Series(range(100, 70, -1))  # Declining prices for high RSI
        rising_prices = pd.Series(range(70, 100))  # Rising prices for low RSI
        
        # Test declining prices (should generate oversold/buy signals)
        test_data = pd.DataFrame({'close': declining_prices})
        signals = self.signal_generator.generate_signals(test_data)
        
        # Should have some BUY signals due to oversold conditions
        self.assertIn('BUY', signals['signal'].values)
    
    def test_signal_consistency(self):
        """Test signal consistency across multiple runs."""
        signals1 = self.signal_generator.generate_signals(self.sample_data)
        signals2 = self.signal_generator.generate_signals(self.sample_data)
        
        # Signals should be identical for same input
        pd.testing.assert_frame_equal(signals1, signals2)
    
    def test_signal_timing(self):
        """Test signal timing and lag."""
        # Signals should not have lookahead bias
        signals = self.signal_generator.generate_signals(self.sample_data)
        
        # Check that signals are based only on current and past data
        # This is implicit in our implementation, but we test that signals exist
        non_hold_signals = signals[signals['signal'] != 'HOLD']
        self.assertGreater(len(non_hold_signals), 0)  # Should have some signals
    
    def test_confidence_calculation(self):
        """Test confidence score calculation."""
        signals = self.signal_generator.generate_signals(self.sample_data)
        
        # Confidence should be higher for BUY/SELL than HOLD
        buy_sell_confidence = signals[signals['signal'].isin(['BUY', 'SELL'])]['confidence']
        hold_confidence = signals[signals['signal'] == 'HOLD']['confidence']
        
        if len(buy_sell_confidence) > 0 and len(hold_confidence) > 0:
            self.assertGreater(buy_sell_confidence.mean(), hold_confidence.mean())


class TestCustomIndicators(unittest.TestCase):
    """Test cases for custom indicators."""
    
    def setUp(self):
        """Set up test data."""
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=100, freq='D')
        prices = 100 + np.cumsum(np.random.randn(100) * 0.01)
        
        self.sample_data = pd.DataFrame({
            'open': prices + np.random.normal(0, 0.1, 100),
            'high': prices + np.abs(np.random.normal(0, 0.3, 100)),
            'low': prices - np.abs(np.random.normal(0, 0.3, 100)),
            'close': prices,
            'volume': np.random.randint(1000, 10000, 100)
        }, index=dates)
    
    def calculate_williams_r(self, high, low, close, period=14):
        """Williams %R indicator."""
        if isinstance(high, (list, np.ndarray)):
            high = pd.Series(high)
        if isinstance(low, (list, np.ndarray)):
            low = pd.Series(low)
        if isinstance(close, (list, np.ndarray)):
            close = pd.Series(close)
        
        highest_high = high.rolling(window=period).max()
        lowest_low = low.rolling(window=period).min()
        
        williams_r = -100 * ((highest_high - close) / (highest_high - lowest_low))
        return williams_r
    
    def calculate_cci(self, high, low, close, period=20):
        """Commodity Channel Index."""
        if isinstance(high, (list, np.ndarray)):
            high = pd.Series(high)
        if isinstance(low, (list, np.ndarray)):
            low = pd.Series(low)
        if isinstance(close, (list, np.ndarray)):
            close = pd.Series(close)
        
        typical_price = (high + low + close) / 3
        sma_tp = typical_price.rolling(window=period).mean()
        mean_deviation = typical_price.rolling(window=period).apply(
            lambda x: np.abs(x - x.mean()).mean()
        )
        
        cci = (typical_price - sma_tp) / (0.015 * mean_deviation)
        return cci
    
    def test_williams_r(self):
        """Test Williams %R calculation."""
        williams_r = self.calculate_williams_r(
            self.sample_data['high'], 
            self.sample_data['low'], 
            self.sample_data['close']
        )
        
        # Check basic properties
        self.assertIsInstance(williams_r, pd.Series)
        
        # Williams %R should be between -100 and 0
        wr_values = williams_r.dropna()
        self.assertTrue((wr_values >= -100).all())
        self.assertTrue((wr_values <= 0).all())
    
    def test_cci(self):
        """Test Commodity Channel Index calculation."""
        cci = self.calculate_cci(
            self.sample_data['high'], 
            self.sample_data['low'], 
            self.sample_data['close']
        )
        
        # Check basic properties
        self.assertIsInstance(cci, pd.Series)
        
        # CCI typically ranges from -300 to +300, but can exceed
        cci_values = cci.dropna()
        self.assertTrue(len(cci_values) > 0)
        
        # Should have some variation
        self.assertGreater(cci_values.std(), 0)


class TestIndicatorPerformance(unittest.TestCase):
    """Test cases for indicator performance and efficiency."""
    
    def setUp(self):
        """Set up large dataset for performance testing."""
        np.random.seed(42)
        # Large dataset for performance testing
        dates = pd.date_range('2020-01-01', periods=10000, freq='H')
        prices = 100 + np.cumsum(np.random.randn(10000) * 0.001)
        
        self.large_data = pd.DataFrame({
            'close': prices,
            'high': prices + np.abs(np.random.normal(0, 0.01, 10000)),
            'low': prices - np.abs(np.random.normal(0, 0.01, 10000)),
            'volume': np.random.randint(1000, 10000, 10000)
        }, index=dates)
        
        self.indicators = MockTechnicalIndicators()
    
    def test_indicator_speed(self):
        """Test indicator calculation speed."""
        import time
        
        # Test SMA speed
        start_time = time.time()
        sma = self.indicators.sma(self.large_data['close'], 20)
        sma_time = time.time() - start_time
        
        # Should complete within reasonable time (2 seconds for 10k data points)
        self.assertLess(sma_time, 2.0)
        
        # Test RSI speed
        start_time = time.time()
        rsi = self.indicators.rsi(self.large_data['close'], 14)
        rsi_time = time.time() - start_time
        
        self.assertLess(rsi_time, 2.0)
    
    def test_memory_usage(self):
        """Test memory usage of indicators."""
        import sys
        
        # Get initial memory usage
        initial_size = sys.getsizeof(self.large_data)
        
        # Calculate multiple indicators
        sma = self.indicators.sma(self.large_data['close'], 20)
        ema = self.indicators.ema(self.large_data['close'], 20)
        rsi = self.indicators.rsi(self.large_data['close'], 14)
        
        # Memory usage should be reasonable
        total_indicator_size = sys.getsizeof(sma) + sys.getsizeof(ema) + sys.getsizeof(rsi)
        
        # Indicators shouldn't use more than 3x the original data size
        self.assertLess(total_indicator_size, initial_size * 3)


if __name__ == "__main__":
    # Run specific test suites
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # Add test cases
    suite.addTest(loader.loadTestsFromTestCase(TestTechnicalIndicators))
    suite.addTest(loader.loadTestsFromTestCase(TestSignalGenerator))
    suite.addTest(loader.loadTestsFromTestCase(TestCustomIndicators))
    suite.addTest(loader.loadTestsFromTestCase(TestIndicatorPerformance))
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2, buffer=True)
    result = runner.run(suite)
    
    # Print summary
    print(f"\nIndicator Tests Summary:")
    print(f"Tests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    
    if result.wasSuccessful():
        print("✅ All indicator tests passed!")
    else:
        print("❌ Some tests failed. Check the output above.")

================================================================================
FILE: tests\test_ml.py
DIRECTORY: tests
SIZE: 34383 bytes
================================================================================

"""
Test suite for machine learning components.
Tests ML models, feature engineering, and prediction systems.
"""

import unittest
import pytest
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import sys
import os
import tempfile
import joblib
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, mean_squared_error
from sklearn.preprocessing import StandardScaler
import warnings

# Suppress sklearn warnings for cleaner test output
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# Add src to path for testing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))


class MockFeatureEngineering:
    """Mock feature engineering class for testing."""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.is_fitted = False
        
    def create_technical_features(self, data):
        """Create technical analysis features."""
        features = data.copy()
        
        # Price-based features
        features['returns'] = data['close'].pct_change()
        features['log_returns'] = np.log(data['close'] / data['close'].shift(1))
        features['volatility'] = features['returns'].rolling(20).std()
        
        # Moving averages
        features['sma_5'] = data['close'].rolling(5).mean()
        features['sma_20'] = data['close'].rolling(20).mean()
        features['sma_50'] = data['close'].rolling(50).mean()
        
        # Price ratios
        features['price_to_sma20'] = data['close'] / features['sma_20']
        features['sma5_to_sma20'] = features['sma_5'] / features['sma_20']
        
        # Volume features
        if 'volume' in data.columns:
            features['volume_sma'] = data['volume'].rolling(20).mean()
            features['volume_ratio'] = data['volume'] / features['volume_sma']
        
        # Time-based features
        if isinstance(data.index, pd.DatetimeIndex):
            features['hour'] = data.index.hour
            features['day_of_week'] = data.index.dayofweek
            features['month'] = data.index.month
        
        return features
    
    def create_lag_features(self, data, target_col='close', lags=[1, 2, 3, 5, 10]):
        """Create lagged features."""
        features = data.copy()
        
        for lag in lags:
            features[f'{target_col}_lag_{lag}'] = data[target_col].shift(lag)
            features[f'{target_col}_change_lag_{lag}'] = data[target_col].pct_change(lag)
        
        return features
    
    def create_target_variable(self, data, target_type='classification', horizon=1):
        """Create target variable for prediction."""
        if target_type == 'classification':
            # Binary classification: price up (1) or down (0)
            future_returns = data['close'].shift(-horizon) / data['close'] - 1
            target = (future_returns > 0).astype(int)
        elif target_type == 'regression':
            # Regression: future price
            target = data['close'].shift(-horizon)
        else:
            raise ValueError("target_type must be 'classification' or 'regression'")
        
        return target
    
    def prepare_ml_dataset(self, data, target_col, feature_cols=None, test_size=0.2):
        """Prepare dataset for ML training."""
        if feature_cols is None:
            # Exclude non-numeric columns and target
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            feature_cols = [col for col in numeric_cols if col != target_col]
        
        # Remove rows with missing values
        dataset = data[feature_cols + [target_col]].dropna()
        
        X = dataset[feature_cols]
        y = dataset[target_col]
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, shuffle=False
        )
        
        return X_train, X_test, y_train, y_test


class MockModelTrainer:
    """Mock model trainer for testing."""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        
    def train_classification_model(self, X_train, y_train, model_type='random_forest'):
        """Train classification model."""
        if model_type == 'random_forest':
            model = RandomForestClassifier(
                n_estimators=100, 
                random_state=42, 
                max_depth=10,
                min_samples_split=5
            )
        elif model_type == 'logistic_regression':
            model = LogisticRegression(random_state=42, max_iter=1000)
        else:
            raise ValueError(f"Unknown model type: {model_type}")
        
        # Scale features for logistic regression
        if model_type == 'logistic_regression':
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            model.fit(X_train_scaled, y_train)
            self.scalers[model_type] = scaler
        else:
            model.fit(X_train, y_train)
        
        self.models[model_type] = model
        return model
    
    def train_regression_model(self, X_train, y_train, model_type='random_forest'):
        """Train regression model."""
        if model_type == 'random_forest':
            model = RandomForestRegressor(
                n_estimators=100, 
                random_state=42, 
                max_depth=10,
                min_samples_split=5
            )
        elif model_type == 'linear_regression':
            model = LinearRegression()
        else:
            raise ValueError(f"Unknown model type: {model_type}")
        
        # Scale features for linear regression
        if model_type == 'linear_regression':
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            model.fit(X_train_scaled, y_train)
            self.scalers[model_type] = scaler
        else:
            model.fit(X_train, y_train)
        
        self.models[model_type] = model
        return model
    
    def predict(self, model_type, X_test):
        """Make predictions."""
        if model_type not in self.models:
            raise ValueError(f"Model {model_type} not trained")
        
        model = self.models[model_type]
        
        # Apply scaling if needed
        if model_type in self.scalers:
            scaler = self.scalers[model_type]
            X_test_scaled = scaler.transform(X_test)
            predictions = model.predict(X_test_scaled)
        else:
            predictions = model.predict(X_test)
        
        return predictions
    
    def predict_proba(self, model_type, X_test):
        """Get prediction probabilities (for classification)."""
        if model_type not in self.models:
            raise ValueError(f"Model {model_type} not trained")
        
        model = self.models[model_type]
        
        if not hasattr(model, 'predict_proba'):
            raise ValueError(f"Model {model_type} doesn't support probability prediction")
        
        # Apply scaling if needed
        if model_type in self.scalers:
            scaler = self.scalers[model_type]
            X_test_scaled = scaler.transform(X_test)
            probabilities = model.predict_proba(X_test_scaled)
        else:
            probabilities = model.predict_proba(X_test)
        
        return probabilities
    
    def save_model(self, model_type, filepath):
        """Save trained model."""
        if model_type not in self.models:
            raise ValueError(f"Model {model_type} not trained")
        
        model_data = {
            'model': self.models[model_type],
            'scaler': self.scalers.get(model_type, None)
        }
        
        joblib.dump(model_data, filepath)
        
    def load_model(self, model_type, filepath):
        """Load trained model."""
        model_data = joblib.load(filepath)
        
        self.models[model_type] = model_data['model']
        if model_data['scaler'] is not None:
            self.scalers[model_type] = model_data['scaler']


class MockPredictionEngine:
    """Mock prediction engine for testing."""
    
    def __init__(self, model_trainer):
        self.model_trainer = model_trainer
        self.feature_engineering = MockFeatureEngineering()
        
    def prepare_features(self, data):
        """Prepare features for prediction."""
        # Create technical features
        features = self.feature_engineering.create_technical_features(data)
        
        # Create lag features
        features = self.feature_engineering.create_lag_features(features)
        
        return features
    
    def predict_direction(self, data, model_type='random_forest'):
        """Predict price direction."""
        features = self.prepare_features(data)
        
        # Select feature columns (numeric only, exclude target)
        numeric_cols = features.select_dtypes(include=[np.number]).columns
        feature_cols = [col for col in numeric_cols if not col.startswith('close') or 'lag' in col]
        
        X = features[feature_cols].dropna()
        
        if len(X) == 0:
            return np.array([])
        
        predictions = self.model_trainer.predict(model_type, X)
        probabilities = self.model_trainer.predict_proba(model_type, X)
        
        return {
            'predictions': predictions,
            'probabilities': probabilities,
            'timestamps': X.index
        }
    
    def predict_price(self, data, model_type='random_forest'):
        """Predict future price."""
        features = self.prepare_features(data)
        
        # Select feature columns
        numeric_cols = features.select_dtypes(include=[np.number]).columns
        feature_cols = [col for col in numeric_cols if not col.startswith('close') or 'lag' in col]
        
        X = features[feature_cols].dropna()
        
        if len(X) == 0:
            return np.array([])
        
        predictions = self.model_trainer.predict(model_type, X)
        
        return {
            'predictions': predictions,
            'timestamps': X.index
        }


class TestFeatureEngineering(unittest.TestCase):
    """Test cases for feature engineering."""
    
    def setUp(self):
        """Set up test data."""
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=200, freq='H')
        
        # Generate realistic price data
        returns = np.random.normal(0, 0.01, 200)
        prices = 100 * np.exp(np.cumsum(returns))
        
        self.sample_data = pd.DataFrame({
            'open': prices + np.random.normal(0, 0.1, 200),
            'high': prices + np.abs(np.random.normal(0, 0.3, 200)),
            'low': prices - np.abs(np.random.normal(0, 0.3, 200)),
            'close': prices,
            'volume': np.random.randint(1000, 10000, 200)
        }, index=dates)
        
        self.feature_eng = MockFeatureEngineering()
    
    def test_technical_features_creation(self):
        """Test technical feature creation."""
        features = self.feature_eng.create_technical_features(self.sample_data)
        
        # Check that new features are created
        expected_features = ['returns', 'log_returns', 'volatility', 'sma_5', 'sma_20', 'sma_50']
        for feature in expected_features:
            self.assertIn(feature, features.columns)
        
        # Check feature properties
        self.assertTrue(features['returns'].dropna().abs().max() < 1)  # Reasonable returns
        self.assertTrue(features['volatility'].dropna().min() >= 0)  # Volatility >= 0
        
        # Check moving averages are properly calculated
        manual_sma5 = self.sample_data['close'].rolling(5).mean()
        np.testing.assert_array_almost_equal(
            features['sma_5'].dropna(), 
            manual_sma5.dropna(), 
            decimal=10
        )
    
    def test_lag_features_creation(self):
        """Test lag feature creation."""
        features = self.feature_eng.create_lag_features(self.sample_data, lags=[1, 5, 10])
        
        # Check that lag features are created
        expected_lags = ['close_lag_1', 'close_lag_5', 'close_lag_10']
        for lag_feature in expected_lags:
            self.assertIn(lag_feature, features.columns)
        
        # Check lag feature correctness
        np.testing.assert_array_equal(
            features['close_lag_1'].dropna().values,
            self.sample_data['close'].shift(1).dropna().values
        )
    
    def test_target_variable_creation(self):
        """Test target variable creation."""
        # Test classification target
        target_class = self.feature_eng.create_target_variable(
            self.sample_data, 'classification', horizon=1
        )
        
        # Should be binary (0 or 1)
        unique_values = target_class.dropna().unique()
        self.assertTrue(set(unique_values).issubset({0, 1}))
        
        # Test regression target
        target_reg = self.feature_eng.create_target_variable(
            self.sample_data, 'regression', horizon=1
        )
        
        # Should be continuous values (future prices)
        self.assertTrue(target_reg.dropna().min() > 0)  # Prices should be positive
    
    def test_ml_dataset_preparation(self):
        """Test ML dataset preparation."""
        # Add target variable
        features = self.feature_eng.create_technical_features(self.sample_data)
        features['target'] = self.feature_eng.create_target_variable(features, 'classification')
        
        X_train, X_test, y_train, y_test = self.feature_eng.prepare_ml_dataset(
            features, 'target', test_size=0.3
        )
        
        # Check shapes
        self.assertEqual(X_train.shape[1], X_test.shape[1])  # Same number of features
        self.assertEqual(len(X_train), len(y_train))
        self.assertEqual(len(X_test), len(y_test))
        
        # Check train/test split ratio
        total_samples = len(X_train) + len(X_test)
        test_ratio = len(X_test) / total_samples
        self.assertAlmostEqual(test_ratio, 0.3, places=1)
        
        # Check no missing values
        self.assertEqual(X_train.isnull().sum().sum(), 0)
        self.assertEqual(X_test.isnull().sum().sum(), 0)


class TestModelTraining(unittest.TestCase):
    """Test cases for model training."""
    
    def setUp(self):
        """Set up test data."""
        np.random.seed(42)
        
        # Create sample training data
        n_samples = 1000
        n_features = 10
        
        self.X_train = pd.DataFrame(
            np.random.randn(n_samples, n_features),
            columns=[f'feature_{i}' for i in range(n_features)]
        )
        
        # Classification target (binary)
        self.y_class = np.random.randint(0, 2, n_samples)
        
        # Regression target (continuous)
        self.y_reg = np.random.randn(n_samples) * 10 + 100
        
        # Test data
        n_test = 200
        self.X_test = pd.DataFrame(
            np.random.randn(n_test, n_features),
            columns=[f'feature_{i}' for i in range(n_features)]
        )
        self.y_test_class = np.random.randint(0, 2, n_test)
        self.y_test_reg = np.random.randn(n_test) * 10 + 100
        
        self.trainer = MockModelTrainer()
    
    def test_random_forest_classification(self):
        """Test Random Forest classification training."""
        model = self.trainer.train_classification_model(
            self.X_train, self.y_class, 'random_forest'
        )
        
        # Check model is trained
        self.assertIsNotNone(model)
        self.assertIn('random_forest', self.trainer.models)
        
        # Test predictions
        predictions = self.trainer.predict('random_forest', self.X_test)
        probabilities = self.trainer.predict_proba('random_forest', self.X_test)
        
        # Check prediction shapes and ranges
        self.assertEqual(len(predictions), len(self.X_test))
        self.assertTrue(set(predictions).issubset({0, 1}))
        self.assertEqual(probabilities.shape, (len(self.X_test), 2))
        self.assertTrue(np.allclose(probabilities.sum(axis=1), 1.0))  # Probabilities sum to 1
    
    def test_logistic_regression_classification(self):
        """Test Logistic Regression classification training."""
        model = self.trainer.train_classification_model(
            self.X_train, self.y_class, 'logistic_regression'
        )
        
        # Check model is trained
        self.assertIsNotNone(model)
        self.assertIn('logistic_regression', self.trainer.models)
        self.assertIn('logistic_regression', self.trainer.scalers)  # Should have scaler
        
        # Test predictions
        predictions = self.trainer.predict('logistic_regression', self.X_test)
        probabilities = self.trainer.predict_proba('logistic_regression', self.X_test)
        
        # Check prediction properties
        self.assertEqual(len(predictions), len(self.X_test))
        self.assertTrue(set(predictions).issubset({0, 1}))
        self.assertEqual(probabilities.shape, (len(self.X_test), 2))
    
    def test_random_forest_regression(self):
        """Test Random Forest regression training."""
        model = self.trainer.train_regression_model(
            self.X_train, self.y_reg, 'random_forest'
        )
        
        # Check model is trained
        self.assertIsNotNone(model)
        
        # Test predictions
        predictions = self.trainer.predict('random_forest', self.X_test)
        
        # Check prediction properties
        self.assertEqual(len(predictions), len(self.X_test))
        self.assertTrue(np.all(np.isfinite(predictions)))  # All predictions should be finite
        
        # Check reasonable prediction range
        self.assertTrue(predictions.min() > 50)  # Should be in reasonable range
        self.assertTrue(predictions.max() < 200)
    
    def test_linear_regression(self):
        """Test Linear Regression training."""
        model = self.trainer.train_regression_model(
            self.X_train, self.y_reg, 'linear_regression'
        )
        
        # Check model is trained
        self.assertIsNotNone(model)
        self.assertIn('linear_regression', self.trainer.scalers)  # Should have scaler
        
        # Test predictions
        predictions = self.trainer.predict('linear_regression', self.X_test)
        
        # Check prediction properties
        self.assertEqual(len(predictions), len(self.X_test))
        self.assertTrue(np.all(np.isfinite(predictions)))
    
    def test_model_evaluation(self):
        """Test model evaluation metrics."""
        # Train classification model
        self.trainer.train_classification_model(self.X_train, self.y_class, 'random_forest')
        predictions = self.trainer.predict('random_forest', self.X_test)
        
        # Calculate metrics
        accuracy = accuracy_score(self.y_test_class, predictions)
        precision = precision_score(self.y_test_class, predictions, average='weighted')
        recall = recall_score(self.y_test_class, predictions, average='weighted')
        
        # Check metric ranges
        self.assertGreaterEqual(accuracy, 0)
        self.assertLessEqual(accuracy, 1)
        self.assertGreaterEqual(precision, 0)
        self.assertLessEqual(precision, 1)
        self.assertGreaterEqual(recall, 0)
        self.assertLessEqual(recall, 1)
        
        # Train regression model
        self.trainer.train_regression_model(self.X_train, self.y_reg, 'random_forest')
        reg_predictions = self.trainer.predict('random_forest', self.X_test)
        
        # Calculate regression metrics
        mse = mean_squared_error(self.y_test_reg, reg_predictions)
        rmse = np.sqrt(mse)
        
        # Check metric properties
        self.assertGreaterEqual(mse, 0)
        self.assertGreaterEqual(rmse, 0)
    
    def test_model_save_load(self):
        """Test model saving and loading."""
        # Train a model
        self.trainer.train_classification_model(self.X_train, self.y_class, 'random_forest')
        
        # Get predictions before saving
        original_predictions = self.trainer.predict('random_forest', self.X_test)
        
        # Save model
        with tempfile.NamedTemporaryFile(suffix='.joblib', delete=False) as f:
            temp_path = f.name
        
        try:
            self.trainer.save_model('random_forest', temp_path)
            
            # Create new trainer and load model
            new_trainer = MockModelTrainer()
            new_trainer.load_model('random_forest', temp_path)
            
            # Get predictions from loaded model
            loaded_predictions = new_trainer.predict('random_forest', self.X_test)
            
            # Predictions should be identical
            np.testing.assert_array_equal(original_predictions, loaded_predictions)
            
        finally:
            os.unlink(temp_path)


class TestPredictionEngine(unittest.TestCase):
    """Test cases for prediction engine."""
    
    def setUp(self):
        """Set up test environment."""
        np.random.seed(42)
        dates = pd.date_range('2023-01-01', periods=500, freq='H')
        
        # Generate realistic price data
        returns = np.random.normal(0, 0.01, 500)
        prices = 100 * np.exp(np.cumsum(returns))
        
        self.sample_data = pd.DataFrame({
            'open': prices + np.random.normal(0, 0.1, 500),
            'high': prices + np.abs(np.random.normal(0, 0.3, 500)),
            'low': prices - np.abs(np.random.normal(0, 0.3, 500)),
            'close': prices,
            'volume': np.random.randint(1000, 10000, 500)
        }, index=dates)
        
        # Prepare training data
        feature_eng = MockFeatureEngineering()
        features = feature_eng.create_technical_features(self.sample_data)
        features = feature_eng.create_lag_features(features)
        features['target_class'] = feature_eng.create_target_variable(features, 'classification')
        features['target_reg'] = feature_eng.create_target_variable(features, 'regression')
        
        # Prepare ML dataset
        numeric_cols = features.select_dtypes(include=[np.number]).columns
        feature_cols = [col for col in numeric_cols if not col.startswith('target')]
        
        dataset = features[feature_cols + ['target_class', 'target_reg']].dropna()
        
        # Split data
        split_idx = int(len(dataset) * 0.8)
        train_data = dataset.iloc[:split_idx]
        
        X_train = train_data[feature_cols]
        y_train_class = train_data['target_class']
        y_train_reg = train_data['target_reg']
        
        # Train models
        self.trainer = MockModelTrainer()
        self.trainer.train_classification_model(X_train, y_train_class, 'random_forest')
        self.trainer.train_regression_model(X_train, y_train_reg, 'random_forest')
        
        self.prediction_engine = MockPredictionEngine(self.trainer)
    
    def test_direction_prediction(self):
        """Test price direction prediction."""
        results = self.prediction_engine.predict_direction(self.sample_data)
        
        # Check result structure
        self.assertIn('predictions', results)
        self.assertIn('probabilities', results)
        self.assertIn('timestamps', results)
        
        # Check predictions
        predictions = results['predictions']
        probabilities = results['probabilities']
        
        if len(predictions) > 0:
            # Predictions should be binary
            self.assertTrue(set(predictions).issubset({0, 1}))
            
            # Probabilities should sum to 1
            self.assertEqual(probabilities.shape[1], 2)
            prob_sums = probabilities.sum(axis=1)
            self.assertTrue(np.allclose(prob_sums, 1.0))
    
    def test_price_prediction(self):
        """Test price level prediction."""
        results = self.prediction_engine.predict_price(self.sample_data)
        
        # Check result structure
        self.assertIn('predictions', results)
        self.assertIn('timestamps', results)
        
        # Check predictions
        predictions = results['predictions']
        
        if len(predictions) > 0:
            # Predictions should be positive (prices)
            self.assertTrue(np.all(predictions > 0))
            
            # Predictions should be in reasonable range
            actual_prices = self.sample_data['close']
            price_min = actual_prices.min() * 0.5
            price_max = actual_prices.max() * 2.0
            
            self.assertTrue(np.all(predictions >= price_min))
            self.assertTrue(np.all(predictions <= price_max))
    
    def test_feature_preparation(self):
        """Test feature preparation for prediction."""
        features = self.prediction_engine.prepare_features(self.sample_data)
        
        # Check that features are created
        self.assertGreater(features.shape[1], self.sample_data.shape[1])
        
        # Check for expected feature types
        feature_names = features.columns.tolist()
        self.assertTrue(any('sma' in name for name in feature_names))
        self.assertTrue(any('lag' in name for name in feature_names))
        self.assertTrue(any('returns' in name for name in feature_names))
    
    def test_prediction_with_insufficient_data(self):
        """Test prediction with insufficient data."""
        # Use very small dataset
        small_data = self.sample_data.iloc[:10]
        
        # Should handle gracefully
        direction_results = self.prediction_engine.predict_direction(small_data)
        price_results = self.prediction_engine.predict_price(small_data)
        
        # May return empty results but shouldn't crash
        self.assertIsInstance(direction_results, dict)
        self.assertIsInstance(price_results, dict)


class TestMLIntegration(unittest.TestCase):
    """Integration tests for ML components."""
    
    def setUp(self):
        """Set up integration test environment."""
        np.random.seed(42)
        
        # Create realistic market data
        dates = pd.date_range('2023-01-01', periods=1000, freq='H')
        
        # Generate price data with some trend
        trend = np.linspace(0, 0.1, 1000)  # 10% upward trend
        noise = np.random.normal(0, 0.02, 1000)  # 2% volatility
        returns = trend + noise
        
        prices = 100 * np.exp(np.cumsum(returns))
        
        self.market_data = pd.DataFrame({
            'open': prices + np.random.normal(0, 0.1, 1000),
            'high': prices + np.abs(np.random.normal(0, 0.3, 1000)),
            'low': prices - np.abs(np.random.normal(0, 0.3, 1000)),
            'close': prices,
            'volume': np.random.randint(1000, 10000, 1000)
        }, index=dates)
        
    def test_complete_ml_pipeline(self):
        """Test complete ML pipeline from data to predictions."""
        # Step 1: Feature Engineering
        feature_eng = MockFeatureEngineering()
        features = feature_eng.create_technical_features(self.market_data)
        features = feature_eng.create_lag_features(features)
        features['target'] = feature_eng.create_target_variable(features, 'classification')
        
        # Step 2: Prepare ML dataset
        X_train, X_test, y_train, y_test = feature_eng.prepare_ml_dataset(
            features, 'target', test_size=0.2
        )
        
        # Step 3: Train model
        trainer = MockModelTrainer()
        model = trainer.train_classification_model(X_train, y_train, 'random_forest')
        
        # Step 4: Make predictions
        predictions = trainer.predict('random_forest', X_test)
        probabilities = trainer.predict_proba('random_forest', X_test)
        
        # Step 5: Evaluate performance
        accuracy = accuracy_score(y_test, predictions)
        
        # Verify pipeline worked
        self.assertIsNotNone(model)
        self.assertEqual(len(predictions), len(y_test))
        self.assertGreaterEqual(accuracy, 0.3)  # Should be better than random for trending data
        self.assertLessEqual(accuracy, 1.0)
        
        # Step 6: Use prediction engine
        prediction_engine = MockPredictionEngine(trainer)
        recent_data = self.market_data.iloc[-100:]  # Last 100 points
        
        direction_results = prediction_engine.predict_direction(recent_data)
        
        # Verify prediction engine works
        if len(direction_results['predictions']) > 0:
            self.assertTrue(set(direction_results['predictions']).issubset({0, 1}))
    
    def test_multiple_models_comparison(self):
        """Test training and comparing multiple models."""
        # Prepare data
        feature_eng = MockFeatureEngineering()
        features = feature_eng.create_technical_features(self.market_data)
        features = feature_eng.create_lag_features(features)
        features['target'] = feature_eng.create_target_variable(features, 'classification')
        
        X_train, X_test, y_train, y_test = feature_eng.prepare_ml_dataset(
            features, 'target', test_size=0.2
        )
        
        # Train multiple models
        trainer = MockModelTrainer()
        
        rf_model = trainer.train_classification_model(X_train, y_train, 'random_forest')
        lr_model = trainer.train_classification_model(X_train, y_train, 'logistic_regression')
        
        # Compare predictions
        rf_predictions = trainer.predict('random_forest', X_test)
        lr_predictions = trainer.predict('logistic_regression', X_test)
        
        # Evaluate both models
        rf_accuracy = accuracy_score(y_test, rf_predictions)
        lr_accuracy = accuracy_score(y_test, lr_predictions)
        
        # Both models should produce valid results
        self.assertGreaterEqual(rf_accuracy, 0)
        self.assertGreaterEqual(lr_accuracy, 0)
        self.assertLessEqual(rf_accuracy, 1)
        self.assertLessEqual(lr_accuracy, 1)
        
        # Models should have some predictive power (better than random)
        self.assertTrue(rf_accuracy > 0.3 or lr_accuracy > 0.3)
    
    def test_model_persistence(self):
        """Test model training, saving, and loading cycle."""
        # Prepare and train model
        feature_eng = MockFeatureEngineering()
        features = feature_eng.create_technical_features(self.market_data)
        features = feature_eng.create_lag_features(features)
        features['target'] = feature_eng.create_target_variable(features, 'classification')
        
        X_train, X_test, y_train, y_test = feature_eng.prepare_ml_dataset(
            features, 'target', test_size=0.2
        )
        
        trainer = MockModelTrainer()
        trainer.train_classification_model(X_train, y_train, 'random_forest')
        
        # Get predictions before saving
        original_predictions = trainer.predict('random_forest', X_test)
        
        # Save and load model
        with tempfile.NamedTemporaryFile(suffix='.joblib', delete=False) as f:
            temp_path = f.name
        
        try:
            trainer.save_model('random_forest', temp_path)
            
            # Load in new trainer
            new_trainer = MockModelTrainer()
            new_trainer.load_model('random_forest', temp_path)
            
            # Test predictions with loaded model
            loaded_predictions = new_trainer.predict('random_forest', X_test)
            
            # Predictions should be identical
            np.testing.assert_array_equal(original_predictions, loaded_predictions)
            
            # Test with prediction engine
            prediction_engine = MockPredictionEngine(new_trainer)
            results = prediction_engine.predict_direction(self.market_data.iloc[-50:])
            
            # Should work without errors
            self.assertIsInstance(results, dict)
            
        finally:
            os.unlink(temp_path)


if __name__ == "__main__":
    # Run all ML tests
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # Add test cases
    suite.addTest(loader.loadTestsFromTestCase(TestFeatureEngineering))
    suite.addTest(loader.loadTestsFromTestCase(TestModelTraining))
    suite.addTest(loader.loadTestsFromTestCase(TestPredictionEngine))
    suite.addTest(loader.loadTestsFromTestCase(TestMLIntegration))
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2, buffer=True)
    result = runner.run(suite)
    
    # Print summary
    print(f"\nML Tests Summary:")
    print(f"Tests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    
    if result.wasSuccessful():
        print("✅ All ML tests passed!")
    else:
        print("❌ Some tests failed. Check the output above.")
        
        if result.failures:
            print("\nFailures:")
            for test, traceback in result.failures:
                print(f"- {test}")
        
        if result.errors:
            print("\nErrors:")
            for test, traceback in result.errors:
                print(f"- {test}")

================================================================================
PROJECT SUMMARY
================================================================================
Total files combined: 38
Total lines of code: 11455
Generated: 2025-07-16 23:11:05.726212
================================================================================
